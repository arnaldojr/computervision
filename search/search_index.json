{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Vis\u00e3o Computacional Aplicada","text":"<p>Bem-vindx \u00e0 disciplina de Vis\u00e3o Computacional Aplicada do curso de Engenharia de Software.</p> <p>Nesta p\u00e1gina voc\u00ea encontrar\u00e1 os conte\u00fados da disciplina (atividades, laborat\u00f3rios, materiais de apoio, dicas e refer\u00eancias) organizados para estudo e acompanhamento das aulas.</p> <p>Come\u00e7ar Ferramentas / Instala\u00e7\u00e3o Bibliografia</p>"},{"location":"#informacoes-rapidas","title":"Informa\u00e7\u00f5es r\u00e1pidas","text":"<ul> <li>Curso: Engenharia de Software (ES)  </li> <li>Disciplina: Vis\u00e3o Computacional Aplicada  </li> <li>Turmas (2026): 4ES  </li> <li>Professores: Arnaldo Viana \u00b7 Celio Soares \u00b7 Paulo Sergio  </li> </ul> <p>Como usar este site</p> <p>Use o menu lateral para navegar pelos laborat\u00f3rios, materiais de apoio e recursos. Se voc\u00ea estiver no in\u00edcio, clique em Come\u00e7ar agora.</p>"},{"location":"#o-que-voce-vai-aprender","title":"O que voc\u00ea vai aprender","text":"<p>O curso percorre uma trilha completa e pr\u00e1tica:</p> <ul> <li>Fundamentos de processamento digital de imagens</li> <li>Segmenta\u00e7\u00e3o, contornos, histogramas e filtragem</li> <li>Features e pipelines cl\u00e1ssicos</li> <li>Introdu\u00e7\u00e3o a CNNs e modelos modernos</li> <li>Integra\u00e7\u00e3o: do notebook ao sistema de software (APIs, deploy e boas pr\u00e1ticas)</li> </ul>"},{"location":"#estrutura-do-curso","title":"Estrutura do curso","text":"<p>O curso \u00e9 dividido em 5 unidades tem\u00e1ticas, combinando teoria e pr\u00e1tica com foco em aplica\u00e7\u00f5es reais.</p> <p>Regra do jogo</p> <p>Cada unidade tem conte\u00fado guiado + laborat\u00f3rio. O objetivo \u00e9 produzir artefatos que voc\u00ea conseguiria usar em um projeto real.</p>"},{"location":"#aulas-praticas","title":"Aulas pr\u00e1ticas","text":"<ul> <li> <p> Comece por aqui</p> <p>Guia inicial com contexto, como rodar os labs e o que esperar.</p> <p>Abrir introdu\u00e7\u00e3o</p> </li> <li> <p> Ferramentas</p> <p>Instala\u00e7\u00e3o local, venv e bibliotecas.</p> <p>Abrir guia</p> </li> <li> <p> Agenda</p> <p>Datas, sequ\u00eancia das aulas e checkpoints.</p> <p>Ver agenda</p> </li> <li> <p> Bibliografia</p> <p>Refer\u00eancias essenciais e materiais de apoio.</p> <p>Ver bibliografia</p> </li> </ul>"},{"location":"#repositorio-do-curso","title":"Reposit\u00f3rio do curso","text":"<p>C\u00f3digo-fonte e materiais</p> <p>Todo o conte\u00fado, notebooks e arquivos do curso est\u00e3o no reposit\u00f3rio: https://github.com/arnaldojr/computervision/</p>"},{"location":"agenda/agenda/","title":"Agenda","text":""},{"location":"agenda/agenda/#cronograma-1o-semestre-2026","title":"Cronograma 1\u00ba Semestre - 2026","text":"1\u00ba Semestre \u2014 Conte\u00fado 4ESTer\u00e7a-Feira Aula MagnaApresenta\u00e7\u00e3o do curso, din\u00e2mica das aulas, datas importantes (CP) 10/02/2026 feriado carnaval 17/02/2026 Processamento de imagem digital 24/02/2026 filtos de convolu\u00e7\u00e3o, espa\u00e7o de cores e contorno 03/03/2026 CP1 10/03/2026 template matching, features ORB, SIFT 17/03/2026 Classifica\u00e7\u00e3o ML 24/03/2026 API para CV 31/03/2026 CP2 07/04/2026 Aplica\u00e7\u00f5es media pipe 14/04/2026 Aplica\u00e7\u00f5es em Redes Neurais pr\u00e9-treinadas 21/04/2026 Aplica\u00e7\u00e3o de Transfer Learning 28/04/2026 Detec\u00e7\u00e3o Moderna 05/05/2026 Deploy 12/05/2026 CP3 19/05/2026"},{"location":"aulas/aula1/","title":"Aula 1 \u2014 Fundamentos de Imagem Digital e Representa\u00e7\u00e3o em Mem\u00f3ria (PDI)","text":"<p>Nesta aula voc\u00ea vai consolidar a teoria m\u00ednima necess\u00e1ria para executar o notebook pr\u00e1tico:</p> <p>Lab01 \u2014 Intro PID</p> <p>O objetivo n\u00e3o \u00e9 decorar termos, mas entender como a imagem \u00e9 representada e o que acontece quando voc\u00ea l\u00ea, visualiza e altera pixels.</p>"},{"location":"aulas/aula1/#como-usar-este-handout","title":"Como usar este handout","text":"<p>Fluxo recomendado</p> <ol> <li>Antes do notebook: leia at\u00e9 a se\u00e7\u00e3o 5 e responda os quizzes conforme aparecem.  </li> <li>Durante o notebook: use este material como refer\u00eancia (principalmente BGR/RGB, <code>shape</code>, <code>dtype</code>, <code>resize</code>).  </li> <li>Depois (5 min): revise apenas as perguntas que errou e valide no c\u00f3digo.</li> </ol>"},{"location":"aulas/aula1/#objetivos-de-aprendizagem","title":"Objetivos de aprendizagem","text":"<p>Ao final desta aula, voc\u00ea deve ser capaz de:</p> <ol> <li>Explicar o que significa uma imagem ser um sinal discreto 2D (amostragem + quantiza\u00e7\u00e3o).</li> <li>Interpretar a estrutura de uma imagem em Python como arrays NumPy (<code>H\u00d7W\u00d7C</code>).</li> <li>Diferenciar BGR (OpenCV) de RGB (Matplotlib) e evitar visualiza\u00e7\u00e3o \u201ccom cores erradas\u201d.</li> <li>Identificar <code>dtype</code> (ex.: <code>uint8</code>) e o range de valores que ele suporta.</li> <li>Aplicar opera\u00e7\u00f5es simples: tons de cinza, resize/amostragem, leitura de pixel, altera\u00e7\u00e3o de pixels.</li> <li>Entender por que varrer imagem com <code>for</code> \u00e9 lento e quando isso faz sentido.</li> </ol>"},{"location":"aulas/aula1/#1-imagem-digital-amostragem-e-quantizacao","title":"1) Imagem digital: amostragem e quantiza\u00e7\u00e3o","text":"<p>Uma imagem digital pode ser vista como um sinal cont\u00ednuo (mundo real) convertido em uma estrutura discreta:</p> <ul> <li>Amostragem (sampling): define quantos pixels teremos (resolu\u00e7\u00e3o).</li> <li>Quantiza\u00e7\u00e3o (quantization): define quantos valores poss\u00edveis cada pixel pode assumir.</li> </ul> <p>Em termos pr\u00e1ticos: - Mais resolu\u00e7\u00e3o \u21d2 mais detalhes espaciais, mas maior custo de processamento. - Mais bits por pixel \u21d2 mais n\u00edveis de intensidade (ex.: 8 bits \u21d2 256 n\u00edveis).</p> # <p>Qual defini\u00e7\u00e3o descreve melhor amostragem em imagens?</p> Definir a quantidade de pixels (resolu\u00e7\u00e3o) que representa a cenaDefinir quantos n\u00edveis de intensidade cada pixel pode assumirDefinir a ordem dos canais de cor (RGB/BGR) <p>Amostragem est\u00e1 ligada \u00e0 densidade espacial (quantos pontos/pixels).</p> # <p>Uma imagem de 8 bits por pixel tem quantos n\u00edveis poss\u00edveis de intensidade?</p> 1282561024 <p>8 bits \u21d2 2\u2078 = 256 valores.</p>"},{"location":"aulas/aula1/#2-como-a-imagem-aparece-no-codigo-numpy","title":"2) Como a imagem aparece no c\u00f3digo (NumPy)","text":"<p>No notebook, ap\u00f3s carregar uma imagem, voc\u00ea ver\u00e1 algo como:</p> <ul> <li>Imagem em tons de cinza: array 2D <code>shape = (H, W)</code></li> <li>Imagem colorida: array 3D <code>shape = (H, W, C)</code> com <code>C = 3</code> (canais)</li> </ul> <p>Interpreta\u00e7\u00e3o: - <code>H</code> = n\u00famero de linhas (altura) - <code>W</code> = n\u00famero de colunas (largura) - <code>C</code> = canais de cor</p> <p>Regra pr\u00e1tica de indexa\u00e7\u00e3o</p> <p>Em NumPy/OpenCV voc\u00ea acessa como <code>img[y, x]</code> (linha primeiro, coluna depois). Ou seja: (y, x) \u2248 (linha, coluna).</p> # <p>Em NumPy/OpenCV, se <code>img.shape == (720, 1280, 3)</code>, o que isso significa?</p> A imagem tem 720 linhas, 1280 colunas e 3 canaisA imagem tem 1280 linhas, 720 colunas e 3 canaisA imagem tem 720 linhas, 1280 colunas e 1280 canais <p><code>shape = (H, W, C)</code> onde <code>H</code> \u00e9 altura (linhas) e <code>W</code> \u00e9 largura (colunas).</p> # <p>Qual acesso \u00e9 correto para pegar um pixel na posi\u00e7\u00e3o (x=50, y=10) em NumPy/OpenCV?</p> <code>img[10, 50]</code><code>img[50, 10]</code><code>img(x=50, y=10)</code> <p>A indexa\u00e7\u00e3o \u00e9 <code>img[y, x]</code> (linha, coluna).</p>"},{"location":"aulas/aula1/#3-bgr-vs-rgb-por-que-as-cores-ficam-erradas","title":"3) BGR vs RGB: por que as cores \u201cficam erradas\u201d?","text":"<ul> <li>OpenCV (<code>cv2</code>) l\u00ea imagens em BGR (Blue, Green, Red).</li> <li>Matplotlib (<code>plt.imshow</code>) espera RGB.</li> </ul> <p>Se voc\u00ea fizer: <pre><code>img = cv2.imread(\"NATUREZA_1.jpg\")\nplt.imshow(img)\n</code></pre> as cores provavelmente ficar\u00e3o trocadas.</p> <p>A corre\u00e7\u00e3o \u00e9 converter: <pre><code>img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.imshow(img_rgb)\n</code></pre></p> <p>Pegadinha cl\u00e1ssica</p> <p>Se voc\u00ea n\u00e3o converter BGR\u2192RGB, seu pipeline pode \u201cparecer\u201d errado, mas o problema \u00e9 apenas de visualiza\u00e7\u00e3o (n\u00e3o necessariamente do processamento).</p> # <p>Qual \u00e9 a ordem padr\u00e3o de canais ao carregar uma imagem com <code>cv2.imread()</code>?</p> BGRRGBHSV <p>OpenCV usa BGR por padr\u00e3o.</p> # <p>Para visualizar corretamente no Matplotlib uma imagem lida pelo OpenCV, voc\u00ea normalmente faz:</p> Converter BGR \u2192 RGB com <code>cv2.cvtColor</code>Converter RGB \u2192 HSV com <code>cv2.cvtColor</code>Apenas usar <code>plt.imshow(img)</code> sem convers\u00e3o <p>Matplotlib espera RGB.</p>"},{"location":"aulas/aula1/#4-tons-de-cinza-o-que-muda","title":"4) Tons de cinza: o que muda?","text":"<p>Uma imagem em tons de cinza pode ser vista como uma \u00fanica banda de intensidade.</p> <p>No OpenCV: - <code>cv2.imread(path, cv2.IMREAD_GRAYSCALE)</code> - ou <code>cv2.imread(path, 0)</code></p> <p>Na visualiza\u00e7\u00e3o com Matplotlib, use: <pre><code>plt.imshow(img_gray, cmap=\"gray\")\n</code></pre></p> <p>Por que usar <code>cmap='gray'</code>?</p> <p>Sem o <code>cmap</code>, o Matplotlib pode aplicar um mapa de cores (colormap) que n\u00e3o representa \u201ccinza\u201d de verdade, o que atrapalha sua interpreta\u00e7\u00e3o.</p> # <p>No OpenCV, carregar uma imagem diretamente em tons de cinza pode ser feito com:</p> <code>cv2.imread(path, cv2.IMREAD_GRAYSCALE)</code><code>cv2.imread(path, 0)</code><code>cv2.imread(path, cv2.COLOR_BGR2GRAY)</code> Enviar <p><code>cv2.COLOR_BGR2GRAY</code> \u00e9 usado com <code>cv2.cvtColor</code>, n\u00e3o como flag do <code>imread</code>.</p> # <p>Ao usar <code>plt.imshow()</code> para mostrar uma imagem em tons de cinza, o mais correto \u00e9:</p> <code>plt.imshow(img_gray, cmap=\"gray\")</code><code>plt.imshow(img_gray)</code> (sem nada)<code>plt.imshow(img_gray, cmap=\"rgb\")</code> <p>O <code>cmap=\"gray\"</code> preserva a interpreta\u00e7\u00e3o correta de intensidade.</p>"},{"location":"aulas/aula1/#5-tipos-dtype-e-range-por-que-isso-importa","title":"5) Tipos (<code>dtype</code>) e range: por que isso importa?","text":"<p>A maioria das imagens lidas com OpenCV vem como:</p> <ul> <li><code>dtype = uint8</code></li> <li>valores no range [0, 255]</li> </ul> <p>Isso tem consequ\u00eancias: - Ao fazer contas, voc\u00ea pode ter overflow ou clipping se n\u00e3o controlar o tipo. - Para opera\u00e7\u00f5es matem\u00e1ticas (ex.: normaliza\u00e7\u00e3o, filtros, gamma), \u00e9 comum converter para <code>float32</code>,   calcular e depois voltar para <code>uint8</code> com recorte.</p> <p>Exemplo de boa pr\u00e1tica (ideia geral): <pre><code>img_f = img.astype(\"float32\") / 255.0\n# ... processa ...\nimg_u8 = (img_f * 255).clip(0, 255).astype(\"uint8\")\n</code></pre></p> #          Uma imagem <code>uint8</code> tipicamente tem valores de intensidade entre  e .      Enviar <p><code>uint8</code> comporta 256 n\u00edveis. Se voc\u00ea fizer opera\u00e7\u00f5es sem cuidado, pode ocorrer clipping/overflow.</p> # <p>Qual \u00e9 o motivo mais comum para converter uma imagem para <code>float32</code> antes de certas opera\u00e7\u00f5es?</p> Evitar problemas de overflow/clipping e trabalhar com valores normalizadosPorque <code>uint8</code> n\u00e3o suporta imagens coloridasPorque <code>float32</code> \u00e9 sempre mais r\u00e1pido em todas as opera\u00e7\u00f5es <p>Em processamento, <code>float32</code> ajuda a manter a matem\u00e1tica correta, principalmente ao normalizar.</p>"},{"location":"aulas/aula1/#6-amostragem-na-pratica-resize-e-interpolacao","title":"6) Amostragem na pr\u00e1tica: resize e interpola\u00e7\u00e3o","text":"<p>No notebook, voc\u00ea usa <code>cv2.resize()</code> para mudar o tamanho:</p> <pre><code>img2 = cv2.resize(img_rgb, (600, 400), cv2.INTER_LINEAR)\n</code></pre> <p>Dois pontos importantes: 1. O tamanho \u00e9 informado como (largura, altura) \u2014 aten\u00e7\u00e3o: \u00e9 o inverso do <code>shape</code>. 2. A interpola\u00e7\u00e3o define como os novos pixels s\u00e3o estimados:    - <code>INTER_NEAREST</code>: r\u00e1pido, pode \u201cpixelar\u201d    - <code>INTER_LINEAR</code>: padr\u00e3o, bom para muitos casos    - <code>INTER_AREA</code>: geralmente bom para reduzir (downsample)</p> # <p>O <code>cv2.resize(img, (600, 400), ...)</code> recebe o tamanho no formato:</p> (largura, altura)(altura, largura)(linhas, colunas, canais) <p>Aten\u00e7\u00e3o: isso difere do <code>shape</code>, que \u00e9 (altura, largura, canais).</p> # <p>Qual interpola\u00e7\u00e3o tende a ser boa para reduzir a imagem (downsample)?</p> <code>cv2.INTER_NEAREST</code><code>cv2.INTER_LINEAR</code><code>cv2.INTER_AREA</code> <p><code>INTER_AREA</code> costuma ser uma boa escolha ao diminuir imagens.</p>"},{"location":"aulas/aula1/#7-acessando-e-alterando-pixels","title":"7) Acessando e alterando pixels","text":"<p>Acesso a um pixel (colorido): <pre><code>(b, g, r) = img_bgr[y, x]\n</code></pre></p> <p>Acesso a um pixel (cinza): <pre><code>v = img_gray[y, x]\n</code></pre></p> <p>Alterar pixels pode ser feito com <code>for</code>, mas isso \u00e9 caro:</p> <pre><code>for y in range(img.shape[0]):\n    for x in range(img.shape[1]):\n        img[y, x] = (255, 0, 0)\n</code></pre> <p>Custo computacional</p> <p>Uma imagem 1080p tem ~2 milh\u00f5es de pixels. Dois <code>for</code> aninhados em Python ficam lentos rapidamente.</p> <p>Alternativa profissional: usar opera\u00e7\u00f5es vetorizadas (NumPy) e m\u00e1scaras. Voc\u00ea ver\u00e1 isso nas pr\u00f3ximas aulas.</p> # <p>Marque as afirmativas verdadeiras sobre varrer uma imagem com <code>for</code> em Python:</p> Dois loops aninhados percorrem todos os pixels e podem ser lentos em imagens grandesEm geral, opera\u00e7\u00f5es vetorizadas com NumPy s\u00e3o mais eficientes que loops puros em PythonLoops sempre s\u00e3o mais r\u00e1pidos que opera\u00e7\u00f5es vetorizadasVarrer a imagem n\u00e3o muda a complexidade do algoritmo Enviar <p>Loops em Python puro s\u00e3o did\u00e1ticos, mas em produ\u00e7\u00e3o evitamos quando poss\u00edvel.</p>"},{"location":"aulas/aula10/","title":"Aula 10 - Detec\u00e7\u00e3o Moderna (YOLO)","text":""},{"location":"aulas/aula10/#objetivo-da-aula","title":"Objetivo da Aula","text":"<p>Implementar detec\u00e7\u00e3o de objetos moderna com YOLO (You Only Look Once), entender bounding boxes, tempo de infer\u00eancia, quantiza\u00e7\u00e3o e containeriza\u00e7\u00e3o de modelos.</p>"},{"location":"aulas/aula10/#conteudo-teorico","title":"Conte\u00fado Te\u00f3rico","text":""},{"location":"aulas/aula10/#deteccao-de-objetos-moderna","title":"Detec\u00e7\u00e3o de Objetos Moderna","text":"<p>A detec\u00e7\u00e3o de objetos \u00e9 uma tarefa fundamental em vis\u00e3o computacional que combina classifica\u00e7\u00e3o e localiza\u00e7\u00e3o. Diferente da classifica\u00e7\u00e3o de imagens, a detec\u00e7\u00e3o de objetos identifica:</p> <ul> <li>O que est\u00e1 na imagem (classifica\u00e7\u00e3o)</li> <li>Onde est\u00e1 na imagem (localiza\u00e7\u00e3o com bounding boxes)</li> <li>Qu\u00e3o certo est\u00e1 da detec\u00e7\u00e3o (confian\u00e7a)</li> </ul>"},{"location":"aulas/aula10/#yolo-you-only-look-once","title":"YOLO (You Only Look Once)","text":"<p>YOLO \u00e9 uma arquitetura popular para detec\u00e7\u00e3o de objetos em tempo real que processa a imagem inteira em uma \u00fanica passagem:</p> <p>Vantagens: - Alta velocidade de infer\u00eancia - Boa acur\u00e1cia para aplica\u00e7\u00f5es em tempo real - Arquitetura elegante e eficiente</p> <p>Componentes principais: - Grid de detec\u00e7\u00e3o - Bounding boxes com confian\u00e7a - Classifica\u00e7\u00e3o de objetos - Non-maximum suppression</p>"},{"location":"aulas/aula10/#bounding-boxes","title":"Bounding Boxes","text":"<p>As bounding boxes s\u00e3o ret\u00e2ngulos que delimitam objetos detectados, tipicamente representados por: - (x, y, w, h): Coordenadas do centro e dimens\u00f5es - (x_min, y_min, x_max, y_max): Coordenadas dos cantos</p>"},{"location":"aulas/aula10/#aplicacoes-reais","title":"Aplica\u00e7\u00f5es Reais","text":"<ul> <li>Seguran\u00e7a: Detec\u00e7\u00e3o de pessoas e ve\u00edculos</li> <li>Varejo: An\u00e1lise de comportamento do cliente</li> <li>Agricultura: Detec\u00e7\u00e3o de pragas e colheitas</li> <li>Sa\u00fade: Identifica\u00e7\u00e3o de anomalias em imagens m\u00e9dicas</li> </ul>"},{"location":"aulas/aula10/#atividade-pratica","title":"Atividade Pr\u00e1tica","text":""},{"location":"aulas/aula10/#implementar-detector-de-objetos","title":"Implementar Detector de Objetos","text":"<pre><code># src/models/yolo_detector.py\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom typing import List, Dict, Tuple\nimport time\n\nclass YOLODetector:\n    def __init__(self, model_path: str = None, config_path: str = None, classes_path: str = None):\n        \"\"\"\n        Inicializa o detector YOLO\n        Nota: Esta \u00e9 uma implementa\u00e7\u00e3o conceitual\n        Para uso real, recomenda-se ultralytics/yolov5 ou tensorflow-models\n        \"\"\"\n        self.model_path = model_path\n        self.config_path = config_path\n        self.classes_path = classes_path\n        self.net = None\n        self.classes = []\n        self.output_layers = []\n\n        # Em uma implementa\u00e7\u00e3o real, carregar\u00edamos o modelo aqui\n        self._setup_mock_model()\n\n    def _setup_mock_model(self):\n        \"\"\"Configura modelo mock para demonstra\u00e7\u00e3o\"\"\"\n        print(\"Configurando modelo mock para demonstra\u00e7\u00e3o...\")\n\n        # Classes comuns para demonstra\u00e7\u00e3o\n        self.classes = [\n            'pessoa', 'bicicleta', 'carro', 'moto', 'avi\u00e3o', '\u00f4nibus', \n            'trem', 'caminh\u00e3o', 'barco', 'sem\u00e1foro', 'hidrante', \n            'placa de parar', 'estacionamento', 'banco', '\u00e1rvore', \n            'luz vermelha', 'sinal de m\u00e3o', 'cone', 'p\u00e1ssaro', 'gato', \n            'cachorro', 'cavalo', 'ovelha', 'vaca', 'elefante', \n            'urso', 'zebra', 'girafa', 'mochila', 'guarda-chuva', \n            'bolsa', 'gravata', 'maleta', 'frisbee', 'skate', \n            'surf', 'bola', 'pipa', 'bast\u00e3o', 'luva', 't\u00eanis', \n            'shorts', 'camisa', 'vestido', 'casaco', 'meia', \n            '\u00f3culos', 'rel\u00f3gio', 'bolsa', 'chave', 'carteira', \n            'celular', 'perfume', 'espelho', 'escova', 'batom', \n            'livro', 'caneta', 'l\u00e1pis', 'caderno', 'borracha', \n            'cola', 'tesoura', 'grampeador', 'clipe', 'agulha', \n            'linha', 'bot\u00e3o', 'fita', 'tesoura', 'martelo', \n            'prego', 'parafuso', 'arruela', 'porca', 'chave', \n            'faca', 'garfo', 'colher', 'copo', 'prato', 'panela', \n            'frigideira', 'fog\u00e3o', 'geladeira', 'microondas', \n            'forno', 'liquidificador', 'cafeteira', 'torradeira', \n            'batedeira', 'cortador', 'ralador', 'abridor', \n            'faca', 't\u00e1bua', 'pia', 'fog\u00e3o', 'geladeira'\n        ]\n\n        # Limitar para as primeiras 10 classes para simplificar\n        self.classes = self.classes[:10]\n\n    def load_model(self):\n        \"\"\"Carrega modelo YOLO (implementa\u00e7\u00e3o real)\"\"\"\n        if self.model_path and self.config_path:\n            self.net = cv2.dnn.readNet(self.model_path, self.config_path)\n\n            # Obter nomes das camadas de sa\u00edda\n            layer_names = self.net.getLayerNames()\n            self.output_layers = [layer_names[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]\n        else:\n            print(\"Usando modelo mock para demonstra\u00e7\u00e3o\")\n\n    def detect_objects(self, image: np.ndarray, confidence_threshold: float = 0.5) -&gt; List[Dict]:\n        \"\"\"\n        Detecta objetos na imagem\n        Retorna lista de dicion\u00e1rios com informa\u00e7\u00f5es sobre detec\u00e7\u00f5es\n        \"\"\"\n        start_time = time.time()\n\n        height, width, channels = image.shape\n\n        # Em uma implementa\u00e7\u00e3o real, far\u00edamos:\n        # 1. Criar blob da imagem\n        # 2. Passar pela rede\n        # 3. Processar outputs\n\n        # Para esta demonstra\u00e7\u00e3o, simularemos detec\u00e7\u00f5es\n        detections = self._simulate_detections(image, confidence_threshold)\n\n        processing_time = time.time() - start_time\n\n        # Adicionar tempo de processamento \u00e0s detec\u00e7\u00f5es\n        for detection in detections:\n            detection['processing_time'] = processing_time / len(detections) if detections else processing_time\n\n        return detections\n\n    def _simulate_detections(self, image: np.ndarray, confidence_threshold: float) -&gt; List[Dict]:\n        \"\"\"Simula detec\u00e7\u00f5es para demonstra\u00e7\u00e3o\"\"\"\n        height, width = image.shape[:2]\n\n        # Gerar detec\u00e7\u00f5es simuladas\n        num_detections = np.random.randint(1, 5)  # 1 a 4 detec\u00e7\u00f5es\n        detections = []\n\n        for _ in range(num_detections):\n            # Gerar bounding box aleat\u00f3ria\n            x = np.random.randint(0, width // 2)\n            y = np.random.randint(0, height // 2)\n            w = np.random.randint(width // 4, width // 2)\n            h = np.random.randint(height // 4, height // 2)\n\n            # Garantir que a bounding box esteja dentro dos limites\n            x = min(x, width - w)\n            y = min(y, height - h)\n\n            # Classe aleat\u00f3ria\n            class_id = np.random.randint(0, len(self.classes))\n            class_name = self.classes[class_id]\n\n            # Confian\u00e7a aleat\u00f3ria acima do threshold\n            confidence = np.random.uniform(confidence_threshold, 1.0)\n\n            detection = {\n                'class_id': class_id,\n                'class_name': class_name,\n                'confidence': confidence,\n                'bbox': [int(x), int(y), int(w), int(h)],\n                'coordinates': {\n                    'x_min': int(x),\n                    'y_min': int(y),\n                    'x_max': int(x + w),\n                    'y_max': int(y + h)\n                }\n            }\n\n            detections.append(detection)\n\n        return detections\n\n    def draw_detections(self, image: np.ndarray, detections: List[Dict], \n                       show_confidence: bool = True) -&gt; np.ndarray:\n        \"\"\"Desenha detec\u00e7\u00f5es na imagem\"\"\"\n        result_image = image.copy()\n\n        # Cores para desenho (BGR)\n        colors = np.random.uniform(0, 255, size=(len(self.classes), 3))\n\n        for detection in detections:\n            # Obter coordenadas\n            x, y, w, h = detection['bbox']\n\n            # Obter cor para esta classe\n            class_id = detection['class_id']\n            color = [int(c) for c in colors[class_id]]\n\n            # Desenhar bounding box\n            cv2.rectangle(result_image, (x, y), (x + w, y + h), color, 2)\n\n            # Preparar texto\n            label = detection['class_name']\n            if show_confidence:\n                label += f\" {detection['confidence']:.2f}\"\n\n            # Calcular tamanho do texto\n            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n\n            # Desenhar fundo para o texto\n            cv2.rectangle(result_image, (x, y - label_size[1] - 10), \n                         (x + label_size[0], y), color, -1)\n\n            # Colocar texto\n            cv2.putText(result_image, label, (x, y - 5), \n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n\n        return result_image\n\n    def get_model_info(self) -&gt; Dict:\n        \"\"\"Retorna informa\u00e7\u00f5es sobre o modelo\"\"\"\n        return {\n            'model_type': 'YOLO (simulado para demonstra\u00e7\u00e3o)',\n            'classes_count': len(self.classes),\n            'classes': self.classes,\n            'input_resolution': 'Vari\u00e1vel',\n            'real_model_available': self.model_path is not None\n        }\n</code></pre>"},{"location":"aulas/aula10/#implementar-quantizacao-e-otimizacao","title":"Implementar Quantiza\u00e7\u00e3o e Otimiza\u00e7\u00e3o","text":"<pre><code># src/models/optimization.py\nimport tensorflow as tf\nimport numpy as np\nfrom typing import Any\n\nclass ModelOptimizer:\n    def __init__(self, model):\n        self.model = model\n\n    def quantize_model(self) -&gt; bytes:\n        \"\"\"Converte modelo para vers\u00e3o quantizada (TensorFlow Lite)\"\"\"\n        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\n\n        # Configurar quantiza\u00e7\u00e3o\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n        # Converter modelo\n        quantized_model = converter.convert()\n\n        return quantized_model\n\n    def quantize_with_calibration(self, calibration_dataset) -&gt; bytes:\n        \"\"\"Quantiza modelo com calibra\u00e7\u00e3o\"\"\"\n        def representative_dataset():\n            for _ in range(100):  # Usar 100 amostras para calibra\u00e7\u00e3o\n                data = next(calibration_dataset)\n                yield [data]\n\n        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        converter.representative_dataset = representative_dataset\n\n        # Configurar infer\u00eancia de int8\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n        converter.inference_input_type = tf.int8\n        converter.inference_output_type = tf.int8\n\n        quantized_model = converter.convert()\n\n        return quantized_model\n\n    def benchmark_model(self, tflite_model: bytes, test_data: np.ndarray) -&gt; Dict[str, Any]:\n        \"\"\"Avalia desempenho do modelo TensorFlow Lite\"\"\"\n        import time\n\n        # Carregar modelo TFLite\n        interpreter = tf.lite.Interpreter(model_content=tflite_model)\n        interpreter.allocate_tensors()\n\n        # Obter detalhes de entrada e sa\u00edda\n        input_details = interpreter.get_input_details()\n        output_details = interpreter.get_output_details()\n\n        # Preparar dados de teste\n        input_shape = input_details[0]['shape']\n\n        # Medir tempo de infer\u00eancia\n        inference_times = []\n\n        for i in range(len(test_data)):\n            # Preparar entrada\n            input_data = test_data[i:i+1].astype(np.float32)\n\n            # Definir tensor de entrada\n            interpreter.set_tensor(input_details[0]['index'], input_data)\n\n            # Medir tempo de infer\u00eancia\n            start_time = time.time()\n            interpreter.invoke()\n            end_time = time.time()\n\n            inference_times.append(end_time - start_time)\n\n        avg_inference_time = np.mean(inference_times)\n        fps = 1.0 / avg_inference_time if avg_inference_time &gt; 0 else 0\n\n        return {\n            'avg_inference_time': avg_inference_time,\n            'fps': fps,\n            'model_size_mb': len(tflite_model) / (1024 * 1024),\n            'num_inferences': len(test_data)\n        }\n\n    def compare_models(self, original_model, quantized_model_bytes, test_data: np.ndarray) -&gt; Dict[str, Any]:\n        \"\"\"Compara desempenho de modelo original e quantizado\"\"\"\n        # Testar modelo original\n        original_start = time.time()\n        original_predictions = original_model.predict(test_data[:10])  # Testar com poucas amostras\n        original_time = time.time() - original_start\n\n        # Testar modelo quantizado\n        interpreter = tf.lite.Interpreter(model_content=quantized_model_bytes)\n        interpreter.allocate_tensors()\n\n        input_details = interpreter.get_input_details()\n        quantized_times = []\n\n        for i in range(min(10, len(test_data))):\n            input_data = test_data[i:i+1].astype(np.float32)\n            interpreter.set_tensor(input_details[0]['index'], input_data)\n\n            start = time.time()\n            interpreter.invoke()\n            quantized_times.append(time.time() - start)\n\n        avg_quantized_time = np.mean(quantized_times)\n\n        return {\n            'original_time': original_time,\n            'quantized_time': avg_quantized_time,\n            'speedup': original_time / avg_quantized_time if avg_quantized_time &gt; 0 else 0,\n            'original_size_mb': original_model.count_params() * 4 / (1024 * 1024),  # Aproximado\n            'quantized_size_mb': len(quantized_model_bytes) / (1024 * 1024)\n        }\n</code></pre>"},{"location":"aulas/aula10/#implementar-containerizacao","title":"Implementar Containeriza\u00e7\u00e3o","text":"<pre><code># src/deployment/containerizer.py\nimport docker\nimport os\nfrom typing import Dict, List\nimport tempfile\n\nclass ModelContainerizer:\n    def __init__(self):\n        self.client = docker.from_env()\n\n    def create_dockerfile_content(self, model_path: str, requirements: List[str] = None) -&gt; str:\n        \"\"\"Cria conte\u00fado do Dockerfile para modelo\"\"\"\n        if requirements is None:\n            requirements = [\n                'tensorflow==2.15.0',\n                'opencv-python==4.8.1.78',\n                'numpy==1.24.3',\n                'Pillow==10.1.0',\n                'fastapi==0.104.1',\n                'uvicorn[standard]==0.24.0'\n            ]\n\n        dockerfile_content = f\"\"\"\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Instalar depend\u00eancias do sistema\nRUN apt-get update &amp;&amp; apt-get install -y \\\\\n    gcc \\\\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copiar e instalar requisitos\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copiar modelo\nCOPY {model_path} /app/model/\n\n# Copiar c\u00f3digo da aplica\u00e7\u00e3o\nCOPY . /app/\n\n# Criar usu\u00e1rio n\u00e3o-root\nRUN useradd --create-home --shell /bin/bash appuser\nRUN chown -R appuser:appuser /app\nUSER appuser\n\n# Expor porta\nEXPOSE 8000\n\n# Comando para rodar a aplica\u00e7\u00e3o\nCMD [\"uvicorn\", \"api.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\"\"\"\n        return dockerfile_content\n\n    def create_requirements_txt(self, requirements: List[str]) -&gt; str:\n        \"\"\"Cria conte\u00fado do requirements.txt\"\"\"\n        return '\\n'.join(requirements)\n\n    def build_container(self, dockerfile_content: str, requirements_content: str, \n                       model_path: str, tag: str, build_context: str = \".\") -&gt; Dict[str, any]:\n        \"\"\"Constroi container Docker\"\"\"\n        # Criar arquivos tempor\u00e1rios\n        with tempfile.TemporaryDirectory() as temp_dir:\n            # Salvar Dockerfile\n            dockerfile_path = os.path.join(temp_dir, 'Dockerfile')\n            with open(dockerfile_path, 'w') as f:\n                f.write(dockerfile_content)\n\n            # Salvar requirements.txt\n            requirements_path = os.path.join(temp_dir, 'requirements.txt')\n            with open(requirements_path, 'w') as f:\n                f.write(requirements_content)\n\n            try:\n                # Fazer build da imagem\n                image, build_logs = self.client.images.build(\n                    path=temp_dir,\n                    tag=tag,\n                    rm=True,\n                    dockerfile='Dockerfile'\n                )\n\n                # Coletar logs de build\n                build_log_messages = []\n                for chunk in build_logs:\n                    if 'stream' in chunk:\n                        build_log_messages.append(chunk['stream'].strip())\n\n                return {\n                    'success': True,\n                    'image_id': image.id,\n                    'tag': tag,\n                    'logs': build_log_messages\n                }\n\n            except docker.errors.BuildError as e:\n                return {\n                    'success': False,\n                    'error': str(e),\n                    'logs': [line.get('stream', '').strip() for line in e.build_log]\n                }\n\n    def run_container(self, image_tag: str, ports: Dict[str, str] = None, \n                     environment: Dict[str, str] = None) -&gt; Dict[str, any]:\n        \"\"\"Executa container\"\"\"\n        try:\n            container = self.client.containers.run(\n                image=image_tag,\n                ports=ports,\n                environment=environment,\n                detach=True,\n                remove=False  # N\u00e3o remover automaticamente\n            )\n\n            return {\n                'success': True,\n                'container_id': container.id,\n                'status': container.status\n            }\n\n        except docker.errors.APIError as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n\n    def stop_container(self, container_id: str) -&gt; bool:\n        \"\"\"Para container\"\"\"\n        try:\n            container = self.client.containers.get(container_id)\n            container.stop()\n            container.remove()\n            return True\n        except docker.errors.NotFound:\n            return False\n        except docker.errors.APIError:\n            return False\n</code></pre>"},{"location":"aulas/aula10/#exemplo-de-uso-integrado","title":"Exemplo de Uso Integrado","text":"<p>```python</p>"},{"location":"aulas/aula10/#srcexamplesobject_detection_examplepy","title":"src/examples/object_detection_example.py","text":"<p>from models.yolo_detector import YOLODetector from models.optimization import ModelOptimizer from deployment.containerizer import ModelContainerizer import numpy as np import cv2 import matplotlib.pyplot as plt</p> <p>def demonstrate_object_detection():     \"\"\"Demonstra detec\u00e7\u00e3o de objetos com YOLO\"\"\"     print(\"=== Demonstra\u00e7\u00e3o de Detec\u00e7\u00e3o de Objetos ===\\n\")</p> <pre><code># Criar detector\ndetector = YOLODetector()\n\n# Obter informa\u00e7\u00f5es do modelo\nmodel_info = detector.get_model_info()\nprint(f\"Informa\u00e7\u00f5es do modelo: {model_info}\")\n\n# Criar imagem de exemplo\nsample_image = np.random.randint(0, 255, (416, 416, 3), dtype=np.uint8)\n\n# Detectar objetos\ndetections = detector.detect_objects(sample_image, confidence_threshold=0.5)\n\nprint(f\"\\nDetec\u00e7\u00f5es encontradas: {len(detections)}\")\nfor i, detection in enumerate(detections):\n    print(f\"  Detec\u00e7\u00e3o {i+1}: {detection['class_name']} \"\n          f\"(confian\u00e7a: {detection['confidence']:.2f})\")\n\n# Desenhar detec\u00e7\u00f5es\nresult_image = detector.draw_detections(sample_image, detections)\n\n# Visualizar resultados\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(sample_image)\nplt.title('Imagem Original')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(result_image)\nplt.title(f'Detec\u00e7\u00f5es ({len(detections)} encontradas)')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nreturn detector, detections\n</code></pre> <p>def demonstrate_model_optimization():     \"\"\"Demonstra otimiza\u00e7\u00e3o de modelo\"\"\"     print(\"\\n=== Demonstra\u00e7\u00e3o de Otimiza\u00e7\u00e3o de Modelo ===\\n\")</p> <pre><code># Criar modelo simples para demonstra\u00e7\u00e3o\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(224, 224, 3)),\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nprint(\"Modelo original criado\")\nprint(f\"Par\u00e2metros: {model.count_params():,}\")\n\n# Criar otimizador\noptimizer = ModelOptimizer(model)\n\n# Quantizar modelo\nprint(\"\\nQuantizando modelo...\")\nquantized_model = optimizer.quantize_model()\n\nprint(f\"Tamanho do modelo original: {model.count_params() * 4 / (1024*1024):.2f} MB\")\nprint(f\"Tamanho do modelo quantizado: {len(quantized_model) / (1024*1024):.2f} MB\")\n\n# Criar dados de teste\ntest_data = np.random.random((20, 224, 224, 3)).astype(np.float32)\n\n# Avaliar desempenho\nbenchmark_results = optimizer.benchmark_model(quantized_model, test_data)\n\nprint(f\"\\nResultados de benchmark:\")\nprint(f\"  Tempo m\u00e9dio de infer\u00eancia: {benchmark_results['avg_inference_time']:.4f}s\")\nprint(f\"  FPS: {benchmark_results['fps']:.2f}\")\nprint(f\"  Tamanho do modelo: {benchmark_results['model_size_mb']:.2f} MB\")\n\nreturn optimizer, quantized_model\n</code></pre> <p>def demonstrate_containerization():     \"\"\"Demonstra containeriza\u00e7\u00e3o de modelo\"\"\"     print(\"\\n=== Demonstra\u00e7\u00e3o de Containeriza\u00e7\u00e3o ===\\n\")</p> <pre><code># Criar containerizer\ncontainerizer = ModelContainerizer()\n\n# Criar conte\u00fado do Dockerfile\ndockerfile_content = containerizer.create_dockerfile_content(\n    model_path=\"models/\",\n    requirements=[\n        'tensorflow==2.15.0',\n        'opencv-python==4.8.1.78',\n        'numpy==1.24.3',\n        'fastapi==0.104.1',\n        'uvicorn[standard]==0.24.0'\n    ]\n)\n\n# Criar conte\u00fado do requirements.txt\nrequirements_content = containerizer.create_requirements_txt([\n    'tensorflow==2.15.0',\n    'opencv-python==4.8.1.78',\n    'numpy==1.24.3',\n    'fastapi==0.104.1',\n    'uvicorn[standard]==0.24.0'\n])\n\nprint(\"Dockerfile e requirements.txt criados\")\nprint(\"Para construir o container, voc\u00ea precisaria:\")\nprint(\"1. Ter os arquivos de modelo e c\u00f3digo\")\nprint(\"2. Executar o build com os caminhos corretos\")\nprint(\"3. Configurar as portas e vari\u00e1veis de ambiente\")\n\n# Exibir conte\u00fado do Dockerfile\nprint(\"\\nConte\u00fado do Dockerfile:\")\nprint(dockerfile_content)\n\nreturn containerizer\n</code></pre> <p>def analyze_performance():     \"\"\"Analisa desempenho em diferentes condi\u00e7\u00f5es\"\"\"     print(\"\\n=== An\u00e1lise de Desempenho ===\\n\")</p> <pre><code># Simular diferentes tamanhos de modelo\nmodel_sizes_mb = [1, 5, 10, 25, 50, 100]\noriginal_times = []\nquantized_times = []\n\nfor size in model_sizes_mb:\n    # Simular tempo baseado no tamanho\n    orig_time = size * 0.02  # Assumindo 0.02s por MB\n    quant_time = orig_time * 0.6  # Quantiza\u00e7\u00e3o melhora em 40%\n\n    original_times.append(orig_time)\n    quantized_times.append(quant_time)\n\n# Plotar gr\u00e1fico de compara\u00e7\u00e3o\nplt.figure(figsize=(10, 6))\nplt.plot(model_sizes_mb, original_times, 'o-', label='Modelo Original', linewidth=2)\nplt.plot(model_sizes_mb, quantized_times, 's-', label='Modelo Quantizado', linewidth=2)\nplt.xlabel('Tamanho do Modelo (MB)')\nplt.ylabel('Tempo de Infer\u00eancia (s)')\nplt.title('Compara\u00e7\u00e3o de Desempenho: Modelo Original vs Quantizado')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"An\u00e1lise mostrando benef\u00edcios da quantiza\u00e7\u00e3o para diferentes tamanhos de modelo\")\n</code></pre> <p>if name == \"main\":     # Executar demonstra\u00e7\u00f5es     detector, detections = demonstrate_object_detection()     optimizer, quantized_model = demonstrate_model_optimization()     containerizer = demonstrate_containerization()     analyze_performance()</p> <pre><code>print(\"\\n=== Resumo da Aula ===\")\nprint(\"Hoje aprendemos:\")\nprint(\"- Detec\u00e7\u00e3o de objetos com YOLO\")\nprint(\"- Bounding boxes e confian\u00e7a de detec\u00e7\u00f5es\")\nprint(\"- Quantiza\u00e7\u00e3o de modelos para otimiza\u00e7\u00e3o\")\nprint(\"- Containeriza\u00e7\u00e3o para deploy\")\nprint(\"- An\u00e1lise de desempenho em diferentes condi\u00e7\u00f5es\")\n</code></pre>"},{"location":"aulas/aula11/","title":"Aula 11 - Deploy de API e Modelos","text":""},{"location":"aulas/aula11/#objetivo-da-aula","title":"Objetivo da Aula","text":"<p>Implementar deploy de API e modelos de vis\u00e3o computacional, com foco em containeriza\u00e7\u00e3o, otimiza\u00e7\u00e3o de infer\u00eancia e estrat\u00e9gias de disponibiliza\u00e7\u00e3o de modelos em produ\u00e7\u00e3o.</p>"},{"location":"aulas/aula11/#conteudo-teorico","title":"Conte\u00fado Te\u00f3rico","text":""},{"location":"aulas/aula11/#estrategias-de-deploy","title":"Estrat\u00e9gias de Deploy","text":"<p>Existem v\u00e1rias abordagens para deploy de modelos de vis\u00e3o computacional:</p>"},{"location":"aulas/aula11/#1-deploy-em-nuvem","title":"1. Deploy em Nuvem","text":"<ul> <li>Vantagens: Escalabilidade, gerenciamento facilitado, seguran\u00e7a</li> <li>Desvantagens: Lat\u00eancia, custos cont\u00ednuos</li> <li>Plataformas: AWS SageMaker, Google Cloud ML Engine, Azure ML</li> </ul>"},{"location":"aulas/aula11/#2-deploy-em-edge","title":"2. Deploy em Edge","text":"<ul> <li>Vantagens: Baixa lat\u00eancia, privacidade, efici\u00eancia de banda</li> <li>Desvantagens: Recursos limitados, manuten\u00e7\u00e3o distribu\u00edda</li> <li>Aplica\u00e7\u00f5es: IoT, c\u00e2meras inteligentes, dispositivos m\u00f3veis</li> </ul>"},{"location":"aulas/aula11/#3-deploy-on-premises","title":"3. Deploy On-Premises","text":"<ul> <li>Vantagens: Controle total, seguran\u00e7a de dados, custos previs\u00edveis</li> <li>Desvantagens: Infraestrutura pr\u00f3pria, manuten\u00e7\u00e3o</li> <li>Aplica\u00e7\u00f5es: Ambientes corporativos, setor p\u00fablico</li> </ul>"},{"location":"aulas/aula11/#containerizacao-com-docker","title":"Containeriza\u00e7\u00e3o com Docker","text":"<p>Docker permite empacotar aplica\u00e7\u00f5es com todas as depend\u00eancias necess\u00e1rias, garantindo consist\u00eancia entre ambientes de desenvolvimento, teste e produ\u00e7\u00e3o.</p>"},{"location":"aulas/aula11/#otimizacao-de-inferencia","title":"Otimiza\u00e7\u00e3o de Infer\u00eancia","text":"<p>T\u00e9cnicas para melhorar o desempenho de modelos em produ\u00e7\u00e3o:</p> <ul> <li>Quantiza\u00e7\u00e3o: Redu\u00e7\u00e3o da precis\u00e3o num\u00e9rica para diminuir tamanho e aumentar velocidade</li> <li>Podas (Pruning): Remo\u00e7\u00e3o de conex\u00f5es irrelevantes</li> <li>Knowledge Distillation: Cria\u00e7\u00e3o de modelos menores que aprendem com modelos maiores</li> <li>TensorRT (NVIDIA): Otimiza\u00e7\u00e3o para GPUs</li> </ul>"},{"location":"aulas/aula11/#monitoramento-e-observabilidade","title":"Monitoramento e Observabilidade","text":"<p>Fundamental para manter modelos em produ\u00e7\u00e3o: - M\u00e9tricas de desempenho: Lat\u00eancia, throughput, acur\u00e1cia - Logs: Rastreamento de requisi\u00e7\u00f5es e erros - Alertas: Notifica\u00e7\u00f5es de anomalias</p>"},{"location":"aulas/aula11/#atividade-pratica","title":"Atividade Pr\u00e1tica","text":""},{"location":"aulas/aula11/#implementar-script-de-deploy","title":"Implementar Script de Deploy","text":"<pre><code># src/deployment/deploy_manager.py\nimport docker\nimport os\nimport subprocess\nimport yaml\nfrom typing import Dict, List, Optional\nimport json\nimport requests\n\nclass DeploymentManager:\n    def __init__(self, config_file: str = None):\n        self.client = docker.from_env()\n        self.config = self._load_config(config_file) if config_file else {}\n        self.containers = {}\n\n    def _load_config(self, config_file: str) -&gt; Dict:\n        \"\"\"Carrega configura\u00e7\u00e3o de deploy\"\"\"\n        with open(config_file, 'r') as f:\n            return yaml.safe_load(f)\n\n    def build_image(self, dockerfile_path: str, context_path: str, image_name: str) -&gt; bool:\n        \"\"\"Constroi imagem Docker\"\"\"\n        try:\n            print(f\"Construindo imagem: {image_name}\")\n            image, build_logs = self.client.images.build(\n                path=context_path,\n                dockerfile=dockerfile_path,\n                tag=image_name,\n                rm=True\n            )\n\n            # Exibir logs de build\n            for chunk in build_logs:\n                if 'stream' in chunk:\n                    print(chunk['stream'].strip())\n\n            print(f\"Imagem {image_name} constru\u00edda com sucesso!\")\n            return True\n\n        except docker.errors.BuildError as e:\n            print(f\"Erro ao construir imagem: {e}\")\n            for line in e.build_log:\n                print(line.get('stream', ''))\n            return False\n\n    def run_container(self, image_name: str, container_name: str, \n                     ports: Dict[str, str] = None, \n                     environment: Dict[str, str] = None,\n                     volumes: Dict[str, dict] = None) -&gt; Optional[str]:\n        \"\"\"Executa container\"\"\"\n        try:\n            print(f\"Executando container: {container_name}\")\n\n            container = self.client.containers.run(\n                image=image_name,\n                name=container_name,\n                ports=ports,\n                environment=environment,\n                volumes=volumes,\n                detach=True,\n                auto_remove=False\n            )\n\n            self.containers[container_name] = container\n            print(f\"Container {container_name} iniciado com ID: {container.id}\")\n            return container.id\n\n        except docker.errors.APIError as e:\n            print(f\"Erro ao executar container: {e}\")\n            return None\n\n    def stop_container(self, container_name: str) -&gt; bool:\n        \"\"\"Para e remove container\"\"\"\n        try:\n            if container_name in self.containers:\n                container = self.containers[container_name]\n                container.stop()\n                container.remove()\n                del self.containers[container_name]\n                print(f\"Container {container_name} parado e removido\")\n                return True\n            else:\n                print(f\"Container {container_name} n\u00e3o encontrado\")\n                return False\n        except docker.errors.APIError as e:\n            print(f\"Erro ao parar container: {e}\")\n            return False\n\n    def check_health(self, container_name: str, health_endpoint: str = \"/health\") -&gt; Dict:\n        \"\"\"Verifica sa\u00fade do servi\u00e7o no container\"\"\"\n        try:\n            container = self.client.containers.get(container_name)\n            port_bindings = container.attrs['NetworkSettings']['Ports']\n\n            # Encontrar porta mapeada\n            mapped_port = None\n            for container_port, host_mapping in port_bindings.items():\n                if container_port == '8000/tcp' and host_mapping:\n                    mapped_port = host_mapping[0]['HostPort']\n                    break\n\n            if mapped_port:\n                url = f\"http://localhost:{mapped_port}{health_endpoint}\"\n                response = requests.get(url, timeout=5)\n\n                return {\n                    'status': 'healthy' if response.status_code == 200 else 'unhealthy',\n                    'response_code': response.status_code,\n                    'response': response.json() if response.content else {}\n                }\n            else:\n                return {'status': 'unknown', 'error': 'Port not mapped'}\n\n        except requests.exceptions.RequestException as e:\n            return {'status': 'unhealthy', 'error': str(e)}\n        except docker.errors.NotFound:\n            return {'status': 'stopped', 'error': 'Container not found'}\n\n    def scale_service(self, service_name: str, replicas: int):\n        \"\"\"Escala servi\u00e7o (requer swarm mode ou kubernetes)\"\"\"\n        # Esta \u00e9 uma implementa\u00e7\u00e3o simplificada\n        # Em produ\u00e7\u00e3o, usaria Kubernetes ou Docker Swarm\n        print(f\"Escalando servi\u00e7o {service_name} para {replicas} r\u00e9plicas\")\n\n        # Em uma implementa\u00e7\u00e3o real, usaria:\n        # - Kubernetes deployments\n        # - Docker Swarm services\n        # - AWS ECS\n        pass\n\n    def deploy_model_api(self, model_path: str, api_port: int = 8000) -&gt; bool:\n        \"\"\"Implementa deploy completo de API de modelo\"\"\"\n        image_name = \"cv-model-api:latest\"\n        container_name = \"cv-model-api-container\"\n\n        # 1. Criar Dockerfile\n        dockerfile_content = f\"\"\"\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Instalar depend\u00eancias do sistema\nRUN apt-get update &amp;&amp; apt-get install -y \\\\\n    gcc \\\\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copiar e instalar requisitos\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copiar modelo\nCOPY {model_path} /app/models/\n\n# Copiar c\u00f3digo da aplica\u00e7\u00e3o\nCOPY app/ /app/app/\nCOPY src/ /app/src/\n\n# Expor porta\nEXPOSE {api_port}\n\n# Comando para rodar a aplica\u00e7\u00e3o\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"{api_port}\"]\n\"\"\"\n\n        # Salvar Dockerfile temporariamente\n        with open('Dockerfile', 'w') as f:\n            f.write(dockerfile_content)\n\n        # Salvar requirements.txt\n        requirements = [\n            'fastapi==0.104.1',\n            'uvicorn[standard]==0.24.0',\n            'tensorflow==2.15.0',\n            'opencv-python==4.8.1.78',\n            'numpy==1.24.3',\n            'Pillow==10.1.0',\n            'pydantic==2.5.0',\n            'pydantic-settings==2.1.0',\n            'python-multipart==0.0.6'\n        ]\n\n        with open('requirements.txt', 'w') as f:\n            f.write('\\n'.join(requirements))\n\n        # 2. Construir imagem\n        if not self.build_image('Dockerfile', '.', image_name):\n            return False\n\n        # 3. Executar container\n        ports = {f'{api_port}/tcp': api_port}\n        environment = {\n            'MODEL_PATH': f'/app/models/{os.path.basename(model_path)}',\n            'DEBUG': 'false'\n        }\n\n        container_id = self.run_container(\n            image_name=image_name,\n            container_name=container_name,\n            ports=ports,\n            environment=environment\n        )\n\n        if not container_id:\n            return False\n\n        # 4. Verificar sa\u00fade\n        import time\n        time.sleep(10)  # Aguardar inicializa\u00e7\u00e3o\n\n        health_status = self.check_health(container_name)\n        print(f\"Status de sa\u00fade: {health_status}\")\n\n        return health_status['status'] == 'healthy'\n</code></pre>"},{"location":"aulas/aula11/#implementar-otimizacao-de-inferencia","title":"Implementar Otimiza\u00e7\u00e3o de Infer\u00eancia","text":"<pre><code># src/deployment/inference_optimizer.py\nimport tensorflow as tf\nimport numpy as np\nfrom typing import Any, Dict, List\nimport time\n\nclass InferenceOptimizer:\n    def __init__(self, model):\n        self.model = model\n        self.optimized_model = None\n\n    def quantize_model(self) -&gt; bytes:\n        \"\"\"Quantiza modelo para infer\u00eancia otimizada\"\"\"\n        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        quantized_model = converter.convert()\n        return quantized_model\n\n    def quantize_int8(self, representative_dataset) -&gt; bytes:\n        \"\"\"Quantiza modelo para INT8 com dataset representativo\"\"\"\n        def representative_data_gen():\n            for batch in representative_dataset.take(100):  # Usar 100 batches\n                yield [batch]\n\n        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        converter.representative_dataset = representative_data_gen\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n        converter.inference_input_type = tf.int8\n        converter.inference_output_type = tf.int8\n\n        quantized_model = converter.convert()\n        return quantized_model\n\n    def create_tensorrt_model(self, input_shape: tuple) -&gt; Any:\n        \"\"\"Cria modelo otimizado com TensorRT (requer GPU NVIDIA)\"\"\"\n        try:\n            from tensorflow.python.compiler.tensorrt import trt_convert as trt\n\n            conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n                precision_mode=trt.TrtPrecisionMode.FP16,\n                max_workspace_size_bytes=8000000000  # 8GB\n            )\n\n            converter = trt.TrtGraphConverterV2(\n                input_saved_model_dir=None,  # Caminho para modelo salvo\n                conversion_params=conversion_params\n            )\n\n            # Esta \u00e9 uma implementa\u00e7\u00e3o conceitual\n            # Em pr\u00e1tica, precisaria de um modelo salvo\n            print(\"TensorRT optimization requires a saved model\")\n            return None\n\n        except ImportError:\n            print(\"TensorRT n\u00e3o dispon\u00edvel\")\n            return None\n\n    def benchmark_models(self, original_model, tflite_model_bytes, \n                        test_data: np.ndarray, num_runs: int = 100) -&gt; Dict[str, Any]:\n        \"\"\"Compara desempenho de modelos\"\"\"\n        # Benchmark modelo original\n        original_times = []\n        for _ in range(num_runs):\n            start = time.time()\n            _ = original_model.predict(test_data[:1])\n            original_times.append(time.time() - start)\n\n        # Benchmark modelo TFLite\n        interpreter = tf.lite.Interpreter(model_content=tflite_model_bytes)\n        interpreter.allocate_tensors()\n\n        input_details = interpreter.get_input_details()\n        output_details = interpreter.get_output_details()\n\n        tflite_times = []\n        for _ in range(num_runs):\n            # Preparar entrada\n            input_data = test_data[:1].astype(np.float32)\n\n            interpreter.set_tensor(input_details[0]['index'], input_data)\n\n            start = time.time()\n            interpreter.invoke()\n            tflite_times.append(time.time() - start)\n\n        return {\n            'original': {\n                'avg_time': np.mean(original_times),\n                'std_time': np.std(original_times),\n                'min_time': np.min(original_times),\n                'max_time': np.max(original_times),\n                'throughput': len(test_data) / sum(original_times)\n            },\n            'tflite': {\n                'avg_time': np.mean(tflite_times),\n                'std_time': np.std(tflite_times),\n                'min_time': np.min(tflite_times),\n                'max_time': np.max(tflite_times),\n                'throughput': len(test_data) / sum(tflite_times)\n            },\n            'improvement': np.mean(original_times) / np.mean(tflite_times)\n        }\n\n    def optimize_for_device(self, device_type: str = 'mobile') -&gt; bytes:\n        \"\"\"Otimiza modelo para dispositivo espec\u00edfico\"\"\"\n        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\n\n        if device_type == 'mobile':\n            # Otimiza\u00e7\u00f5es para dispositivos m\u00f3veis\n            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n            converter.target_spec.supported_types = [tf.float16]  # Meia precis\u00e3o\n        elif device_type == 'edge':\n            # Otimiza\u00e7\u00f5es para dispositivos edge\n            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n            converter.target_spec.supported_ops = [\n                tf.lite.OpsSet.TFLITE_BUILTINS,\n                tf.lite.OpsSet.SELECT_TF_OPS  # Para opera\u00e7\u00f5es n\u00e3o suportadas\n            ]\n        elif device_type == 'server':\n            # Otimiza\u00e7\u00f5es para servidores\n            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n            # Pode adicionar outras otimiza\u00e7\u00f5es espec\u00edficas\n\n        optimized_model = converter.convert()\n        return optimized_model\n\n    def create_optimized_pipeline(self, model, input_shape: tuple, \n                                 device_target: str = 'mobile') -&gt; Any:\n        \"\"\"Cria pipeline otimizado para infer\u00eancia\"\"\"\n        # Converter para TensorFlow Lite otimizado\n        tflite_model = self.optimize_for_device(device_target)\n\n        # Carregar modelo otimizado\n        interpreter = tf.lite.Interpreter(model_content=tflite_model)\n        interpreter.allocate_tensors()\n\n        # Obter detalhes de entrada e sa\u00edda\n        input_details = interpreter.get_input_details()\n        output_details = interpreter.get_output_details()\n\n        class OptimizedModel:\n            def __init__(self, interpreter, input_details, output_details):\n                self.interpreter = interpreter\n                self.input_details = input_details\n                self.output_details = output_details\n\n            def predict(self, input_data):\n                # Preparar entrada\n                input_data = input_data.astype(np.float32)\n\n                # Definir tensor de entrada\n                self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n\n                # Executar infer\u00eancia\n                self.interpreter.invoke()\n\n                # Obter sa\u00edda\n                output = self.interpreter.get_tensor(self.output_details[0]['index'])\n\n                return output\n\n        return OptimizedModel(interpreter, input_details, output_details)\n</code></pre>"},{"location":"aulas/aula11/#implementar-monitoramento","title":"Implementar Monitoramento","text":"<pre><code># src/deployment/monitoring.py\nimport psutil\nimport GPUtil\nimport time\nimport threading\nfrom typing import Dict, Callable\nimport json\nimport requests\nfrom datetime import datetime\n\nclass ModelMonitor:\n    def __init__(self, api_endpoint: str = \"http://localhost:8000\"):\n        self.api_endpoint = api_endpoint\n        self.metrics = {\n            'cpu_usage': [],\n            'memory_usage': [],\n            'gpu_usage': [],\n            'request_latency': [],\n            'throughput': [],\n            'error_rate': []\n        }\n        self.monitoring = False\n        self.monitor_thread = None\n\n    def start_monitoring(self, interval: float = 1.0):\n        \"\"\"Inicia monitoramento em thread separada\"\"\"\n        self.monitoring = True\n        self.monitor_thread = threading.Thread(target=self._monitor_loop, args=(interval,))\n        self.monitor_thread.start()\n        print(\"Monitoramento iniciado\")\n\n    def stop_monitoring(self):\n        \"\"\"Para monitoramento\"\"\"\n        self.monitoring = False\n        if self.monitor_thread:\n            self.monitor_thread.join()\n        print(\"Monitoramento parado\")\n\n    def _monitor_loop(self, interval: float):\n        \"\"\"Loop de monitoramento\"\"\"\n        while self.monitoring:\n            # Coletar m\u00e9tricas do sistema\n            cpu_percent = psutil.cpu_percent(interval=1)\n            memory_percent = psutil.virtual_memory().percent\n\n            # Coletar m\u00e9tricas de GPU se dispon\u00edvel\n            gpu_percent = 0\n            gpus = GPUtil.getGPUs()\n            if gpus:\n                gpu_percent = gpus[0].load * 100\n\n            # Adicionar m\u00e9tricas\n            self.metrics['cpu_usage'].append({\n                'timestamp': datetime.now().isoformat(),\n                'value': cpu_percent\n            })\n\n            self.metrics['memory_usage'].append({\n                'timestamp': datetime.now().isoformat(),\n                'value': memory_percent\n            })\n\n            self.metrics['gpu_usage'].append({\n                'timestamp': datetime.now().isoformat(),\n                'value': gpu_percent\n            })\n\n            time.sleep(interval)\n\n    def test_api_performance(self, endpoint: str, num_requests: int = 100) -&gt; Dict:\n        \"\"\"Testa desempenho da API\"\"\"\n        latencies = []\n        errors = 0\n\n        for i in range(num_requests):\n            start_time = time.time()\n            try:\n                # Fazer requisi\u00e7\u00e3o de teste (exemplo para endpoint de classifica\u00e7\u00e3o)\n                response = requests.post(\n                    f\"{self.api_endpoint}{endpoint}\",\n                    json={'image_base64': 'dummy'}  # Isso falhar\u00e1, mas mede lat\u00eancia\n                )\n                latency = time.time() - start_time\n                latencies.append(latency)\n\n                if response.status_code != 200:\n                    errors += 1\n\n            except requests.exceptions.RequestException:\n                errors += 1\n                latencies.append(time.time() - start_time)  # Contar erro na lat\u00eancia tamb\u00e9m\n\n        return {\n            'avg_latency': np.mean(latencies) if latencies else 0,\n            'p95_latency': np.percentile(latencies, 95) if latencies else 0,\n            'p99_latency': np.percentile(latencies, 99) if latencies else 0,\n            'error_rate': errors / num_requests if num_requests &gt; 0 else 0,\n            'throughput': num_requests / sum(latencies) if latencies and sum(latencies) &gt; 0 else 0\n        }\n\n    def get_current_metrics(self) -&gt; Dict:\n        \"\"\"Obt\u00e9m m\u00e9tricas atuais\"\"\"\n        current_cpu = psutil.cpu_percent(interval=1)\n        current_memory = psutil.virtual_memory().percent\n\n        gpus = GPUtil.getGPUs()\n        current_gpu = gpus[0].load * 100 if gpus else 0\n\n        return {\n            'timestamp': datetime.now().isoformat(),\n            'cpu_usage': current_cpu,\n            'memory_usage': current_memory,\n            'gpu_usage': current_gpu,\n            'active_monitoring': self.monitoring\n        }\n\n    def export_metrics(self, filename: str = 'metrics.json'):\n        \"\"\"Exporta m\u00e9tricas para arquivo\"\"\"\n        with open(filename, 'w') as f:\n            json.dump(self.metrics, f, indent=2, default=str)\n        print(f\"M\u00e9tricas exportadas para {filename}\")\n\n    def setup_alerts(self, thresholds: Dict[str, float], callback: Callable):\n        \"\"\"Configura sistema de alertas\"\"\"\n        # Este \u00e9 um exemplo simplificado\n        # Em produ\u00e7\u00e3o, usaria um sistema mais robusto\n        def alert_checker():\n            while self.monitoring:\n                current_metrics = self.get_current_metrics()\n\n                for metric, threshold in thresholds.items():\n                    if metric in current_metrics and current_metrics[metric] &gt; threshold:\n                        alert_msg = f\"ALERTA: {metric} excedeu threshold! Valor: {current_metrics[metric]}, Limite: {threshold}\"\n                        callback(alert_msg)\n\n                time.sleep(10)  # Verificar a cada 10 segundos\n\n        alert_thread = threading.Thread(target=alert_checker)\n        alert_thread.daemon = True\n        alert_thread.start()\n</code></pre>"},{"location":"aulas/aula11/#exemplo-de-uso-completo","title":"Exemplo de Uso Completo","text":"<p>```python</p>"},{"location":"aulas/aula11/#srcexamplesdeployment_examplepy","title":"src/examples/deployment_example.py","text":"<p>from deployment.deploy_manager import DeploymentManager from deployment.inference_optimizer import InferenceOptimizer from deployment.monitoring import ModelMonitor import tensorflow as tf import numpy as np</p> <p>def demonstrate_full_deployment():     \"\"\"Demonstra deploy completo de modelo de CV\"\"\"     print(\"=== Demonstra\u00e7\u00e3o de Deploy Completo ===\\n\")</p> <pre><code># 1. Criar modelo de exemplo\nprint(\"Criando modelo de exemplo...\")\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(224, 224, 3)),\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nprint(\"Modelo criado com sucesso!\")\n\n# 2. Otimizar modelo para infer\u00eancia\nprint(\"\\nOtimizando modelo para infer\u00eancia...\")\noptimizer = InferenceOptimizer(model)\n\n# Quantizar modelo\nquantized_model = optimizer.quantize_model()\nprint(f\"Modelo quantizado: {len(quantized_model) / (1024*1024):.2f} MB\")\n\n# Criar dados de teste\ntest_data = np.random.random((10, 224, 224, 3)).astype(np.float32)\n\n# Benchmark\nbenchmark_results = optimizer.benchmark_models(model, quantized_model, test_data, num_runs=10)\n\nprint(f\"\\nResultados de benchmark:\")\nprint(f\"  Modelo original - M\u00e9dia: {benchmark_results['original']['avg_time']:.4f}s\")\nprint(f\"  Modelo TFLite - M\u00e9dia: {benchmark_results['tflite']['avg_time']:.4f}s\")\nprint(f\"  Melhoria: {benchmark_results['improvement']:.2f}x\")\n\n# 3. Preparar para deploy\nprint(\"\\nPreparando para deploy...\")\n\n# Criar deploy manager\ndeploy_manager = DeploymentManager()\n\n# Nota: O deploy real requer arquivos reais e infraestrutura\n# Esta \u00e9 uma demonstra\u00e7\u00e3o conceitual\nprint(\"Deploy manager inicializado\")\n\n# 4. Configurar monitoramento\nprint(\"\\nConfigurando monitoramento...\")\nmonitor = ModelMonitor(api_endpoint=\"http://localhost:8000\")\n\n# Iniciar monitoramento\nmonitor.start_monitoring(interval=2.0)\n\n# Obter m\u00e9tricas atuais\ncurrent_metrics = monitor.get_current_metrics()\nprint(f\"M\u00e9tricas atuais: {current_metrics}\")\n\n# Simular teste de performance\nperf_results = monitor.test_api_performance('/vision/classify', num_requests=10)\nprint(f\"Resultados de performance simulada: {perf_results}\")\n\n# Parar monitoramento\nmonitor.stop_monitoring()\n\nprint(\"\\nDemonstra\u00e7\u00e3o de deploy conclu\u00edda!\")\nprint(\"Pr\u00f3ximos passos para deploy real:\")\nprint(\"1. Preparar Dockerfile com modelo e depend\u00eancias\")\nprint(\"2. Construir imagem Docker\")\nprint(\"3. Executar container com configura\u00e7\u00f5es de produ\u00e7\u00e3o\")\nprint(\"4. Configurar balanceamento de carga e escalabilidade\")\nprint(\"5. Implementar monitoramento cont\u00ednuo\")\nprint(\"6. Configurar alertas e recupera\u00e7\u00e3o de falhas\")\n</code></pre> <p>def compare_deployment_strategies():     \"\"\"Compara diferentes estrat\u00e9gias de deployment\"\"\"     strategies = {         'Nuvem': {             'vantagens': ['Escalabilidade autom\u00e1tica', 'Gerenciamento facilitado', 'Alta disponibilidade'],             'desvantagens': ['Lat\u00eancia de rede', 'Custos cont\u00ednuos', 'Depend\u00eancia de provedor'],             'uso': 'Aplica\u00e7\u00f5es com demanda vari\u00e1vel'         },         'Edge': {             'vantagens': ['Baixa lat\u00eancia', 'Privacidade dos dados', 'Efici\u00eancia de banda'],             'desvantagens': ['Recursos limitados', 'Dif\u00edcil manuten\u00e7\u00e3o', 'Atualiza\u00e7\u00f5es complexas'],             'uso': 'IoT, c\u00e2meras inteligentes, dispositivos m\u00f3veis'         },         'On-Premises': {             'vantagens': ['Controle total', 'Seguran\u00e7a de dados', 'Custos previs\u00edveis'],             'desvantagens': ['Infraestrutura pr\u00f3pria', 'Manuten\u00e7\u00e3o', 'Escalabilidade limitada'],             'uso': 'Ambientes corporativos, setor p\u00fablico'         }     }</p> <pre><code>print(\"\\n=== Compara\u00e7\u00e3o de Estrat\u00e9gias de Deployment ===\\n\")\n\nfor strategy, details in strategies.items():\n    print(f\"{strategy}:\")\n    print(f\"  Vantagens: {', '.join(details['vantagens'])}\")\n    print(f\"  Desvantagens: {', '.join(details['desvantagens'])}\")\n    print(f\"  Uso recomendado: {details['uso']}\\n\")\n</code></pre> <p>def optimize_for_production():     \"\"\"Demonstra otimiza\u00e7\u00f5es para produ\u00e7\u00e3o\"\"\"     print(\"=== Otimiza\u00e7\u00f5es para Produ\u00e7\u00e3o ===\\n\")</p> <pre><code>optimizations = [\n    {\n        'nome': 'Quantiza\u00e7\u00e3o',\n        'descricao': 'Redu\u00e7\u00e3o da precis\u00e3o num\u00e9rica para diminuir tamanho e aumentar velocidade',\n        'ganho': '2-4x redu\u00e7\u00e3o de tamanho, 2-3x aumento de velocidade'\n    },\n    {\n        'nome': 'Podas (Pruning)', \n        'descricao': 'Remo\u00e7\u00e3o de conex\u00f5es irrelevantes para reduzir complexidade',\n        'ganho': 'At\u00e9 50% redu\u00e7\u00e3o de par\u00e2metros com m\u00ednima perda de acur\u00e1cia'\n    },\n    {\n        'nome': 'TensorRT (NVIDIA)',\n        'descricao': 'Otimiza\u00e7\u00e3o espec\u00edfica para GPUs NVIDIA',\n        'ganho': '2-7x aumento de velocidade em GPUs compat\u00edveis'\n    },\n    {\n        'nome': 'Model Distillation',\n        'descricao': 'Cria\u00e7\u00e3o de modelos menores que aprendem com modelos maiores',\n        'ganho': 'Modelos 10-100x menores com desempenho similar'\n    }\n]\n\nfor opt in optimizations:\n    print(f\"{opt['nome']}:\")\n    print(f\"  Descri\u00e7\u00e3o: {opt['descricao']}\")\n    print(f\"  Ganho esperado: {opt['ganho']}\\n\")\n</code></pre> <p>if name == \"main\":     # Executar demonstra\u00e7\u00f5es     demonstrate_full_deployment()     compare_deployment_strategies()     optimize_for_production()</p> <pre><code>print(\"\\n=== Resumo da Aula ===\")\nprint(\"Hoje aprendemos:\")\nprint(\"- Estrat\u00e9gias de deploy para modelos de CV\")\nprint(\"- Containeriza\u00e7\u00e3o com Docker\")\nprint(\"- Otimiza\u00e7\u00e3o de infer\u00eancia\")\nprint(\"- Monitoramento e observabilidade\")\nprint(\"- Compara\u00e7\u00e3o de abordagens de deploy\")\nprint(\"- T\u00e9cnicas de otimiza\u00e7\u00e3o para produ\u00e7\u00e3o\")\n</code></pre>"},{"location":"aulas/aula2/","title":"Aula 2 - Processamento de Imagem Aplicado","text":""},{"location":"aulas/aula2/#objetivo-da-aula","title":"Objetivo da Aula","text":"<p>Aplicar t\u00e9cnicas de processamento de imagem em um contexto pr\u00e1tico, criando fun\u00e7\u00f5es reutiliz\u00e1veis e organiza\u00e7\u00e3o modular para o pipeline de vis\u00e3o computacional.</p>"},{"location":"aulas/aula2/#conteudo-teorico","title":"Conte\u00fado Te\u00f3rico","text":""},{"location":"aulas/aula2/#espacos-de-cor","title":"Espa\u00e7os de Cor","text":"<p>O espa\u00e7o de cor define como as cores s\u00e3o representadas em uma imagem digital. Cada espa\u00e7o tem suas vantagens para diferentes tipos de opera\u00e7\u00f5es:</p> <ul> <li>RGB (Red, Green, Blue): Modelo aditivo baseado na combina\u00e7\u00e3o de luz vermelha, verde e azul. Adequado para dispositivos de exibi\u00e7\u00e3o.</li> <li>HSV (Hue, Saturation, Value): Separa cor, satura\u00e7\u00e3o e brilho. Ideal para opera\u00e7\u00f5es baseadas em cor espec\u00edfica.</li> <li>LAB: Separa lumin\u00e2ncia de cor, \u00fatil para corre\u00e7\u00f5es de cor independentes da ilumina\u00e7\u00e3o.</li> </ul>"},{"location":"aulas/aula2/#histogramas","title":"Histogramas","text":"<p>O histograma de uma imagem mostra a distribui\u00e7\u00e3o de intensidades de pixel. \u00c9 \u00fatil para an\u00e1lise e corre\u00e7\u00e3o de contraste, permitindo t\u00e9cnicas como equaliza\u00e7\u00e3o de histograma.</p>"},{"location":"aulas/aula2/#filtros-e-convolucao","title":"Filtros e Convolu\u00e7\u00e3o","text":"<p>A convolu\u00e7\u00e3o \u00e9 uma opera\u00e7\u00e3o fundamental que aplica um kernel (filtro) sobre uma imagem. Tipos comuns:</p> <ul> <li>Filtros Passa-Baixa: Suavizam a imagem, reduzindo ru\u00eddo (m\u00e9dia, gaussiano, mediana)</li> <li>Filtros Passa-Alta: Real\u00e7am bordas e detalhes (Sobel, Laplaciano, Canny)</li> </ul>"},{"location":"aulas/aula2/#operacoes-morfologicas","title":"Opera\u00e7\u00f5es Morfol\u00f3gicas","text":"<p>Opera\u00e7\u00f5es que manipulam a estrutura geom\u00e9trica dos objetos em uma imagem bin\u00e1ria:</p> <ul> <li>Eros\u00e3o: Reduz o tamanho dos objetos brancos</li> <li>Dilata\u00e7\u00e3o: Aumenta o tamanho dos objetos brancos</li> <li>Abertura: Eros\u00e3o seguida de dilata\u00e7\u00e3o (remove ru\u00eddo)</li> <li>Fechamento: Dilata\u00e7\u00e3o seguida de eros\u00e3o (fecha lacunas)</li> </ul>"},{"location":"aulas/aula2/#atividade-pratica","title":"Atividade Pr\u00e1tica","text":""},{"location":"aulas/aula2/#implementar-pipeline-de-pre-processamento","title":"Implementar Pipeline de Pr\u00e9-processamento","text":"<p>Vamos expandir nosso projeto com m\u00f3dulos espec\u00edficos para pr\u00e9-processamento:</p> <pre><code># src/preprocessing/color_spaces.py\nimport cv2\nimport numpy as np\n\ndef rgb_to_hsv(image):\n    \"\"\"Converte imagem de RGB para HSV\"\"\"\n    return cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n\ndef rgb_to_grayscale(image):\n    \"\"\"Converte imagem de RGB para escala de cinza\"\"\"\n    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\ndef adjust_brightness_contrast(image, brightness=0, contrast=1):\n    \"\"\"Ajusta brilho e contraste da imagem\"\"\"\n    # Primeiro ajustar contraste, depois brilho\n    adjusted = image * contrast + brightness\n    # Garantir que os valores estejam entre 0 e 255\n    adjusted = np.clip(adjusted, 0, 255)\n    return adjusted.astype(np.uint8)\n\ndef equalize_histogram(image):\n    \"\"\"Equaliza histograma de imagem em escala de cinza\"\"\"\n    if len(image.shape) == 3:\n        # Se for imagem colorida, converter para escala de cinza\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        equalized = cv2.equalizeHist(gray)\n        return equalized\n    else:\n        return cv2.equalizeHist(image)\n\ndef equalize_histogram_color(image):\n    \"\"\"Equaliza histograma de imagem colorida (canal V do HSV)\"\"\"\n    # Converter para HSV\n    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n\n    # Equalizar canal V (valor/brilho)\n    hsv[:,:,2] = cv2.equalizeHist(hsv[:,:,2])\n\n    # Converter de volta para RGB\n    return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n</code></pre>"},{"location":"aulas/aula2/#implementar-filtros-e-convolucao","title":"Implementar Filtros e Convolu\u00e7\u00e3o","text":"<pre><code># src/preprocessing/filters.py\nimport cv2\nimport numpy as np\n\ndef apply_blur(image, kernel_size=(5, 5)):\n    \"\"\"Aplica filtro de desfoque\"\"\"\n    return cv2.blur(image, kernel_size)\n\ndef apply_gaussian_blur(image, kernel_size=(5, 5), sigma_x=0):\n    \"\"\"Aplica filtro de desfoque gaussiano\"\"\"\n    return cv2.GaussianBlur(image, kernel_size, sigma_x)\n\ndef apply_median_blur(image, kernel_size=5):\n    \"\"\"Aplica filtro de mediana (bom para remover ru\u00eddo sal e pimenta)\"\"\"\n    return cv2.medianBlur(image, kernel_size)\n\ndef detect_edges_sobel(image):\n    \"\"\"Detecta bordas usando o operador Sobel\"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n\n    # Gradientes X e Y\n    sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n    sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n\n    # Magnitude do gradiente\n    sobel_combined = np.sqrt(sobel_x**2 + sobel_y**2)\n\n    # Converter de volta para uint8\n    sobel_combined = np.uint8(255 * sobel_combined / np.max(sobel_combined))\n\n    return sobel_combined\n\ndef detect_edges_laplacian(image):\n    \"\"\"Detecta bordas usando o operador Laplaciano\"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n\n    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n\n    # Converter de volta para uint8\n    laplacian = np.uint8(np.absolute(laplacian))\n\n    return laplacian\n\ndef detect_edges_canny(image, low_threshold=50, high_threshold=150):\n    \"\"\"Detecta bordas usando o detector de Canny\"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n\n    return cv2.Canny(gray, low_threshold, high_threshold)\n\ndef apply_custom_filter(image, kernel):\n    \"\"\"Aplica filtro personalizado usando convolu\u00e7\u00e3o\"\"\"\n    if len(image.shape) == 3:\n        # Aplicar filtro a cada canal\n        filtered = np.zeros_like(image)\n        for i in range(image.shape[2]):\n            filtered[:,:,i] = cv2.filter2D(image[:,:,i], -1, kernel)\n        return filtered\n    else:\n        return cv2.filter2D(image, -1, kernel)\n</code></pre>"},{"location":"aulas/aula2/#implementar-operacoes-morfologicas","title":"Implementar Opera\u00e7\u00f5es Morfol\u00f3gicas","text":"<pre><code># src/preprocessing/morphology.py\nimport cv2\nimport numpy as np\n\ndef create_structuring_element(shape='rect', size=5):\n    \"\"\"Cria elemento estruturante para opera\u00e7\u00f5es morfol\u00f3gicas\"\"\"\n    if shape == 'rect':\n        return cv2.getStructuringElement(cv2.MORPH_RECT, (size, size))\n    elif shape == 'ellipse':\n        return cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (size, size))\n    elif shape == 'cross':\n        return cv2.getStructuringElement(cv2.MORPH_CROSS, (size, size))\n    else:\n        raise ValueError(\"Forma n\u00e3o suportada. Use 'rect', 'ellipse' ou 'cross'\")\n\ndef morphological_erosion(image, kernel_size=5, iterations=1):\n    \"\"\"Aplica eros\u00e3o morfol\u00f3gica\"\"\"\n    kernel = create_structuring_element(size=kernel_size)\n    return cv2.erode(image, kernel, iterations=iterations)\n\ndef morphological_dilation(image, kernel_size=5, iterations=1):\n    \"\"\"Aplica dilata\u00e7\u00e3o morfol\u00f3gica\"\"\"\n    kernel = create_structuring_element(size=kernel_size)\n    return cv2.dilate(image, kernel, iterations=iterations)\n\ndef morphological_opening(image, kernel_size=5, iterations=1):\n    \"\"\"Aplica abertura morfol\u00f3gica (eros\u00e3o seguida de dilata\u00e7\u00e3o)\"\"\"\n    kernel = create_structuring_element(size=kernel_size)\n    return cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel, iterations=iterations)\n\ndef morphological_closing(image, kernel_size=5, iterations=1):\n    \"\"\"Aplica fechamento morfol\u00f3gico (dilata\u00e7\u00e3o seguida de eros\u00e3o)\"\"\"\n    kernel = create_structuring_element(size=kernel_size)\n    return cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel, iterations=iterations)\n\ndef morphological_gradient(image, kernel_size=5):\n    \"\"\"Aplica gradiente morfol\u00f3gico\"\"\"\n    kernel = create_structuring_element(size=kernel_size)\n    return cv2.morphologyEx(image, cv2.MORPH_GRADIENT, kernel)\n\ndef top_hat(image, kernel_size=5):\n    \"\"\"Aplica transformada Top Hat\"\"\"\n    kernel = create_structuring_element(size=kernel_size)\n    return cv2.morphologyEx(image, cv2.MORPH_TOPHAT, kernel)\n\ndef black_hat(image, kernel_size=5):\n    \"\"\"Aplica transformada Black Hat\"\"\"\n    kernel = create_structuring_element(size=kernel_size)\n    return cv2.morphologyEx(image, cv2.MORPH_BLACKHAT, kernel)\n</code></pre>"},{"location":"aulas/aula2/#pipeline-de-pre-processamento-integrado","title":"Pipeline de Pr\u00e9-processamento Integrado","text":"<pre><code># src/preprocessing/pipeline.py\nfrom .color_spaces import *\nfrom .filters import *\nfrom .morphology import *\nimport numpy as np\n\nclass ImagePreprocessingPipeline:\n    def __init__(self):\n        self.steps = []\n\n    def add_resize(self, width, height):\n        \"\"\"Adiciona passo de redimensionamento\"\"\"\n        def resize_step(image):\n            return cv2.resize(image, (width, height))\n        self.steps.append(('resize', resize_step))\n        return self\n\n    def add_grayscale(self):\n        \"\"\"Adiciona passo de convers\u00e3o para escala de cinza\"\"\"\n        self.steps.append(('grayscale', rgb_to_grayscale))\n        return self\n\n    def add_color_conversion(self, conversion_func):\n        \"\"\"Adiciona passo de convers\u00e3o de espa\u00e7o de cor\"\"\"\n        self.steps.append(('color_conversion', conversion_func))\n        return self\n\n    def add_brightness_contrast(self, brightness=0, contrast=1):\n        \"\"\"Adiciona passo de ajuste de brilho e contraste\"\"\"\n        def bc_step(image):\n            return adjust_brightness_contrast(image, brightness, contrast)\n        self.steps.append(('brightness_contrast', bc_step))\n        return self\n\n    def add_blur(self, blur_type='gaussian', kernel_size=(5, 5)):\n        \"\"\"Adiciona passo de desfoque\"\"\"\n        if blur_type == 'gaussian':\n            blur_func = lambda img: apply_gaussian_blur(img, kernel_size)\n        elif blur_type == 'average':\n            blur_func = lambda img: apply_blur(img, kernel_size)\n        elif blur_type == 'median':\n            blur_func = lambda img: apply_median_blur(img, kernel_size[0])\n        else:\n            raise ValueError(\"Tipo de desfoque n\u00e3o suportado\")\n\n        self.steps.append(('blur', blur_func))\n        return self\n\n    def add_edge_detection(self, edge_type='canny', **kwargs):\n        \"\"\"Adiciona passo de detec\u00e7\u00e3o de bordas\"\"\"\n        if edge_type == 'sobel':\n            edge_func = detect_edges_sobel\n        elif edge_type == 'laplacian':\n            edge_func = detect_edges_laplacian\n        elif edge_type == 'canny':\n            def canny_wrapper(image):\n                low = kwargs.get('low_threshold', 50)\n                high = kwargs.get('high_threshold', 150)\n                return detect_edges_canny(image, low, high)\n            edge_func = canny_wrapper\n        else:\n            raise ValueError(\"Tipo de detec\u00e7\u00e3o de bordas n\u00e3o suportado\")\n\n        self.steps.append(('edge_detection', edge_func))\n        return self\n\n    def add_morphological_operation(self, operation, kernel_size=5, **kwargs):\n        \"\"\"Adiciona opera\u00e7\u00e3o morfol\u00f3gica\"\"\"\n        if operation == 'erosion':\n            morph_func = lambda img: morphological_erosion(img, kernel_size, kwargs.get('iterations', 1))\n        elif operation == 'dilation':\n            morph_func = lambda img: morphological_dilation(img, kernel_size, kwargs.get('iterations', 1))\n        elif operation == 'opening':\n            morph_func = lambda img: morphological_opening(img, kernel_size, kwargs.get('iterations', 1))\n        elif operation == 'closing':\n            morph_func = lambda img: morphological_closing(img, kernel_size, kwargs.get('iterations', 1))\n        elif operation == 'gradient':\n            morph_func = lambda img: morphological_gradient(img, kernel_size)\n        else:\n            raise ValueError(\"Opera\u00e7\u00e3o morfol\u00f3gica n\u00e3o suportada\")\n\n        self.steps.append(('morphological', morph_func))\n        return self\n\n    def add_histogram_equalization(self, color_space='grayscale'):\n        \"\"\"Adiciona equaliza\u00e7\u00e3o de histograma\"\"\"\n        if color_space == 'grayscale':\n            eq_func = equalize_histogram\n        elif color_space == 'color':\n            eq_func = equalize_histogram_color\n        else:\n            raise ValueError(\"Espa\u00e7o de cor para equaliza\u00e7\u00e3o n\u00e3o suportado\")\n\n        self.steps.append(('histogram_equalization', eq_func))\n        return self\n\n    def process(self, image):\n        \"\"\"Processa imagem aplicando todos os passos do pipeline\"\"\"\n        result = image.copy()\n\n        for step_name, step_func in self.steps:\n            try:\n                result = step_func(result)\n            except Exception as e:\n                print(f\"Erro no passo '{step_name}': {str(e)}\")\n                # Continuar com a pr\u00f3xima etapa ou retornar imagem anterior\n                continue\n\n        return result\n\n    def reset(self):\n        \"\"\"Reseta o pipeline removendo todos os passos\"\"\"\n        self.steps = []\n        return self\n</code></pre>"},{"location":"aulas/aula2/#exemplo-de-uso-do-pipeline","title":"Exemplo de Uso do Pipeline","text":"<pre><code># src/examples/preprocessing_example.py\nfrom preprocessing.pipeline import ImagePreprocessingPipeline\nfrom utils.io import load_image_rgb, show_image\nimport matplotlib.pyplot as plt\n\ndef demonstrate_preprocessing_pipeline():\n    \"\"\"Demonstra o uso do pipeline de pr\u00e9-processamento\"\"\"\n    # Carregar imagem\n    image = load_image_rgb(\"data/raw/exemplo.jpg\")  # Substitua pelo caminho real\n\n    # Criar diferentes pipelines\n    # Pipeline 1: Ajuste de brilho e contraste + desfoque gaussiano\n    pipeline1 = ImagePreprocessingPipeline()\n    pipeline1.add_brightness_contrast(brightness=20, contrast=1.2) \\\n             .add_blur(blur_type='gaussian', kernel_size=(5, 5))\n\n    # Pipeline 2: Detec\u00e7\u00e3o de bordas Canny\n    pipeline2 = ImagePreprocessingPipeline()\n    pipeline2.add_grayscale() \\\n             .add_edge_detection(edge_type='canny', low_threshold=50, high_threshold=150)\n\n    # Pipeline 3: Equaliza\u00e7\u00e3o de histograma + opera\u00e7\u00f5es morfol\u00f3gicas\n    pipeline3 = ImagePreprocessingPipeline()\n    pipeline3.add_histogram_equalization(color_space='color') \\\n             .add_morphological_operation('opening', kernel_size=3, iterations=1) \\\n             .add_morphological_operation('closing', kernel_size=3, iterations=1)\n\n    # Processar imagens\n    processed1 = pipeline1.process(image)\n    processed2 = pipeline2.process(image)\n    processed3 = pipeline3.process(image)\n\n    # Visualizar resultados\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n    axes[0,0].imshow(image)\n    axes[0,0].set_title('Imagem Original')\n    axes[0,0].axis('off')\n\n    axes[0,1].imshow(processed1)\n    axes[0,1].set_title('Brilho/Contraste + Blur')\n    axes[0,1].axis('off')\n\n    axes[1,0].imshow(processed2, cmap='gray')\n    axes[1,0].set_title('Detec\u00e7\u00e3o de Bordas (Canny)')\n    axes[1,0].axis('off')\n\n    axes[1,1].imshow(processed3)\n    axes[1,1].set_title('Equaliza\u00e7\u00e3o + Morfol\u00f3gico')\n    axes[1,1].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Limpar pipelines\n    pipeline1.reset()\n    pipeline2.reset()\n    pipeline3.reset()\n\nif __name__ == \"__main__\":\n    demonstrate_preprocessing_pipeline()\n</code></pre>"},{"location":"aulas/aula2/#resultado-esperado","title":"Resultado Esperado","text":"<p>Nesta aula, voc\u00ea:</p> <ol> <li>Aprendeu sobre diferentes espa\u00e7os de cor e suas aplica\u00e7\u00f5es</li> <li>Implementou fun\u00e7\u00f5es para manipula\u00e7\u00e3o de histogramas</li> <li>Criou filtros e operadores de detec\u00e7\u00e3o de bordas</li> <li>Desenvolveu opera\u00e7\u00f5es morfol\u00f3gicas</li> <li>Construiu um pipeline de pr\u00e9-processamento modular e reutiliz\u00e1vel</li> <li>Testou diferentes combina\u00e7\u00f5es de opera\u00e7\u00f5es para ver seus efeitos</li> </ol> <p>O pipeline modular permite f\u00e1cil experimenta\u00e7\u00e3o e combina\u00e7\u00e3o de diferentes t\u00e9cnicas de pr\u00e9-processamento, facilitando a cria\u00e7\u00e3o de solu\u00e7\u00f5es espec\u00edficas para diferentes problemas de vis\u00e3o computacional.</p>"},{"location":"aulas/aula3/","title":"Aula 3 - Segmenta\u00e7\u00e3o e Detec\u00e7\u00e3o Baseada em Regras","text":""},{"location":"aulas/aula3/#objetivo-da-aula","title":"Objetivo da Aula","text":"<p>Implementar t\u00e9cnicas de segmenta\u00e7\u00e3o e detec\u00e7\u00e3o de objetos baseadas em regras, criando fun\u00e7\u00f5es reutiliz\u00e1veis e desenvolvendo um sistema que comece a parecer um produto real.</p>"},{"location":"aulas/aula3/#conteudo-teorico","title":"Conte\u00fado Te\u00f3rico","text":""},{"location":"aulas/aula3/#threshold-limiarizacao","title":"Threshold (Limiariza\u00e7\u00e3o)","text":"<p>A limiariza\u00e7\u00e3o \u00e9 uma t\u00e9cnica fundamental de segmenta\u00e7\u00e3o que converte uma imagem em escala de cinza em uma imagem bin\u00e1ria, separando objetos de fundo com base em um valor de limiar.</p> <p>Tipos comuns: - Limiar Global: Um \u00fanico valor para toda a imagem - Limiar Adaptativo: Valores diferentes para diferentes regi\u00f5es - Otsu's Method: M\u00e9todo que automaticamente determina o melhor limiar</p>"},{"location":"aulas/aula3/#contornos","title":"Contornos","text":"<p>Contornos s\u00e3o curvas que conectam pontos cont\u00ednuos de mesma intensidade, \u00fateis para: - Detec\u00e7\u00e3o de formas - An\u00e1lise de componentes conectados - Extra\u00e7\u00e3o de caracter\u00edsticas de objetos</p>"},{"location":"aulas/aula3/#bounding-boxes","title":"Bounding Boxes","text":"<p>Ret\u00e2ngulos que delimitam objetos detectados, fundamentais para: - Localiza\u00e7\u00e3o de objetos - Contagem de inst\u00e2ncias - Extra\u00e7\u00e3o de regi\u00f5es de interesse</p>"},{"location":"aulas/aula3/#mascaras","title":"M\u00e1scaras","text":"<p>Imagens bin\u00e1rias que indicam quais pixels pertencem a um objeto de interesse, usadas para: - Segmenta\u00e7\u00e3o de objetos - Extra\u00e7\u00e3o de regi\u00f5es espec\u00edficas - Aplica\u00e7\u00e3o de opera\u00e7\u00f5es apenas em \u00e1reas espec\u00edficas</p>"},{"location":"aulas/aula3/#atividade-pratica","title":"Atividade Pr\u00e1tica","text":""},{"location":"aulas/aula3/#implementar-tecnicas-de-threshold","title":"Implementar T\u00e9cnicas de Threshold","text":"<pre><code># src/features/thresholding.py\nimport cv2\nimport numpy as np\n\ndef global_threshold(image, threshold_value=127, max_value=255, method='binary'):\n    \"\"\"Aplica threshold global \u00e0 imagem\"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n\n    if method == 'binary':\n        _, binary = cv2.threshold(gray, threshold_value, max_value, cv2.THRESH_BINARY)\n    elif method == 'binary_inv':\n        _, binary = cv2.threshold(gray, threshold_value, max_value, cv2.THRESH_BINARY_INV)\n    elif method == 'truncate':\n        _, binary = cv2.threshold(gray, threshold_value, max_value, cv2.THRESH_TRUNC)\n    elif method == 'tozero':\n        _, binary = cv2.threshold(gray, threshold_value, max_value, cv2.THRESH_TOZERO)\n    elif method == 'tozero_inv':\n        _, binary = cv2.threshold(gray, threshold_value, max_value, cv2.THRESH_TOZERO_INV)\n    else:\n        raise ValueError(\"M\u00e9todo de threshold n\u00e3o suportado\")\n\n    return binary\n\ndef adaptive_threshold(image, max_value=255, adaptive_method='mean', threshold_type='binary', block_size=11, c=2):\n    \"\"\"Aplica threshold adaptativo \u00e0 imagem\"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n\n    if adaptive_method == 'mean':\n        adaptive_method_cv = cv2.ADAPTIVE_THRESH_MEAN_C\n    elif adaptive_method == 'gaussian':\n        adaptive_method_cv = cv2.ADAPTIVE_THRESH_GAUSSIAN_C\n    else:\n        raise ValueError(\"M\u00e9todo adaptativo n\u00e3o suportado\")\n\n    if threshold_type == 'binary':\n        threshold_type_cv = cv2.THRESH_BINARY\n    elif threshold_type == 'binary_inv':\n        threshold_type_cv = cv2.THRESH_BINARY_INV\n    else:\n        raise ValueError(\"Tipo de threshold n\u00e3o suportado\")\n\n    return cv2.adaptiveThreshold(gray, max_value, adaptive_method_cv, threshold_type_cv, block_size, c)\n\ndef otsu_threshold(image, max_value=255):\n    \"\"\"Aplica threshold de Otsu (automaticamente determina o melhor limiar)\"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n\n    _, binary = cv2.threshold(gray, 0, max_value, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n\n    return binary, _\n</code></pre>"},{"location":"aulas/aula3/#implementar-deteccao-de-contornos","title":"Implementar Detec\u00e7\u00e3o de Contornos","text":"<pre><code># src/features/contours.py\nimport cv2\nimport numpy as np\n\ndef find_contours(binary_image, retrieval_mode='external', approximation_method='simple'):\n    \"\"\"Encontra contornos em uma imagem bin\u00e1ria\"\"\"\n    if retrieval_mode == 'external':\n        retrieval_mode_cv = cv2.RETR_EXTERNAL\n    elif retrieval_mode == 'list':\n        retrieval_mode_cv = cv2.RETR_LIST\n    elif retrieval_mode == 'ccomp':\n        retrieval_mode_cv = cv2.RETR_CCOMP\n    elif retrieval_mode == 'tree':\n        retrieval_mode_cv = cv2.RETR_TREE\n    else:\n        raise ValueError(\"Modo de recupera\u00e7\u00e3o n\u00e3o suportado\")\n\n    if approximation_method == 'none':\n        approx_method_cv = cv2.CHAIN_APPROX_NONE\n    elif approximation_method == 'simple':\n        approx_method_cv = cv2.CHAIN_APPROX_SIMPLE\n    elif approximation_method == 'tc89_l1':\n        approx_method_cv = cv2.CHAIN_APPROX_TC89_L1\n    elif approximation_method == 'tc89_kcos':\n        approx_method_cv = cv2.CHAIN_APPROX_TC89_KCOS\n    else:\n        raise ValueError(\"M\u00e9todo de aproxima\u00e7\u00e3o n\u00e3o suportado\")\n\n    contours, hierarchy = cv2.findContours(binary_image, retrieval_mode_cv, approx_method_cv)\n\n    return contours, hierarchy\n\ndef filter_contours_by_area(contours, min_area=0, max_area=float('inf')):\n    \"\"\"Filtra contornos por \u00e1rea\"\"\"\n    filtered_contours = []\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        if min_area &lt;= area &lt;= max_area:\n            filtered_contours.append(contour)\n\n    return filtered_contours\n\ndef filter_contours_by_circularity(contours, min_circularity=0, max_circularity=1):\n    \"\"\"Filtra contornos por circularidade (4*pi*area/perimeter^2)\"\"\"\n    filtered_contours = []\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        perimeter = cv2.arcLength(contour, True)\n\n        if perimeter == 0:\n            continue\n\n        circularity = 4 * np.pi * area / (perimeter * perimeter)\n\n        if min_circularity &lt;= circularity &lt;= max_circularity:\n            filtered_contours.append(contour)\n\n    return filtered_contours\n\ndef filter_contours_by_aspect_ratio(contours, min_ratio=0, max_ratio=float('inf')):\n    \"\"\"Filtra contornos por raz\u00e3o de aspecto (largura/altura do bounding rectangle)\"\"\"\n    filtered_contours = []\n    for contour in contours:\n        x, y, w, h = cv2.boundingRect(contour)\n        aspect_ratio = float(w) / h\n\n        if min_ratio &lt;= aspect_ratio &lt;= max_ratio:\n            filtered_contours.append(contour)\n\n    return filtered_contours\n\ndef get_contour_properties(contour):\n    \"\"\"Obt\u00e9m propriedades de um contorno\"\"\"\n    properties = {}\n\n    # \u00c1rea\n    properties['area'] = cv2.contourArea(contour)\n\n    # Per\u00edmetro\n    properties['perimeter'] = cv2.arcLength(contour, True)\n\n    # Bounding rectangle\n    x, y, w, h = cv2.boundingRect(contour)\n    properties['bounding_rect'] = {'x': x, 'y': y, 'width': w, 'height': h}\n\n    # Bounding rectangle rotacionado\n    rect = cv2.minAreaRect(contour)\n    box = cv2.boxPoints(rect)\n    box = np.int0(box)\n    properties['rotated_rect'] = box\n\n    # Circularity\n    if properties['perimeter'] &gt; 0:\n        properties['circularity'] = 4 * np.pi * properties['area'] / (properties['perimeter'] * properties['perimeter'])\n    else:\n        properties['circularity'] = 0\n\n    # Extent (raz\u00e3o entre \u00e1rea do contorno e \u00e1rea do bounding rectangle)\n    properties['extent'] = properties['area'] / float(w * h)\n\n    # Centroide\n    moments = cv2.moments(contour)\n    if moments['m00'] != 0:\n        cx = int(moments['m10'] / moments['m00'])\n        cy = int(moments['m01'] / moments['m00'])\n        properties['centroid'] = (cx, cy)\n    else:\n        properties['centroid'] = (0, 0)\n\n    return properties\n</code></pre>"},{"location":"aulas/aula3/#implementar-bounding-boxes-e-mascaras","title":"Implementar Bounding Boxes e M\u00e1scaras","text":"<pre><code># src/features/bounding_boxes.py\nimport cv2\nimport numpy as np\n\ndef draw_bounding_boxes(image, contours, color=(0, 255, 0), thickness=2):\n    \"\"\"Desenha bounding boxes ao redor dos contornos\"\"\"\n    result = image.copy()\n\n    for contour in contours:\n        x, y, w, h = cv2.boundingRect(contour)\n        cv2.rectangle(result, (x, y), (x + w, y + h), color, thickness)\n\n    return result\n\ndef draw_rotated_bounding_boxes(image, contours, color=(0, 255, 255), thickness=2):\n    \"\"\"Desenha bounding boxes rotacionadas ao redor dos contornos\"\"\"\n    result = image.copy()\n\n    for contour in contours:\n        rect = cv2.minAreaRect(contour)\n        box = cv2.boxPoints(rect)\n        box = np.int0(box)\n        cv2.drawContours(result, [box], 0, color, thickness)\n\n    return result\n\ndef get_bounding_boxes(contours):\n    \"\"\"Obt\u00e9m as coordenadas das bounding boxes para cada contorno\"\"\"\n    boxes = []\n    for contour in contours:\n        x, y, w, h = cv2.boundingRect(contour)\n        boxes.append({'x': x, 'y': y, 'width': w, 'height': h})\n\n    return boxes\n\ndef create_masks_from_contours(image_shape, contours):\n    \"\"\"Cria m\u00e1scaras bin\u00e1rias para cada contorno\"\"\"\n    masks = []\n\n    for contour in contours:\n        mask = np.zeros(image_shape[:2], dtype=np.uint8)\n        cv2.fillPoly(mask, [contour], 255)\n        masks.append(mask)\n\n    return masks\n\ndef extract_roi(image, bounding_box):\n    \"\"\"Extrai regi\u00e3o de interesse (ROI) baseada em bounding box\"\"\"\n    x, y, w, h = bounding_box['x'], bounding_box['y'], bounding_box['width'], bounding_box['height']\n    return image[y:y+h, x:x+w]\n\ndef create_combined_mask(image_shape, contours):\n    \"\"\"Cria uma m\u00e1scara combinada para todos os contornos\"\"\"\n    combined_mask = np.zeros(image_shape[:2], dtype=np.uint8)\n\n    for contour in contours:\n        cv2.fillPoly(combined_mask, [contour], 255)\n\n    return combined_mask\n</code></pre>"},{"location":"aulas/aula3/#detector-de-objetos-por-cor","title":"Detector de Objetos por Cor","text":"<pre><code># src/features/color_detector.py\nimport cv2\nimport numpy as np\n\nclass ColorDetector:\n    def __init__(self):\n        self.color_ranges = {\n            'red': [(0, 50, 50), (10, 255, 255), (170, 50, 50), (180, 255, 255)],  # Precisa de dois ranges para vermelho\n            'green': [(40, 50, 50), (80, 255, 255)],\n            'blue': [(100, 50, 50), (130, 255, 255)],\n            'yellow': [(20, 50, 50), (40, 255, 255)],\n            'purple': [(130, 50, 50), (160, 255, 255)],\n            'orange': [(10, 50, 50), (20, 255, 255)],\n        }\n\n    def detect_by_color_range(self, image, color_name, min_area=100):\n        \"\"\"Detecta objetos de uma cor espec\u00edfica\"\"\"\n        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n\n        if color_name not in self.color_ranges:\n            raise ValueError(f\"Cor '{color_name}' n\u00e3o suportada\")\n\n        color_range = self.color_ranges[color_name]\n\n        if color_name == 'red':\n            # Vermelho tem dois ranges no HSV\n            lower1, upper1 = color_range[0], color_range[1]\n            lower2, upper2 = color_range[2], color_range[3]\n\n            mask1 = cv2.inRange(hsv, lower1, upper1)\n            mask2 = cv2.inRange(hsv, lower2, upper2)\n            mask = mask1 + mask2\n        else:\n            lower, upper = color_range[0], color_range[1]\n            mask = cv2.inRange(hsv, lower, upper)\n\n        # Aplicar opera\u00e7\u00f5es morfol\u00f3gicas para limpar a m\u00e1scara\n        kernel = np.ones((5,5), np.uint8)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n        # Encontrar contornos\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Filtrar por \u00e1rea m\u00ednima\n        filtered_contours = [c for c in contours if cv2.contourArea(c) &gt;= min_area]\n\n        return filtered_contours, mask\n\n    def detect_multiple_colors(self, image, colors, min_area=100):\n        \"\"\"Detecta m\u00faltiplas cores na mesma imagem\"\"\"\n        results = {}\n\n        for color in colors:\n            contours, mask = self.detect_by_color_range(image, color, min_area)\n            results[color] = {\n                'contours': contours,\n                'mask': mask,\n                'count': len(contours)\n            }\n\n        return results\n\n    def draw_color_detections(self, image, detection_results):\n        \"\"\"Desenha detec\u00e7\u00f5es de cores na imagem\"\"\"\n        result_image = image.copy()\n\n        # Cores para desenho (BGR)\n        colors_bgr = {\n            'red': (0, 0, 255),\n            'green': (0, 255, 0),\n            'blue': (255, 0, 0),\n            'yellow': (0, 255, 255),\n            'purple': (128, 0, 128),\n            'orange': (0, 165, 255)\n        }\n\n        for color_name, data in detection_results.items():\n            contours = data['contours']\n            color_bgr = colors_bgr.get(color_name, (255, 255, 255))  # Branco padr\u00e3o\n\n            # Desenhar contornos\n            cv2.drawContours(result_image, contours, -1, color_bgr, 2)\n\n            # Desenhar bounding boxes\n            for contour in contours:\n                x, y, w, h = cv2.boundingRect(contour)\n                cv2.rectangle(result_image, (x, y), (x+w, y+h), color_bgr, 2)\n\n                # Adicionar texto com nome da cor e contagem\n                cv2.putText(result_image, f'{color_name}', (x, y-10), \n                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color_bgr, 1)\n\n        return result_image\n</code></pre>"},{"location":"aulas/aula3/#sistema-de-contagem-automatica","title":"Sistema de Contagem Autom\u00e1tica","text":"<pre><code># src/features/object_counter.py\nimport cv2\nimport numpy as np\n\nclass ObjectCounter:\n    def __init__(self):\n        self.detection_history = []\n\n    def count_objects(self, image, detection_method='contours', **kwargs):\n        \"\"\"Conta objetos na imagem usando diferentes m\u00e9todos\"\"\"\n        if detection_method == 'contours':\n            # Converter para escala de cinza\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n\n            # Aplicar threshold\n            threshold_value = kwargs.get('threshold_value', 127)\n            _, binary = cv2.threshold(gray, threshold_value, 255, cv2.THRESH_BINARY)\n\n            # Encontrar contornos\n            contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            # Filtrar por \u00e1rea m\u00ednima\n            min_area = kwargs.get('min_area', 100)\n            filtered_contours = [c for c in contours if cv2.contourArea(c) &gt;= min_area]\n\n            return len(filtered_contours), filtered_contours\n\n        elif detection_method == 'color':\n            from .color_detector import ColorDetector\n            detector = ColorDetector()\n\n            color = kwargs.get('color', 'red')\n            min_area = kwargs.get('min_area', 100)\n\n            contours, _ = detector.detect_by_color_range(image, color, min_area)\n\n            return len(contours), contours\n\n        else:\n            raise ValueError(f\"M\u00e9todo de detec\u00e7\u00e3o '{detection_method}' n\u00e3o suportado\")\n\n    def count_and_save_results(self, image, output_path, detection_method='contours', **kwargs):\n        \"\"\"Conta objetos e salva resultados\"\"\"\n        count, contours = self.count_objects(image, detection_method, **kwargs)\n\n        # Desenhar resultados na imagem\n        result_image = image.copy()\n        cv2.drawContours(result_image, contours, -1, (0, 255, 0), 2)\n\n        # Adicionar texto com contagem\n        cv2.putText(result_image, f'Contagem: {count}', (10, 30), \n                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n        # Salvar imagem\n        cv2.imwrite(output_path, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))\n\n        # Salvar informa\u00e7\u00f5es\n        result_info = {\n            'count': count,\n            'timestamp': str(pd.Timestamp.now()) if 'pd' in globals() else 'timestamp_not_available',\n            'contour_areas': [cv2.contourArea(c) for c in contours],\n            'image_path': output_path\n        }\n\n        self.detection_history.append(result_info)\n\n        return result_info\n</code></pre>"},{"location":"aulas/aula3/#exemplo-de-uso-integrado","title":"Exemplo de Uso Integrado","text":"<pre><code># src/examples/segmentation_example.py\nfrom features.thresholding import *\nfrom features.contours import *\nfrom features.bounding_boxes import *\nfrom features.color_detector import ColorDetector\nfrom features.object_counter import ObjectCounter\nfrom utils.io import load_image_rgb, show_image\nimport matplotlib.pyplot as plt\n\ndef demonstrate_segmentation_techniques():\n    \"\"\"Demonstra t\u00e9cnicas de segmenta\u00e7\u00e3o e detec\u00e7\u00e3o\"\"\"\n    # Carregar imagem\n    image = load_image_rgb(\"data/raw/exemplo.jpg\")  # Substitua pelo caminho real\n\n    # 1. Demonstrar diferentes tipos de threshold\n    global_thresh = global_threshold(image, threshold_value=127)\n    adaptive_thresh = adaptive_threshold(image)\n    otsu_thresh, otsu_value = otsu_threshold(image)\n\n    # 2. Encontrar contornos\n    contours, hierarchy = find_contours(otsu_thresh)\n\n    # 3. Filtrar contornos por \u00e1rea\n    filtered_contours = filter_contours_by_area(contours, min_area=100)\n\n    # 4. Desenhar bounding boxes\n    bbox_image = draw_bounding_boxes(image, filtered_contours)\n\n    # 5. Detector de cores\n    color_detector = ColorDetector()\n    color_results = color_detector.detect_multiple_colors(\n        image, ['red', 'blue', 'green'], min_area=50\n    )\n    color_detected_image = color_detector.draw_color_detections(image, color_results)\n\n    # 6. Contador de objetos\n    counter = ObjectCounter()\n    object_count, detected_contours = counter.count_objects(\n        image, detection_method='contours', min_area=100\n    )\n\n    # Visualizar resultados\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n    axes[0,0].imshow(image)\n    axes[0,0].set_title('Imagem Original')\n    axes[0,0].axis('off')\n\n    axes[0,1].imshow(global_thresh, cmap='gray')\n    axes[0,1].set_title('Threshold Global')\n    axes[0,1].axis('off')\n\n    axes[0,2].imshow(adaptive_thresh, cmap='gray')\n    axes[0,2].set_title('Threshold Adaptativo')\n    axes[0,2].axis('off')\n\n    axes[1,0].imshow(otsu_thresh, cmap='gray')\n    axes[1,0].set_title(f'Threshold Otsu (v={otsu_value:.2f})')\n    axes[1,0].axis('off')\n\n    axes[1,1].imshow(bbox_image)\n    axes[1,1].set_title(f'Bounding Boxes (Contornos: {len(filtered_contours)})')\n    axes[1,1].axis('off')\n\n    axes[1,2].imshow(color_detected_image)\n    axes[1,2].set_title(f'Detec\u00e7\u00e3o por Cor (Objetos: {sum(r[\"count\"] for r in color_results.values())})')\n    axes[1,2].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n    print(f\"Contagem de objetos: {object_count}\")\n    print(f\"Contornos detectados: {len(detected_contours)}\")\n\n    # Mostrar propriedades de alguns contornos\n    if detected_contours:\n        for i, contour in enumerate(detected_contours[:3]):  # Mostrar as primeiras 3\n            props = get_contour_properties(contour)\n            print(f\"\\nContorno {i+1}:\")\n            print(f\"  \u00c1rea: {props['area']:.2f}\")\n            print(f\"  Circularidade: {props['circularity']:.3f}\")\n            print(f\"  Raz\u00e3o de aspecto: {props['extent']:.3f}\")\n            print(f\"  Centroide: {props['centroid']}\")\n\nif __name__ == \"__main__\":\n    demonstrate_segmentation_techniques()\n</code></pre>"},{"location":"aulas/aula3/#resultado-esperado","title":"Resultado Esperado","text":"<p>Nesta aula, voc\u00ea:</p> <ol> <li>Implementou diferentes t\u00e9cnicas de threshold (global, adaptativo, Otsu)</li> <li>Desenvolveu fun\u00e7\u00f5es para detec\u00e7\u00e3o e filtragem de contornos</li> <li>Criou funcionalidades para desenhar bounding boxes e extrair propriedades</li> <li>Construiu um detector de objetos por cor</li> <li>Desenvolveu um sistema de contagem autom\u00e1tica de objetos</li> <li>Integrar todas essas funcionalidades em um exemplo pr\u00e1tico</li> </ol> <p>O sistema agora come\u00e7a a ter caracter\u00edsticas de um produto real, com m\u00f3dulos bem definidos e funcionalidades que podem ser combinadas para resolver problemas espec\u00edficos de detec\u00e7\u00e3o e segmenta\u00e7\u00e3o.</p>"},{"location":"aulas/aula5/","title":"Aula 5 - Feature Extraction e Matching","text":""},{"location":"aulas/aula5/#objetivo-da-aula","title":"Objetivo da Aula","text":"<p>Aprender t\u00e9cnicas avan\u00e7adas de extra\u00e7\u00e3o de caracter\u00edsticas visuais e matching, explorando algoritmos como ORB, SIFT e aplicando-os em contextos reais como AR, biometria e inspe\u00e7\u00e3o industrial.</p>"},{"location":"aulas/aula5/#conteudo-teorico","title":"Conte\u00fado Te\u00f3rico","text":""},{"location":"aulas/aula5/#keypoints-e-descritores","title":"Keypoints e Descritores","text":"<p>Keypoints s\u00e3o pontos espec\u00edficos em uma imagem que possuem propriedades distintivas, como cantos, bordas ou regi\u00f5es com alta varia\u00e7\u00e3o de intensidade. S\u00e3o invariantes a transforma\u00e7\u00f5es como rota\u00e7\u00e3o, escala e ilumina\u00e7\u00e3o.</p> <p>Descritores s\u00e3o vetores num\u00e9ricos que codificam informa\u00e7\u00f5es sobre a vizinhan\u00e7a de um keypoint, permitindo compara\u00e7\u00e3o entre diferentes keypoints.</p>"},{"location":"aulas/aula5/#algoritmos-de-deteccao-de-keypoints","title":"Algoritmos de Detec\u00e7\u00e3o de Keypoints","text":""},{"location":"aulas/aula5/#orb-oriented-fast-and-rotated-brief","title":"ORB (Oriented FAST and Rotated BRIEF)","text":"<ul> <li>Combina\u00e7\u00e3o do detector FAST e descritor BRIEF</li> <li>R\u00e1pido e eficiente</li> <li>Livre de patentes</li> <li>Adequado para aplica\u00e7\u00f5es em tempo real</li> </ul>"},{"location":"aulas/aula5/#sift-scale-invariant-feature-transform","title":"SIFT (Scale-Invariant Feature Transform)","text":"<ul> <li>Invariante a escala e rota\u00e7\u00e3o</li> <li>Robusto a mudan\u00e7as de ilumina\u00e7\u00e3o</li> <li>Muito eficaz mas computacionalmente custoso</li> <li>Patenteado (expirou em 2020)</li> </ul>"},{"location":"aulas/aula5/#surf-speeded-up-robust-features","title":"SURF (Speeded Up Robust Features)","text":"<ul> <li>Vers\u00e3o mais r\u00e1pida do SIFT</li> <li>Tamb\u00e9m invariante a escala e rota\u00e7\u00e3o</li> <li>Menos robusto que SIFT em algumas situa\u00e7\u00f5es</li> </ul>"},{"location":"aulas/aula5/#matching-de-caracteristicas","title":"Matching de Caracter\u00edsticas","text":"<p>O matching envolve encontrar correspond\u00eancias entre keypoints de diferentes imagens, fundamental para:</p> <ul> <li>Reconhecimento de objetos</li> <li>Reconstru\u00e7\u00e3o 3D</li> <li>Augmented Reality</li> <li>Biometria</li> <li>Inspe\u00e7\u00e3o de qualidade industrial</li> </ul>"},{"location":"aulas/aula5/#atividade-pratica","title":"Atividade Pr\u00e1tica","text":""},{"location":"aulas/aula5/#implementar-detector-e-descritor-orb","title":"Implementar Detector e Descritor ORB","text":"<pre><code># src/features/orb_detector.py\nimport cv2\nimport numpy as np\n\nclass ORBDetector:\n    def __init__(self, n_features=500, scale_factor=1.2, n_levels=8, edge_threshold=31, \n                 first_level=0, WTA_K=2, score_type=cv2.ORB_HARRIS_SCORE, patch_size=31, fast_threshold=20):\n        \"\"\"\n        Inicializa o detector ORB\n        \"\"\"\n        self.detector = cv2.ORB_create(\n            nfeatures=n_features,\n            scaleFactor=scale_factor,\n            nlevels=n_levels,\n            edgeThreshold=edge_threshold,\n            firstLevel=first_level,\n            WTA_K=WTA_K,\n            scoreType=score_type,\n            patchSize=patch_size,\n            fastThreshold=fast_threshold\n        )\n\n    def detect_and_compute(self, image):\n        \"\"\"\n        Detecta keypoints e computa descritores\n        \"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n        keypoints, descriptors = self.detector.detectAndCompute(gray, None)\n        return keypoints, descriptors\n\n    def draw_keypoints(self, image, keypoints, color=(0, 255, 0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS):\n        \"\"\"\n        Desenha keypoints na imagem\n        \"\"\"\n        return cv2.drawKeypoints(image, keypoints, None, color=color, flags=flags)\n\n    def match_features(self, descriptors1, descriptors2, matcher_type='bf', cross_check=True):\n        \"\"\"\n        Realiza matching entre descritores\n        \"\"\"\n        if matcher_type == 'bf':\n            # Brute Force Matcher\n            matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=cross_check)\n            matches = matcher.match(descriptors1, descriptors2)\n        elif matcher_type == 'flann':\n            # FLANN Matcher (mais r\u00e1pido para grandes conjuntos de dados)\n            FLANN_INDEX_LSH = 6\n            index_params = dict(algorithm=FLANN_INDEX_LSH, table_number=6, key_size=12, multi_probe_level=1)\n            search_params = dict(checks=50)\n            matcher = cv2.FlannBasedMatcher(index_params, search_params)\n            matches = matcher.knnMatch(descriptors1, descriptors2, k=2)\n            # Aplicar Lowe's ratio test\n            good_matches = []\n            for match_pair in matches:\n                if len(match_pair) == 2:\n                    m, n = match_pair\n                    if m.distance &lt; 0.7 * n.distance:\n                        good_matches.append(m)\n            matches = good_matches\n        else:\n            raise ValueError(\"Tipo de matcher n\u00e3o suportado\")\n\n        # Ordenar matches por dist\u00e2ncia\n        matches = sorted(matches, key=lambda x: x.distance)\n        return matches\n\n    def draw_matches(self, img1, kp1, img2, kp2, matches, n_matches=50):\n        \"\"\"\n        Desenha matches entre duas imagens\n        \"\"\"\n        # Limitar n\u00famero de matches para visualiza\u00e7\u00e3o\n        matches = matches[:min(n_matches, len(matches))]\n\n        # Converter imagens para BGR se forem RGB\n        if len(img1.shape) == 3 and img1.shape[2] == 3:\n            img1 = cv2.cvtColor(img1, cv2.COLOR_RGB2BGR)\n        if len(img2.shape) == 3 and img2.shape[2] == 3:\n            img2 = cv2.cvtColor(img2, cv2.COLOR_RGB2BGR)\n\n        result = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, \n                                flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n        return result\n</code></pre>"},{"location":"aulas/aula5/#implementar-detector-sift-alternativa","title":"Implementar Detector SIFT (Alternativa)","text":"<pre><code># src/features/sift_detector.py\nimport cv2\nimport numpy as np\n\nclass SIFTDetector:\n    def __init__(self, n_features=4000, n_octave_layers=3, contrast_threshold=0.04, \n                 edge_threshold=10, sigma=1.6):\n        \"\"\"\n        Inicializa o detector SIFT\n        Nota: SIFT est\u00e1 dispon\u00edvel apenas em vers\u00f5es do OpenCV com licen\u00e7a n\u00e3o GPL\n        \"\"\"\n        try:\n            self.detector = cv2.SIFT_create(\n                nfeatures=n_features,\n                nOctaveLayers=n_octave_layers,\n                contrastThreshold=contrast_threshold,\n                edgeThreshold=edge_threshold,\n                sigma=sigma\n            )\n        except AttributeError:\n            raise ImportError(\"SIFT n\u00e3o dispon\u00edvel nesta vers\u00e3o do OpenCV\")\n\n    def detect_and_compute(self, image):\n        \"\"\"\n        Detecta keypoints e computa descritores SIFT\n        \"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n        keypoints, descriptors = self.detector.detectAndCompute(gray, None)\n        return keypoints, descriptors\n\n    def draw_keypoints(self, image, keypoints, color=(0, 255, 0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS):\n        \"\"\"\n        Desenha keypoints na imagem\n        \"\"\"\n        return cv2.drawKeypoints(image, keypoints, None, color=color, flags=flags)\n\n    def match_features(self, descriptors1, descriptors2, matcher_type='bf', cross_check=False):\n        \"\"\"\n        Realiza matching entre descritores SIFT\n        \"\"\"\n        if matcher_type == 'bf':\n            # Brute Force com norma L2 (adequada para SIFT)\n            matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=cross_check)\n            matches = matcher.match(descriptors1, descriptors2)\n        elif matcher_type == 'flann':\n            # FLANN para SIFT\n            FLANN_INDEX_KDTREE = 1\n            index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n            search_params = dict(checks=50)\n            matcher = cv2.FlannBasedMatcher(index_params, search_params)\n            matches = matcher.knnMatch(descriptors1, descriptors2, k=2)\n            # Aplicar Lowe's ratio test\n            good_matches = []\n            for match_pair in matches:\n                if len(match_pair) == 2:\n                    m, n = match_pair\n                    if m.distance &lt; 0.7 * n.distance:\n                        good_matches.append(m)\n            matches = good_matches\n        else:\n            raise ValueError(\"Tipo de matcher n\u00e3o suportado\")\n\n        # Ordenar matches por dist\u00e2ncia\n        matches = sorted(matches, key=lambda x: x.distance)\n        return matches\n</code></pre>"},{"location":"aulas/aula5/#implementar-comparacao-de-imagens","title":"Implementar Compara\u00e7\u00e3o de Imagens","text":"<pre><code># src/features/image_matching.py\nimport cv2\nimport numpy as np\nfrom .orb_detector import ORBDetector\n\nclass ImageMatcher:\n    def __init__(self, detector_type='orb'):\n        if detector_type == 'orb':\n            self.detector = ORBDetector()\n        else:\n            raise ValueError(\"Tipo de detector n\u00e3o suportado\")\n\n    def compare_two_images(self, img1, img2, min_matches=10):\n        \"\"\"\n        Compara duas imagens e retorna n\u00famero de matches e similaridade\n        \"\"\"\n        # Detectar e computar keypoints e descritores\n        kp1, desc1 = self.detector.detect_and_compute(img1)\n        kp2, desc2 = self.detector.detect_and_compute(img2)\n\n        if desc1 is None or desc2 is None:\n            return {\n                'matches_count': 0,\n                'similarity': 0.0,\n                'has_match': False,\n                'keypoints1': len(kp1) if kp1 else 0,\n                'keypoints2': len(kp2) if kp2 else 0\n            }\n\n        # Realizar matching\n        matches = self.detector.match_features(desc1, desc2)\n\n        # Calcular similaridade baseada em n\u00famero de matches\n        similarity = min(len(matches) / min(len(kp1), len(kp2)), 1.0) if kp1 and kp2 else 0.0\n\n        result = {\n            'matches_count': len(matches),\n            'similarity': similarity,\n            'has_match': len(matches) &gt;= min_matches,\n            'keypoints1': len(kp1),\n            'keypoints2': len(kp2),\n            'matches': matches,\n            'keypoints1_raw': kp1,\n            'keypoints2_raw': kp2\n        }\n\n        return result\n\n    def find_template_in_image(self, template, image, threshold=0.7):\n        \"\"\"\n        Encontra uma template em uma imagem maior usando feature matching\n        \"\"\"\n        # Detectar features na template e na imagem\n        kp_template, desc_template = self.detector.detect_and_compute(template)\n        kp_image, desc_image = self.detector.detect_and_compute(image)\n\n        if desc_template is None or desc_image is None:\n            return []\n\n        # Matching\n        matches = self.detector.match_features(desc_template, desc_image)\n\n        # Filtrar matches baseado em threshold\n        filtered_matches = [m for m in matches if m.distance &lt; threshold * 100]\n\n        if len(filtered_matches) &lt; 10:  # M\u00ednimo de matches para considerar uma correspond\u00eancia\n            return []\n\n        # Extrair coordenadas dos keypoints correspondentes\n        src_pts = np.float32([kp_template[m.queryIdx].pt for m in filtered_matches]).reshape(-1, 1, 2)\n        dst_pts = np.float32([kp_image[m.trainIdx].pt for m in filtered_matches]).reshape(-1, 1, 2)\n\n        # Calcular homografia\n        try:\n            homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n            if homography is not None:\n                # Obter cantos da template\n                h, w = template.shape[:2]\n                template_corners = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)\n                # Transformar cantos para a imagem\n                image_corners = cv2.perspectiveTransform(template_corners, homography)\n                # Converter para formato adequado\n                image_corners = np.int32(image_corners).reshape(-1, 2)\n                return image_corners\n        except:\n            pass\n\n        return []\n\n    def calculate_feature_similarity(self, img1, img2, metric='ratio'):\n        \"\"\"\n        Calcula similaridade entre duas imagens baseado em features\n        \"\"\"\n        comparison = self.compare_two_images(img1, img2)\n\n        if metric == 'ratio':\n            # Similaridade baseada na raz\u00e3o de matches\n            return comparison['similarity']\n        elif metric == 'count':\n            # Similaridade baseada no n\u00famero absoluto de matches\n            return min(comparison['matches_count'] / 100, 1.0)  # Normalizar\n        else:\n            raise ValueError(\"M\u00e9trica n\u00e3o suportada\")\n</code></pre>"},{"location":"aulas/aula5/#implementar-aplicacoes-reais","title":"Implementar Aplica\u00e7\u00f5es Reais","text":"<pre><code># src/features/applications.py\nimport cv2\nimport numpy as np\nfrom .image_matching import ImageMatcher\n\nclass FeatureApplications:\n    def __init__(self):\n        self.matcher = ImageMatcher()\n\n    def augmented_reality_overlay(self, reference_img, live_frame):\n        \"\"\"\n        Demonstra\u00e7\u00e3o de overlay AR baseado em matching de features\n        \"\"\"\n        # Encontrar correspond\u00eancias entre refer\u00eancia e frame ao vivo\n        comparison = self.matcher.compare_two_images(reference_img, live_frame)\n\n        if comparison['has_match']:\n            matches = comparison['matches']\n            kp_ref = comparison['keypoints1_raw']\n            kp_live = comparison['keypoints2_raw']\n\n            # Extrair pontos correspondentes\n            if len(matches) &gt;= 4:\n                src_pts = np.float32([kp_ref[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n                dst_pts = np.float32([kp_live[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n                # Calcular homografia\n                homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n\n                if homography is not None:\n                    # Definir regi\u00e3o para overlay (ex: cantos da refer\u00eancia)\n                    h, w = reference_img.shape[:2]\n                    ref_corners = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)\n                    live_corners = cv2.perspectiveTransform(ref_corners, homography)\n\n                    # Retornar cantos para desenho de overlay\n                    return np.int32(live_corners).reshape(-1, 2)\n\n        return None\n\n    def quality_inspection(self, reference_img, test_img, threshold=0.7):\n        \"\"\"\n        Sistema de inspe\u00e7\u00e3o de qualidade baseado em matching\n        \"\"\"\n        similarity = self.matcher.calculate_feature_similarity(reference_img, test_img)\n\n        result = {\n            'similarity': similarity,\n            'pass': similarity &gt;= threshold,\n            'difference': abs(similarity - threshold)\n        }\n\n        return result\n\n    def object_recognition(self, object_templates, scene_img, threshold=0.6):\n        \"\"\"\n        Sistema de reconhecimento de objetos em cena\n        \"\"\"\n        recognized_objects = []\n\n        for obj_name, template in object_templates.items():\n            # Tentar encontrar o objeto na cena\n            corners = self.matcher.find_template_in_image(template, scene_img)\n\n            if len(corners) &gt; 0:\n                # Calcular confian\u00e7a baseada em n\u00famero de matches\n                comparison = self.matcher.compare_two_images(template, scene_img)\n                confidence = min(1.0, comparison['matches_count'] / 50)  # Normalizar\n\n                if confidence &gt;= threshold:\n                    recognized_objects.append({\n                        'name': obj_name,\n                        'location': corners,\n                        'confidence': confidence\n                    })\n\n        return recognized_objects\n</code></pre>"},{"location":"aulas/aula5/#exemplo-de-uso-integrado","title":"Exemplo de Uso Integrado","text":"<pre><code># src/examples/feature_extraction_example.py\nfrom features.orb_detector import ORBDetector\nfrom features.image_matching import ImageMatcher\nfrom features.applications import FeatureApplications\nfrom utils.io import load_image_rgb, show_image\nimport matplotlib.pyplot as plt\n\ndef demonstrate_feature_extraction():\n    \"\"\"Demonstra extra\u00e7\u00e3o e matching de features\"\"\"\n    # Carregar imagens\n    img1 = load_image_rgb(\"data/raw/exemplo1.jpg\")  # Substitua pelos caminhos reais\n    img2 = load_image_rgb(\"data/raw/exemplo2.jpg\")  # Imagem similar ou transformada\n\n    # Inicializar detector\n    detector = ORBDetector(n_features=500)\n\n    # Detectar keypoints e descritores\n    kp1, desc1 = detector.detect_and_compute(img1)\n    kp2, desc2 = detector.detect_and_compute(img2)\n\n    print(f\"Keypoints na imagem 1: {len(kp1) if kp1 else 0}\")\n    print(f\"Keypoints na imagem 2: {len(kp2) if kp2 else 0}\")\n\n    # Desenhar keypoints\n    img1_kp = detector.draw_keypoints(img1, kp1)\n    img2_kp = detector.draw_keypoints(img2, kp2)\n\n    # Realizar matching\n    matches = detector.match_features(desc1, desc2)\n    print(f\"Matches encontrados: {len(matches)}\")\n\n    # Desenhar matches\n    if matches:\n        img_matches = detector.draw_matches(img1, kp1, img2, kp2, matches, n_matches=20)\n\n    # Comparar imagens\n    matcher = ImageMatcher()\n    comparison = matcher.compare_two_images(img1, img2)\n    print(f\"Similaridade: {comparison['similarity']:.3f}\")\n    print(f\"Tem correspond\u00eancia: {comparison['has_match']}\")\n\n    # Visualizar resultados\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n    axes[0,0].imshow(img1_kp)\n    axes[0,0].set_title(f'Keypoints - Imagem 1 ({len(kp1) if kp1 else 0})')\n    axes[0,0].axis('off')\n\n    axes[0,1].imshow(img2_kp)\n    axes[0,1].set_title(f'Keypoints - Imagem 2 ({len(kp2) if kp2 else 0})')\n    axes[0,1].axis('off')\n\n    if matches and 'img_matches' in locals():\n        axes[1,0].imshow(img_matches)\n        axes[1,0].set_title(f'Matches ({len(matches)} encontrados)')\n        axes[1,0].axis('off')\n    else:\n        axes[1,0].text(0.5, 0.5, 'Nenhum match encontrado', \n                      horizontalalignment='center', verticalalignment='center',\n                      transform=axes[1,0].transAxes, fontsize=14)\n        axes[1,0].axis('off')\n\n    # Aplica\u00e7\u00e3o: inspe\u00e7\u00e3o de qualidade\n    app = FeatureApplications()\n    if img1 is not None and img2 is not None:\n        inspection_result = app.quality_inspection(img1, img2)\n        axes[1,1].text(0.1, 0.8, f'Similaridade: {inspection_result[\"similarity\"]:.3f}', \n                      transform=axes[1,1].transAxes, fontsize=12)\n        axes[1,1].text(0.1, 0.7, f'Resultado: {\"PASS\" if inspection_result[\"pass\"] else \"FAIL\"}', \n                      transform=axes[1,1].transAxes, fontsize=12, \n                      color='green' if inspection_result[\"pass\"] else 'red')\n        axes[1,1].set_title('Inspe\u00e7\u00e3o de Qualidade')\n    else:\n        axes[1,1].text(0.5, 0.5, 'Imagens n\u00e3o carregadas', \n                      horizontalalignment='center', verticalalignment='center',\n                      transform=axes[1,1].transAxes, fontsize=14)\n    axes[1,1].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\ndef demonstrate_object_recognition():\n    \"\"\"Demonstra reconhecimento de objetos\"\"\"\n    # Carregar cena e templates\n    scene = load_image_rgb(\"data/raw/cena.jpg\")  # Cena com m\u00faltiplos objetos\n    template1 = load_image_rgb(\"data/raw/template1.jpg\")  # Template de objeto 1\n    template2 = load_image_rgb(\"data/raw/template2.jpg\")  # Template de objeto 2\n\n    # Criar dicion\u00e1rio de templates\n    templates = {\n        'objeto1': template1,\n        'objeto2': template2\n    }\n\n    # Inicializar aplica\u00e7\u00e3o\n    app = FeatureApplications()\n\n    # Reconhecer objetos\n    recognized = app.object_recognition(templates, scene)\n\n    print(f\"Objetos reconhecidos: {len(recognized)}\")\n    for obj in recognized:\n        print(f\"  - {obj['name']}: confian\u00e7a {obj['confidence']:.3f}\")\n\n    # Desenhar resultados na cena\n    result_scene = scene.copy()\n    for obj in recognized:\n        points = obj['location']\n        # Desenhar pol\u00edgono ao redor do objeto reconhecido\n        cv2.polylines(result_scene, [points], True, (0, 255, 0), 2)\n        # Adicionar texto\n        cv2.putText(result_scene, f\"{obj['name']} ({obj['confidence']:.2f})\", \n                   tuple(points[0]), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n    # Mostrar resultado\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.imshow(scene)\n    plt.title('Cena Original')\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(result_scene)\n    plt.title('Objetos Reconhecidos')\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\nif __name__ == \"__main__\":\n    # Executar demonstra\u00e7\u00f5es\n    try:\n        demonstrate_feature_extraction()\n    except Exception as e:\n        print(f\"Erro na demonstra\u00e7\u00e3o de extra\u00e7\u00e3o de features: {e}\")\n\n    try:\n        demonstrate_object_recognition()\n    except Exception as e:\n        print(f\"Erro na demonstra\u00e7\u00e3o de reconhecimento de objetos: {e}\")\n</code></pre>"},{"location":"aulas/aula5/#resultado-esperado","title":"Resultado Esperado","text":"<p>Nesta aula, voc\u00ea:</p> <ol> <li>Aprendeu sobre keypoints e descritores de imagens</li> <li>Implementou o detector e descritor ORB</li> <li>Criou funcionalidades para matching de features</li> <li>Desenvolveu aplica\u00e7\u00f5es pr\u00e1ticas como AR, inspe\u00e7\u00e3o de qualidade e reconhecimento de objetos</li> <li>Testou os algoritmos em diferentes cen\u00e1rios</li> <li>Entendeu as aplica\u00e7\u00f5es reais dessas t\u00e9cnicas em AR, biometria e inspe\u00e7\u00e3o industrial</li> </ol> <p>Essas t\u00e9cnicas s\u00e3o fundamentais para muitas aplica\u00e7\u00f5es avan\u00e7adas de vis\u00e3o computacional e formam a base para sistemas mais complexos que utilizam aprendizado de m\u00e1quina.</p>"},{"location":"aulas/aula6/","title":"Aula 6 - Classifica\u00e7\u00e3o com ML Tradicional","text":""},{"location":"aulas/aula6/#objetivo-da-aula","title":"Objetivo da Aula","text":"<p>Implementar classificadores baseados em machine learning tradicional, utilizando extra\u00e7\u00e3o de features e pipelines sklearn, com foco em valida\u00e7\u00e3o e preven\u00e7\u00e3o de overfitting.</p>"},{"location":"aulas/aula6/#conteudo-teorico","title":"Conte\u00fado Te\u00f3rico","text":""},{"location":"aulas/aula6/#extracao-de-features-para-classificacao","title":"Extra\u00e7\u00e3o de Features para Classifica\u00e7\u00e3o","text":"<p>Para classifica\u00e7\u00e3o tradicional de imagens, \u00e9 necess\u00e1rio converter imagens em vetores num\u00e9ricos que representem caracter\u00edsticas relevantes:</p> <ul> <li>Features de cor: Histogramas de cores em diferentes espa\u00e7os (RGB, HSV, LAB)</li> <li>Features de textura: Propriedades estat\u00edsticas da distribui\u00e7\u00e3o de intensidades</li> <li>Features de forma: Caracter\u00edsticas geom\u00e9tricas dos objetos</li> <li>Features compostas: Combina\u00e7\u00f5es de diferentes tipos de features</li> </ul>"},{"location":"aulas/aula6/#pipeline-sklearn","title":"Pipeline Sklearn","text":"<p>O scikit-learn oferece uma estrutura padronizada para constru\u00e7\u00e3o de pipelines de ML:</p> <pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier())\n])\n</code></pre>"},{"location":"aulas/aula6/#tecnicas-de-validacao","title":"T\u00e9cnicas de Valida\u00e7\u00e3o","text":"<ul> <li>Holdout: Divis\u00e3o simples em treino/teste</li> <li>Cross-validation: Valida\u00e7\u00e3o cruzada para estimativa mais robusta</li> <li>Stratified sampling: Manuten\u00e7\u00e3o da propor\u00e7\u00e3o de classes em divis\u00f5es</li> </ul>"},{"location":"aulas/aula6/#overfitting","title":"Overfitting","text":"<p>Ocorre quando o modelo aprende excessivamente os dados de treinamento, perdendo capacidade de generaliza\u00e7\u00e3o. T\u00e9cnicas para mitiga\u00e7\u00e3o:</p> <ul> <li>Regulariza\u00e7\u00e3o: Penaliza\u00e7\u00e3o de modelos complexos</li> <li>Valida\u00e7\u00e3o cruzada: Estimativa mais realista do desempenho</li> <li>Early stopping: Parada prematura do treinamento</li> <li>Dados adicionais: Aumento do conjunto de treinamento</li> </ul>"},{"location":"aulas/aula6/#atividade-pratica","title":"Atividade Pr\u00e1tica","text":""},{"location":"aulas/aula6/#implementar-extrator-de-features","title":"Implementar Extrator de Features","text":"<pre><code># src/features/traditional_features.py\nimport cv2\nimport numpy as np\nfrom skimage.feature import local_binary_pattern, hog\nfrom skimage import exposure\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nclass TraditionalImageFeatures:\n    def __init__(self):\n        self.scaler = StandardScaler()\n\n    def extract_color_histogram(self, image, bins=32):\n        \"\"\"Extrai histograma de cores como feature\"\"\"\n        # Converter para HSV para melhor representa\u00e7\u00e3o de cor\n        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n\n        # Calcular histograma para cada canal\n        hist_h = cv2.calcHist([hsv], [0], None, [bins], [0, 180])\n        hist_s = cv2.calcHist([hsv], [1], None, [bins], [0, 256])\n        hist_v = cv2.calcHist([hsv], [2], None, [bins], [0, 256])\n\n        # Normalizar histogramas\n        hist_h = hist_h.flatten() / hist_h.sum()\n        hist_s = hist_s.flatten() / hist_s.sum()\n        hist_v = hist_v.flatten() / hist_v.sum()\n\n        # Concatenar histogramas\n        feature_vector = np.concatenate([hist_h, hist_s, hist_v])\n\n        return feature_vector\n\n    def extract_texture_features(self, image):\n        \"\"\"Extrai features de textura usando LBP e estat\u00edsticas\"\"\"\n        # Converter para escala de cinza\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n        # Local Binary Pattern\n        lbp = local_binary_pattern(gray, P=8, R=1, method='uniform')\n\n        # Histograma do LBP\n        lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), density=True)\n\n        # Estat\u00edsticas b\u00e1sicas\n        mean = np.mean(gray)\n        std = np.std(gray)\n        skewness = np.mean(((gray - mean) / std) ** 3) if std != 0 else 0\n        kurtosis = np.mean(((gray - mean) / std) ** 4) if std != 0 else 3\n\n        # Concatenar features\n        texture_features = np.concatenate([lbp_hist, [mean, std, skewness, kurtosis]])\n\n        return texture_features\n\n    def extract_shape_features(self, image):\n        \"\"\"Extrai features de forma baseadas em contornos\"\"\"\n        # Converter para escala de cinza e binarizar\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n\n        # Encontrar contornos\n        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        if not contours:\n            # Retornar zeros se n\u00e3o houver contornos\n            return np.zeros(10)\n\n        # Pegar o maior contorno\n        largest_contour = max(contours, key=cv2.contourArea)\n\n        # Calcular features de forma\n        area = cv2.contourArea(largest_contour)\n        perimeter = cv2.arcLength(largest_contour, True)\n\n        # Circularidade\n        circularity = 4 * np.pi * area / (perimeter * perimeter) if perimeter &gt; 0 else 0\n\n        # Raz\u00e3o de aspecto\n        x, y, w, h = cv2.boundingRect(largest_contour)\n        aspect_ratio = float(w) / h if h != 0 else 0\n\n        # Extens\u00e3o (extent)\n        extent = float(area) / (w * h) if w * h != 0 else 0\n\n        # Solidez (solidity)\n        hull = cv2.convexHull(largest_contour)\n        hull_area = cv2.contourArea(hull)\n        solidity = float(area) / hull_area if hull_area &gt; 0 else 0\n\n        # Equivalent diameter\n        equiv_diameter = np.sqrt(4 * area / np.pi) if area &gt;= 0 else 0\n\n        # Contar buracos (componentes internos)\n        # Esta \u00e9 uma aproxima\u00e7\u00e3o simples\n        holes = len(cv2.findContours(255 - binary, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)[0]) if area &gt; 0 else 0\n\n        shape_features = np.array([\n            area, perimeter, circularity, aspect_ratio, \n            extent, solidity, equiv_diameter, holes, w, h\n        ])\n\n        return shape_features\n\n    def extract_hog_features(self, image, pixels_per_cell=(8, 8), cells_per_block=(2, 2)):\n        \"\"\"Extrai features HOG (Histogram of Oriented Gradients)\"\"\"\n        # Converter para escala de cinza\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n        # Calcular HOG\n        features = hog(\n            gray,\n            orientations=9,\n            pixels_per_cell=pixels_per_cell,\n            cells_per_block=cells_per_block,\n            block_norm='L2-Hys',\n            feature_vector=True\n        )\n\n        return features\n\n    def extract_all_features(self, image):\n        \"\"\"Extrai todas as features e concatena\"\"\"\n        color_features = self.extract_color_histogram(image)\n        texture_features = self.extract_texture_features(image)\n        shape_features = self.extract_shape_features(image)\n        hog_features = self.extract_hog_features(image)\n\n        # Concatenar todas as features\n        all_features = np.concatenate([\n            color_features,\n            texture_features,\n            shape_features,\n            hog_features\n        ])\n\n        return all_features\n\n    def extract_features_batch(self, images):\n        \"\"\"Extrai features para um batch de imagens\"\"\"\n        features_list = []\n\n        for img in images:\n            features = self.extract_all_features(img)\n            features_list.append(features)\n\n        return np.array(features_list)\n\n    def fit_scaler(self, features):\n        \"\"\"Ajusta o scaler com base nas features\"\"\"\n        self.scaler.fit(features)\n\n    def transform_features(self, features):\n        \"\"\"Transforma features usando o scaler ajustado\"\"\"\n        return self.scaler.transform(features)\n</code></pre>"},{"location":"aulas/aula6/#implementar-pipeline-de-classificacao","title":"Implementar Pipeline de Classifica\u00e7\u00e3o","text":"<pre><code># src/models/traditional_classifier.py\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass TraditionalImageClassifier:\n    def __init__(self, classifier_type='random_forest', **kwargs):\n        \"\"\"\n        Inicializa o classificador tradicional\n        classifier_type: 'random_forest', 'svm', 'logistic_regression'\n        \"\"\"\n        self.classifier_type = classifier_type\n        self.feature_extractor = None\n        self.pipeline = None\n        self.classes_ = None\n\n        # Configurar classificador baseado no tipo\n        if classifier_type == 'random_forest':\n            self.classifier = RandomForestClassifier(random_state=42, **kwargs)\n        elif classifier_type == 'svm':\n            self.classifier = SVC(random_state=42, **kwargs)\n        elif classifier_type == 'logistic_regression':\n            self.classifier = LogisticRegression(random_state=42, **kwargs)\n        else:\n            raise ValueError(f\"Tipo de classificador n\u00e3o suportado: {classifier_type}\")\n\n    def setup_pipeline(self):\n        \"\"\"Configura o pipeline com scaler e classificador\"\"\"\n        self.pipeline = Pipeline([\n            ('scaler', StandardScaler()),\n            ('classifier', self.classifier)\n        ])\n\n    def prepare_data(self, images, labels, feature_extractor):\n        \"\"\"Prepara dados para treinamento\"\"\"\n        self.feature_extractor = feature_extractor\n\n        # Extrair features\n        X = feature_extractor.extract_features_batch(images)\n\n        # Armazenar classes\n        self.classes_ = np.unique(labels)\n\n        return X, labels\n\n    def train(self, X, y, cv_folds=5):\n        \"\"\"Treina o classificador\"\"\"\n        if self.pipeline is None:\n            self.setup_pipeline()\n\n        # Treinar o pipeline\n        self.pipeline.fit(X, y)\n\n        # Validar com cross-validation\n        cv_scores = cross_val_score(self.pipeline, X, y, cv=cv_folds, scoring='accuracy')\n\n        print(f\"Cross-validation scores: {cv_scores}\")\n        print(f\"Mean CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n\n        return cv_scores\n\n    def predict(self, X):\n        \"\"\"Faz previs\u00f5es\"\"\"\n        if self.pipeline is None:\n            raise ValueError(\"Modelo n\u00e3o foi treinado ainda\")\n\n        return self.pipeline.predict(X)\n\n    def predict_proba(self, X):\n        \"\"\"Faz previs\u00f5es com probabilidades (se suportado)\"\"\"\n        if self.pipeline is None:\n            raise ValueError(\"Modelo n\u00e3o foi treinado ainda\")\n\n        # Verificar se o classificador suporta probabilidades\n        if hasattr(self.pipeline.named_steps['classifier'], 'predict_proba'):\n            return self.pipeline.predict_proba(X)\n        else:\n            raise NotImplementedError(\"Este classificador n\u00e3o suporta previs\u00e3o de probabilidades\")\n\n    def evaluate(self, X_test, y_test):\n        \"\"\"Avalia o modelo\"\"\"\n        if self.pipeline is None:\n            raise ValueError(\"Modelo n\u00e3o foi treinado ainda\")\n\n        # Fazer previs\u00f5es\n        y_pred = self.predict(X_test)\n\n        # Calcular acur\u00e1cia\n        accuracy = accuracy_score(y_test, y_pred)\n\n        # Gerar relat\u00f3rio de classifica\u00e7\u00e3o\n        report = classification_report(y_test, y_pred, target_names=self.classes_)\n\n        # Gerar matriz de confus\u00e3o\n        cm = confusion_matrix(y_test, y_pred)\n\n        print(f\"Acur\u00e1cia no conjunto de teste: {accuracy:.3f}\")\n        print(\"\\nRelat\u00f3rio de Classifica\u00e7\u00e3o:\")\n        print(report)\n\n        return accuracy, report, cm\n\n    def plot_confusion_matrix(self, y_true, y_pred, title=\"Matriz de Confus\u00e3o\"):\n        \"\"\"Plota matriz de confus\u00e3o\"\"\"\n        cm = confusion_matrix(y_true, y_pred)\n\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                    xticklabels=self.classes_, yticklabels=self.classes_)\n        plt.title(title)\n        plt.ylabel('Verdadeiro')\n        plt.xlabel('Previsto')\n        plt.show()\n\n    def get_feature_importance(self):\n        \"\"\"Obt\u00e9m import\u00e2ncia das features (se dispon\u00edvel)\"\"\"\n        if self.classifier_type == 'random_forest':\n            return self.pipeline.named_steps['classifier'].feature_importances_\n        else:\n            raise NotImplementedError(\"Import\u00e2ncia de features dispon\u00edvel apenas para Random Forest\")\n\n    def save_model(self, filepath):\n        \"\"\"Salva o modelo treinado\"\"\"\n        model_data = {\n            'pipeline': self.pipeline,\n            'classes': self.classes_,\n            'classifier_type': self.classifier_type\n        }\n        joblib.dump(model_data, filepath)\n        print(f\"Modelo salvo em: {filepath}\")\n\n    def load_model(self, filepath):\n        \"\"\"Carrega modelo previamente treinado\"\"\"\n        model_data = joblib.load(filepath)\n        self.pipeline = model_data['pipeline']\n        self.classes_ = model_data['classes']\n        self.classifier_type = model_data['classifier_type']\n        print(f\"Modelo carregado de: {filepath}\")\n</code></pre>"},{"location":"aulas/aula6/#implementar-pipeline-completo","title":"Implementar Pipeline Completo","text":"<pre><code># src/pipelines/traditional_ml_pipeline.py\nfrom features.traditional_features import TraditionalImageFeatures\nfrom models.traditional_classifier import TraditionalImageClassifier\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nclass TraditionalMLPipeline:\n    def __init__(self, classifier_type='random_forest', feature_params=None):\n        self.feature_extractor = TraditionalImageFeatures()\n        self.classifier = TraditionalImageClassifier(classifier_type)\n        self.feature_params = feature_params or {}\n        self.is_trained = False\n\n    def prepare_dataset(self, images, labels):\n        \"\"\"Prepara o dataset completo\"\"\"\n        # Extrair features\n        print(\"Extraindo features...\")\n        X = self.feature_extractor.extract_features_batch(images)\n        y = np.array(labels)\n\n        print(f\"Features extra\u00eddas: {X.shape}\")\n\n        # Dividir em treino e teste\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.2, random_state=42, stratify=y\n        )\n\n        print(f\"Conjunto de treino: {X_train.shape[0]} amostras\")\n        print(f\"Conjunto de teste: {X_test.shape[0]} amostras\")\n\n        return X_train, X_test, y_train, y_test\n\n    def train(self, X_train, y_train, cv_folds=5):\n        \"\"\"Treina o modelo\"\"\"\n        print(\"Treinando modelo...\")\n\n        # Treinar o classificador\n        cv_scores = self.classifier.train(X_train, y_train, cv_folds)\n\n        self.is_trained = True\n\n        return cv_scores\n\n    def evaluate(self, X_test, y_test):\n        \"\"\"Avalia o modelo\"\"\"\n        if not self.is_trained:\n            raise ValueError(\"Modelo n\u00e3o foi treinado ainda\")\n\n        print(\"Avaliando modelo...\")\n        accuracy, report, cm = self.classifier.evaluate(X_test, y_test)\n\n        return accuracy, report, cm\n\n    def predict_single(self, image):\n        \"\"\"Faz previs\u00e3o para uma \u00fanica imagem\"\"\"\n        if not self.is_trained:\n            raise ValueError(\"Modelo n\u00e3o foi treinado ainda\")\n\n        # Extrair features da imagem\n        features = self.feature_extractor.extract_all_features(image)\n        features = features.reshape(1, -1)  # Adicionar dimens\u00e3o de batch\n\n        # Fazer previs\u00e3o\n        prediction = self.classifier.predict(features)[0]\n\n        # Obter probabilidade se dispon\u00edvel\n        try:\n            probabilities = self.classifier.predict_proba(features)[0]\n            return prediction, probabilities\n        except:\n            return prediction, None\n\n    def get_model_info(self):\n        \"\"\"Obt\u00e9m informa\u00e7\u00f5es sobre o modelo\"\"\"\n        if not self.is_trained:\n            return \"Modelo n\u00e3o treinado\"\n\n        info = {\n            'classifier_type': self.classifier.classifier_type,\n            'feature_dimension': self.feature_extractor.extract_all_features(\n                np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n            ).shape[0],\n            'classes': self.classifier.classes_.tolist() if self.classifier.classes_ is not None else None\n        }\n\n        return info\n</code></pre>"},{"location":"aulas/aula6/#exemplo-de-uso","title":"Exemplo de Uso","text":"<pre><code># src/examples/traditional_ml_example.py\nfrom pipelines.traditional_ml_pipeline import TraditionalMLPipeline\nfrom utils.io import load_image_rgb\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef create_sample_dataset():\n    \"\"\"Cria um dataset de exemplo (substituir com dados reais)\"\"\"\n    # Este \u00e9 um exemplo simplificado\n    # Em uma aplica\u00e7\u00e3o real, voc\u00ea carregaria imagens reais\n\n    # Simular imagens de 3 classes diferentes\n    n_samples_per_class = 50\n    classes = ['classe_a', 'classe_b', 'classe_c']\n\n    images = []\n    labels = []\n\n    for class_idx, class_name in enumerate(classes):\n        for _ in range(n_samples_per_class):\n            # Criar imagem simulada com padr\u00e3o diferente para cada classe\n            if class_name == 'classe_a':\n                # Padr\u00e3o com mais vermelho\n                img = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n                img[:, :, 0] = np.clip(img[:, :, 0] + 50, 0, 255)  # Mais vermelho\n            elif class_name == 'classe_b':\n                # Padr\u00e3o com mais verde\n                img = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n                img[:, :, 1] = np.clip(img[:, :, 1] + 50, 0, 255)  # Mais verde\n            else:  # classe_c\n                # Padr\u00e3o com mais azul\n                img = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n                img[:, :, 2] = np.clip(img[:, :, 2] + 50, 0, 255)  # Mais azul\n\n            images.append(img)\n            labels.append(class_name)\n\n    return images, labels\n\ndef demonstrate_traditional_ml():\n    \"\"\"Demonstra classifica\u00e7\u00e3o com ML tradicional\"\"\"\n    print(\"=== Demonstra\u00e7\u00e3o de Classifica\u00e7\u00e3o com ML Tradicional ===\\n\")\n\n    # Criar dataset de exemplo\n    print(\"Criando dataset de exemplo...\")\n    images, labels = create_sample_dataset()\n\n    print(f\"Dataset criado: {len(images)} imagens, {len(set(labels))} classes\")\n\n    # Inicializar pipeline\n    print(\"\\nInicializando pipeline...\")\n    pipeline = TraditionalMLPipeline(classifier_type='random_forest')\n\n    # Preparar dataset\n    X_train, X_test, y_train, y_test = pipeline.prepare_dataset(images, labels)\n\n    # Treinar modelo\n    cv_scores = pipeline.train(X_train, y_train, cv_folds=5)\n\n    # Avaliar modelo\n    accuracy, report, cm = pipeline.evaluate(X_test, y_test)\n\n    # Informa\u00e7\u00f5es do modelo\n    model_info = pipeline.get_model_info()\n    print(f\"\\nInforma\u00e7\u00f5es do modelo: {model_info}\")\n\n    # Testar previs\u00e3o em uma imagem\n    print(f\"\\nTestando previs\u00e3o em uma imagem...\")\n    sample_prediction, sample_probabilities = pipeline.predict_single(images[0])\n    print(f\"Predi\u00e7\u00e3o para imagem de exemplo: {sample_prediction}\")\n    if sample_probabilities is not None:\n        print(f\"Probabilidades: {sample_probabilities}\")\n\n    # Plotar matriz de confus\u00e3o\n    pipeline.classifier.plot_confusion_matrix(y_test, pipeline.classifier.predict(X_test))\n\n    return pipeline\n\ndef compare_classifiers():\n    \"\"\"Compara diferentes classificadores tradicionais\"\"\"\n    print(\"\\n=== Compara\u00e7\u00e3o de Classificadores Tradicionais ===\\n\")\n\n    # Criar dataset\n    images, labels = create_sample_dataset()\n\n    classifiers = ['random_forest', 'svm', 'logistic_regression']\n    results = {}\n\n    for clf_type in classifiers:\n        print(f\"\\nTreinando {clf_type}...\")\n\n        pipeline = TraditionalMLPipeline(classifier_type=clf_type)\n        X_train, X_test, y_train, y_test = pipeline.prepare_dataset(images, labels)\n\n        # Treinar\n        pipeline.train(X_train, y_train, cv_folds=3)  # Menos folds para acelerar\n\n        # Avaliar\n        accuracy, _, _ = pipeline.evaluate(X_test, y_test)\n        results[clf_type] = accuracy\n\n        print(f\"Acur\u00e1cia de {clf_type}: {accuracy:.3f}\")\n\n    print(f\"\\nResultados finais:\")\n    for clf_type, acc in results.items():\n        print(f\"  {clf_type}: {acc:.3f}\")\n\n    best_clf = max(results, key=results.get)\n    print(f\"\\nMelhor classificador: {best_clf} com acur\u00e1cia {results[best_clf]:.3f}\")\n\n    return results\n\ndef analyze_overfitting():\n    \"\"\"Analisa potencial overfitting\"\"\"\n    print(\"\\n=== An\u00e1lise de Overfitting ===\\n\")\n\n    # Criar dataset com mais amostras para an\u00e1lise\n    images, labels = create_sample_dataset()\n\n    # Dividir em treino e teste\n    from sklearn.model_selection import train_test_split\n    X_temp, _, y_temp, _ = train_test_split(\n        np.array(images), np.array(labels), test_size=0.2, random_state=42, stratify=labels\n    )\n\n    # Criar vers\u00f5es com diferentes tamanhos\n    sizes = [10, 20, 30, 40, 50]\n    train_accuracies = []\n    val_accuracies = []\n\n    for size in sizes:\n        # Pegar subset dos dados\n        subset_indices = np.random.choice(len(X_temp), size=size, replace=False)\n        X_subset = [images[i] for i in subset_indices]\n        y_subset = [labels[i] for i in subset_indices]\n\n        pipeline = TraditionalMLPipeline(classifier_type='random_forest')\n        X_train, X_test, y_train, y_test = pipeline.prepare_dataset(X_subset, y_subset)\n\n        # Treinar\n        pipeline.train(X_train, y_train, cv_folds=min(3, len(np.unique(y_train))))\n\n        # Avaliar em treino e valida\u00e7\u00e3o\n        train_pred = pipeline.classifier.predict(X_train)\n        train_acc = np.mean(train_pred == y_train)\n\n        val_pred = pipeline.classifier.predict(X_test)\n        val_acc = np.mean(val_pred == y_test)\n\n        train_accuracies.append(train_acc)\n        val_accuracies.append(val_acc)\n\n        print(f\"Tamanho: {size}, Treino: {train_acc:.3f}, Valida\u00e7\u00e3o: {val_acc:.3f}\")\n\n    # Plotar curvas de aprendizado\n    plt.figure(figsize=(10, 6))\n    plt.plot(sizes, train_accuracies, 'o-', label='Treino', color='blue')\n    plt.plot(sizes, val_accuracies, 'o-', label='Valida\u00e7\u00e3o', color='red')\n    plt.xlabel('Tamanho do Conjunto de Treino')\n    plt.ylabel('Acur\u00e1cia')\n    plt.title('Curva de Aprendizado - An\u00e1lise de Overfitting')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nif __name__ == \"__main__\":\n    # Executar demonstra\u00e7\u00f5es\n    trained_pipeline = demonstrate_traditional_ml()\n    compare_classifiers()\n    analyze_overfitting()\n</code></pre>"},{"location":"aulas/aula6/#resultado-esperado","title":"Resultado Esperado","text":"<p>Nesta aula, voc\u00ea:</p> <ol> <li>Implementou extratores de features tradicionais (cor, textura, forma, HOG)</li> <li>Criou um pipeline completo de classifica\u00e7\u00e3o com sklearn</li> <li>Comparou diferentes algoritmos de ML tradicional</li> <li>Aplicou t\u00e9cnicas de valida\u00e7\u00e3o cruzada</li> <li>Analisou o fen\u00f4meno de overfitting</li> <li>Testou o sistema com diferentes configura\u00e7\u00f5es</li> </ol> <p>Este pipeline de ML tradicional serve como base importante para compreens\u00e3o dos fundamentos de classifica\u00e7\u00e3o de imagens, mesmo com o advento do deep learning.</p>"},{"location":"aulas/aula7/","title":"Aula 7 - Construindo uma API para Modelo de CV","text":""},{"location":"aulas/aula7/#objetivo-da-aula","title":"Objetivo da Aula","text":"<p>Criar uma API funcional que transforme um classificador de vis\u00e3o computacional em um servi\u00e7o web, separando claramente as responsabilidades em Controller, Service e Model, elevando o n\u00edvel da disciplina para integra\u00e7\u00e3o real de modelos em sistemas.</p>"},{"location":"aulas/aula7/#conteudo-teorico","title":"Conte\u00fado Te\u00f3rico","text":""},{"location":"aulas/aula7/#arquitetura-de-apis-para-visao-computacional","title":"Arquitetura de APIs para Vis\u00e3o Computacional","text":"<p>Uma API bem projetada para vis\u00e3o computacional deve seguir princ\u00edpios de engenharia de software:</p> <ul> <li>Separa\u00e7\u00e3o de Responsabilidades: Cada componente tem uma fun\u00e7\u00e3o clara</li> <li>Baixo Acoplamento: Componentes devem depender minimamente uns dos outros</li> <li>Alta Coes\u00e3o: Funcionalidades relacionadas devem estar juntas</li> <li>Facilidade de Teste: Componentes devem ser facilmente test\u00e1veis isoladamente</li> </ul>"},{"location":"aulas/aula7/#padroes-de-arquitetura","title":"Padr\u00f5es de Arquitetura","text":""},{"location":"aulas/aula7/#mvc-model-view-controller","title":"MVC (Model-View-Controller)","text":"<ul> <li>Model: Representa os dados e a l\u00f3gica de neg\u00f3cios</li> <li>View: Interface com o usu\u00e1rio (n\u00e3o aplic\u00e1vel diretamente em APIs REST)</li> <li>Controller: Lida com as requisi\u00e7\u00f5es e respostas</li> </ul>"},{"location":"aulas/aula7/#clean-architecture","title":"Clean Architecture","text":"<ul> <li>Entities: Objetos de neg\u00f3cio centrais</li> <li>Use Cases: L\u00f3gica de neg\u00f3cios espec\u00edfica</li> <li>Interface Adapters: Adaptadores para frameworks e drivers</li> <li>Frameworks &amp; Drivers: Frameworks externos e UI</li> </ul>"},{"location":"aulas/aula7/#fastapi-vs-flask","title":"FastAPI vs Flask","text":"<p>FastAPI: - Moderno e r\u00e1pido (baseado em Starlette e Pydantic) - Suporte nativo a tipagem e documenta\u00e7\u00e3o autom\u00e1tica - Alto desempenho com async/await - Gera\u00e7\u00e3o autom\u00e1tica de documenta\u00e7\u00e3o OpenAPI</p> <p>Flask: - Leve e flex\u00edvel - Grande ecossistema de extens\u00f5es - Curva de aprendizado mais suave - Menos recursos embutidos</p>"},{"location":"aulas/aula7/#atividade-pratica","title":"Atividade Pr\u00e1tica","text":""},{"location":"aulas/aula7/#estrutura-do-projeto","title":"Estrutura do Projeto","text":"<pre><code>api_cv/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py                  # Ponto de entrada da API\n\u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 routers/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 vision_router.py # Rotas para vis\u00e3o computacional\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 health_router.py # Rotas de sa\u00fade do sistema\n\u2502   \u2502   \u2514\u2500\u2500 schemas/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 request_schemas.py  # Modelos de requisi\u00e7\u00e3o\n\u2502   \u2502       \u2514\u2500\u2500 response_schemas.py # Modelos de resposta\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 vision_service.py    # L\u00f3gica de neg\u00f3cio de vis\u00e3o computacional\n\u2502   \u2502   \u2514\u2500\u2500 model_service.py     # Gerenciamento de modelos\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 vision_models.py     # Modelos de dados\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 image_utils.py       # Utilit\u00e1rios para processamento de imagem\n\u2502   \u2502   \u2514\u2500\u2500 validation_utils.py  # Utilit\u00e1rios de valida\u00e7\u00e3o\n\u2502   \u2514\u2500\u2500 config/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 settings.py          # Configura\u00e7\u00f5es da aplica\u00e7\u00e3o\n\u251c\u2500\u2500 models/                      # Diret\u00f3rio para modelos treinados\n\u251c\u2500\u2500 tests/                       # Testes da API\n\u251c\u2500\u2500 requirements.txt             # Depend\u00eancias\n\u2514\u2500\u2500 Dockerfile                  # Para containeriza\u00e7\u00e3o\n</code></pre>"},{"location":"aulas/aula7/#configuracao-inicial","title":"Configura\u00e7\u00e3o Inicial","text":"<pre><code># app/config/settings.py\nfrom pydantic_settings import Settings\nfrom typing import Optional\n\nclass Settings(Settings):\n    app_name: str = \"API de Vis\u00e3o Computacional\"\n    app_version: str = \"1.0.0\"\n    debug: bool = False\n    model_path: str = \"models/classifier.pkl\"\n    allowed_image_types: list = [\"image/jpeg\", \"image/jpg\", \"image/png\", \"image/webp\"]\n    max_file_size: int = 10 * 1024 * 1024  # 10MB\n    api_prefix: str = \"/api/v1\"\n\n    class Config:\n        env_file = \".env\"\n\nsettings = Settings()\n</code></pre>"},{"location":"aulas/aula7/#modelos-de-dados-pydantic-schemas","title":"Modelos de Dados (Pydantic Schemas)","text":"<pre><code># app/api/schemas/request_schemas.py\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\nfrom enum import Enum\n\nclass ImageClassificationRequest(BaseModel):\n    \"\"\"Schema para requisi\u00e7\u00e3o de classifica\u00e7\u00e3o de imagem\"\"\"\n    image_url: Optional[str] = Field(None, description=\"URL da imagem para classifica\u00e7\u00e3o\")\n    image_base64: Optional[str] = Field(None, description=\"Imagem em formato base64\")\n\nclass ObjectDetectionRequest(BaseModel):\n    \"\"\"Schema para requisi\u00e7\u00e3o de detec\u00e7\u00e3o de objetos\"\"\"\n    image_url: Optional[str] = Field(None, description=\"URL da imagem para detec\u00e7\u00e3o\")\n    image_base64: Optional[str] = Field(None, description=\"Imagem em formato base64\")\n    confidence_threshold: float = Field(default=0.5, ge=0.0, le=1.0, description=\"Limiar de confian\u00e7a para detec\u00e7\u00e3o\")\n\nclass ImageFormat(str, Enum):\n    \"\"\"Formatos de imagem suportados\"\"\"\n    JPEG = \"jpeg\"\n    PNG = \"png\"\n    JPG = \"jpg\"\n    WEBP = \"webp\"\n\nclass PreprocessingRequest(BaseModel):\n    \"\"\"Schema para requisi\u00e7\u00e3o de pr\u00e9-processamento de imagem\"\"\"\n    image_url: Optional[str] = Field(None, description=\"URL da imagem para pr\u00e9-processamento\")\n    image_base64: Optional[str] = Field(None, description=\"Imagem em formato base64\")\n    target_format: Optional[ImageFormat] = Field(None, description=\"Formato de sa\u00edda desejado\")\n    target_size: Optional[tuple[int, int]] = Field(None, description=\"Tamanho de sa\u00edda (largura, altura)\")\n    apply_grayscale: bool = Field(default=False, description=\"Converter para escala de cinza\")\n    apply_blur: bool = Field(default=False, description=\"Aplicar desfoque gaussiano\")\n    blur_kernel_size: int = Field(default=5, description=\"Tamanho do kernel para desfoque\")\n</code></pre> <pre><code># app/api/schemas/response_schemas.py\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Optional\nfrom datetime import datetime\n\nclass BaseResponse(BaseModel):\n    \"\"\"Resposta base para todas as opera\u00e7\u00f5es\"\"\"\n    success: bool\n    message: Optional[str] = None\n    timestamp: datetime = datetime.now()\n\nclass ClassificationResponse(BaseResponse):\n    \"\"\"Resposta para classifica\u00e7\u00e3o de imagem\"\"\"\n    prediction: str\n    confidence: float\n    processing_time: float\n    all_predictions: Optional[List[Dict[str, float]]] = None\n\nclass DetectionResponse(BaseResponse):\n    \"\"\"Resposta para detec\u00e7\u00e3o de objetos\"\"\"\n    detections: List[Dict]\n    count: int\n    processing_time: float\n\nclass PreprocessingResponse(BaseResponse):\n    \"\"\"Resposta para pr\u00e9-processamento de imagem\"\"\"\n    processed_image_url: Optional[str] = None\n    processing_time: float\n    original_size: tuple[int, int]\n    processed_size: tuple[int, int]\n\nclass ModelInfoResponse(BaseResponse):\n    \"\"\"Resposta com informa\u00e7\u00f5es do modelo\"\"\"\n    model_name: str\n    model_version: str\n    input_shape: tuple\n    classes: List[str]\n    loaded_successfully: bool\n</code></pre>"},{"location":"aulas/aula7/#utilitarios","title":"Utilit\u00e1rios","text":"<pre><code># app/utils/image_utils.py\nimport base64\nimport io\nfrom PIL import Image\nimport numpy as np\nimport cv2\nfrom typing import Tuple, Optional\nfrom fastapi import HTTPException\nimport requests\nfrom config.settings import settings\n\ndef decode_base64_image(base64_str: str) -&gt; Image.Image:\n    \"\"\"Decodifica imagem de base64 para objeto PIL\"\"\"\n    try:\n        # Remover cabe\u00e7alho de dados se presente\n        if ',' in base64_str:\n            base64_str = base64_str.split(',')[1]\n\n        image_bytes = base64.b64decode(base64_str)\n        image = Image.open(io.BytesIO(image_bytes))\n        return image\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Erro ao decodificar imagem base64: {str(e)}\")\n\ndef load_image_from_url(url: str) -&gt; Image.Image:\n    \"\"\"Carrega imagem de URL\"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        image = Image.open(io.BytesIO(response.content))\n        return image\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Erro ao carregar imagem da URL: {str(e)}\")\n\ndef validate_image_content_type(content_type: str) -&gt; bool:\n    \"\"\"Valida tipo de conte\u00fado da imagem\"\"\"\n    return content_type.lower() in settings.allowed_image_types\n\ndef validate_image_size(image: Image.Image, max_pixels: int = 10000000) -&gt; bool:\n    \"\"\"Valida tamanho da imagem em pixels\"\"\"\n    return image.width * image.height &lt;= max_pixels\n\ndef pil_to_numpy(image: Image.Image) -&gt; np.ndarray:\n    \"\"\"Converte PIL Image para numpy array\"\"\"\n    return np.array(image)\n\ndef numpy_to_pil(array: np.ndarray) -&gt; Image.Image:\n    \"\"\"Converte numpy array para PIL Image\"\"\"\n    if array.dtype != np.uint8:\n        array = np.clip(array, 0, 255).astype(np.uint8)\n    return Image.fromarray(array)\n\ndef resize_image(image: Image.Image, target_size: Tuple[int, int]) -&gt; Image.Image:\n    \"\"\"Redimensiona imagem mantendo propor\u00e7\u00e3o se necess\u00e1rio\"\"\"\n    return image.resize(target_size, Image.Resampling.LANCZOS)\n\ndef convert_to_grayscale(image: Image.Image) -&gt; Image.Image:\n    \"\"\"Converte imagem para escala de cinza\"\"\"\n    if image.mode != 'L':\n        return image.convert('L')\n    return image\n\ndef apply_gaussian_blur(image: Image.Image, kernel_size: int) -&gt; Image.Image:\n    \"\"\"Aplica desfoque gaussiano\"\"\"\n    # Converter PIL para OpenCV\n    img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n\n    # Aplicar desfoque\n    blurred = cv2.GaussianBlur(img_cv, (kernel_size, kernel_size), 0)\n\n    # Converter de volta para PIL\n    blurred_pil = Image.fromarray(cv2.cvtColor(blurred, cv2.COLOR_BGR2RGB))\n\n    return blurred_pil\n</code></pre>"},{"location":"aulas/aula7/#servicos","title":"Servi\u00e7os","text":"<pre><code># app/services/model_service.py\nimport joblib\nimport pickle\nimport os\nfrom typing import Any, Dict\nimport tensorflow as tf\nimport numpy as np\nfrom config.settings import settings\n\nclass ModelService:\n    def __init__(self):\n        self.model = None\n        self.model_info = {}\n        self._load_model()\n\n    def _load_model(self):\n        \"\"\"Carrega modelo treinado\"\"\"\n        try:\n            model_path = settings.model_path\n\n            if not os.path.exists(model_path):\n                # Se n\u00e3o encontrar o modelo, criar um modelo mock para demonstra\u00e7\u00e3o\n                print(f\"Aviso: Modelo n\u00e3o encontrado em {model_path}. Criando modelo mock.\")\n                self.model = self._create_mock_model()\n                self.model_info = {\n                    'model_name': 'Mock Model',\n                    'model_version': '1.0.0',\n                    'input_shape': (224, 224, 3),\n                    'classes': ['classe_a', 'classe_b', 'classe_c'],\n                    'loaded_successfully': False\n                }\n                return\n\n            # Tentar carregar como modelo TensorFlow/Keras primeiro\n            if model_path.endswith('.h5') or model_path.endswith('.keras'):\n                self.model = tf.keras.models.load_model(model_path)\n                # Extrair informa\u00e7\u00f5es do modelo\n                self.model_info = {\n                    'model_name': os.path.basename(model_path),\n                    'model_version': '1.0.0',\n                    'input_shape': self.model.input_shape,\n                    'classes': getattr(self.model, 'classes', ['classe_a', 'classe_b', 'classe_c']),\n                    'loaded_successfully': True\n                }\n            else:\n                # Tentar carregar com joblib ou pickle\n                try:\n                    self.model = joblib.load(model_path)\n                except:\n                    with open(model_path, 'rb') as f:\n                        self.model = pickle.load(f)\n\n                # Informa\u00e7\u00f5es para modelos sklearn\n                self.model_info = {\n                    'model_name': os.path.basename(model_path),\n                    'model_version': '1.0.0',\n                    'input_shape': 'Vari\u00e1vel (depende do modelo)',\n                    'classes': list(getattr(self.model, 'classes_', ['classe_a', 'classe_b', 'classe_c'])),\n                    'loaded_successfully': True\n                }\n\n        except Exception as e:\n            print(f\"Erro ao carregar modelo: {e}\")\n            # Criar modelo mock\n            self.model = self._create_mock_model()\n            self.model_info = {\n                'model_name': 'Mock Model',\n                'model_version': '1.0.0',\n                'input_shape': (224, 224, 3),\n                'classes': ['classe_a', 'classe_b', 'classe_c'],\n                'loaded_successfully': False\n            }\n\n    def _create_mock_model(self):\n        \"\"\"Cria modelo mock para demonstra\u00e7\u00e3o\"\"\"\n        class MockModel:\n            def predict(self, X):\n                # Simular predi\u00e7\u00f5es\n                n_samples = X.shape[0]\n                return np.random.rand(n_samples, 3)  # 3 classes\n\n            def predict_proba(self, X):\n                # Simular probabilidades\n                n_samples = X.shape[0]\n                probs = np.random.rand(n_samples, 3)\n                probs = probs / probs.sum(axis=1, keepdims=True)  # Normalizar\n                return probs\n\n        return MockModel()\n\n    def get_model_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Retorna informa\u00e7\u00f5es sobre o modelo carregado\"\"\"\n        return self.model_info\n\n    def preprocess_input(self, image_array: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Pr\u00e9-processa entrada para o modelo\"\"\"\n        # Redimensionar para o tamanho esperado pelo modelo\n        if hasattr(self.model, 'input_shape'):\n            expected_shape = self.model.input_shape[1:]  # Remover dimens\u00e3o do batch\n            if len(expected_shape) == 3:  # Imagem colorida\n                from PIL import Image\n                img = Image.fromarray(image_array.astype('uint8'))\n                img = img.resize((expected_shape[1], expected_shape[0]))  # (width, height)\n                processed = np.array(img)\n            else:  # Imagem em escala de cinza\n                from PIL import Image\n                img = Image.fromarray(image_array.astype('uint8'))\n                img = img.resize((expected_shape[1], expected_shape[0]))\n                processed = np.array(img)\n                if len(processed.shape) == 3:\n                    processed = np.mean(processed, axis=2)  # Converter para escala de cinza\n                processed = processed.reshape(processed.shape + (1,))  # Adicionar dimens\u00e3o do canal\n        else:\n            # Para modelo mock, usar tamanho padr\u00e3o\n            from PIL import Image\n            img = Image.fromarray(image_array.astype('uint8'))\n            img = img.resize((224, 224))\n            processed = np.array(img)\n\n        # Normalizar valores para o intervalo [0, 1] se necess\u00e1rio\n        if processed.max() &gt; 1.0:\n            processed = processed.astype(np.float32) / 255.0\n\n        # Adicionar dimens\u00e3o do batch\n        if len(processed.shape) == 3:\n            processed = np.expand_dims(processed, axis=0)\n\n        return processed\n\n    def predict(self, image_array: np.ndarray) -&gt; Dict[str, Any]:\n        \"\"\"Faz predi\u00e7\u00e3o com o modelo\"\"\"\n        import time\n        start_time = time.time()\n\n        # Pr\u00e9-processar imagem\n        processed_input = self.preprocess_input(image_array)\n\n        # Fazer predi\u00e7\u00e3o\n        if hasattr(self.model, 'predict_proba'):\n            probabilities = self.model.predict_proba(processed_input)[0]\n        else:\n            predictions = self.model.predict(processed_input)[0]\n            if predictions.ndim == 0:  # Se for um escalar\n                probabilities = np.zeros(len(self.model_info['classes']))\n                probabilities[int(predictions)] = 1.0\n            else:\n                probabilities = predictions\n\n        # Obter classe com maior probabilidade\n        predicted_class_idx = np.argmax(probabilities)\n        predicted_class = self.model_info['classes'][predicted_class_idx]\n        confidence = float(probabilities[predicted_class_idx])\n\n        # Obter todas as predi\u00e7\u00f5es com probabilidades\n        all_predictions = [\n            {\"class\": cls, \"probability\": float(prob)}\n            for cls, prob in zip(self.model_info['classes'], probabilities)\n        ]\n\n        processing_time = time.time() - start_time\n\n        return {\n            'prediction': predicted_class,\n            'confidence': confidence,\n            'all_predictions': all_predictions,\n            'processing_time': processing_time\n        }\n</code></pre> <pre><code># app/services/visionservice.py\nimport numpy as np\nfrom typing import Dict, List\nfrom PIL import Image\nimport cv2\nimport time\nfrom utils.image_utils import pil_to_numpy, numpy_to_pil\nfrom services.model_service import ModelService\n\nclass VisionService:\n    def __init__(self):\n        self.model_service = ModelService()\n\n    def classify_image(self, image: Image.Image) -&gt; Dict:\n        \"\"\"Classifica uma imagem\"\"\"\n        image_array = pil_to_numpy(image)\n        return self.model_service.predict(image_array)\n\n    def detect_objects(self, image: Image.Image, confidence_threshold: float = 0.5) -&gt; Dict:\n        \"\"\"Detecta objetos em uma imagem (implementa\u00e7\u00e3o mock para demonstra\u00e7\u00e3o)\"\"\"\n        start_time = time.time()\n\n        # Converter imagem para array numpy\n        image_array = pil_to_numpy(image)\n        height, width = image_array.shape[:2]\n\n        # Simular detec\u00e7\u00f5es (em uma implementa\u00e7\u00e3o real, usaria um modelo como YOLO)\n        # Gerar detec\u00e7\u00f5es mock\n        mock_detections = [\n            {\n                \"label\": \"pessoa\",\n                \"confidence\": 0.89,\n                \"bbox\": [int(0.1 * width), int(0.2 * height), \n                         int(0.3 * width), int(0.4 * height)],\n                \"coordinates\": {\n                    \"x_min\": int(0.1 * width),\n                    \"y_min\": int(0.2 * height),\n                    \"x_max\": int(0.3 * width),\n                    \"y_max\": int(0.4 * height)\n                }\n            },\n            {\n                \"label\": \"carro\",\n                \"confidence\": 0.76,\n                \"bbox\": [int(0.5 * width), int(0.3 * height), \n                         int(0.8 * width), int(0.7 * height)],\n                \"coordinates\": {\n                    \"x_min\": int(0.5 * width),\n                    \"y_min\": int(0.3 * height),\n                    \"x_max\": int(0.8 * width),\n                    \"y_max\": int(0.7 * height)\n                }\n            }\n        ]\n\n        # Filtrar por threshold de confian\u00e7a\n        filtered_detections = [\n            det for det in mock_detections \n            if det[\"confidence\"] &gt;= confidence_threshold\n        ]\n\n        processing_time = time.time() - start_time\n\n        return {\n            'detections': filtered_detections,\n            'count': len(filtered_detections),\n            'processing_time': processing_time\n        }\n\n    def preprocess_image(self, \n                        image: Image.Image, \n                        target_format: str = None,\n                        target_size: tuple = None,\n                        apply_grayscale: bool = False,\n                        apply_blur: bool = False,\n                        blur_kernel_size: int = 5) -&gt; Dict:\n        \"\"\"Pr\u00e9-processa imagem conforme especifica\u00e7\u00f5es\"\"\"\n        start_time = time.time()\n\n        original_size = image.size\n\n        # Aplicar transforma\u00e7\u00f5es\n        processed_image = image.copy()\n\n        if apply_grayscale:\n            processed_image = processed_image.convert('L').convert('RGB')  # Converter para L e depois para RGB para manter 3 canais\n\n        if apply_blur:\n            from utils.image_utils import apply_gaussian_blur\n            processed_image = apply_gaussian_blur(processed_image, blur_kernel_size)\n\n        if target_size:\n            from utils.image_utils import resize_image\n            processed_image = resize_image(processed_image, target_size)\n\n        if target_format:\n            # Ajustar formato (na verdade, o PIL j\u00e1 armazena internamente)\n            pass\n\n        processed_size = processed_image.size\n\n        processing_time = time.time() - start_time\n\n        return {\n            'processed_image': processed_image,\n            'processing_time': processing_time,\n            'original_size': original_size,\n            'processed_size': processed_size\n        }\n</code></pre>"},{"location":"aulas/aula7/#controladores-routers","title":"Controladores (Routers)","text":"<pre><code># app/api/routers/vision_router.py\nfrom fastapi import APIRouter, File, UploadFile, HTTPException, Query\nfrom fastapi.responses import JSONResponse\nfrom typing import Optional\nimport base64\nimport io\nfrom PIL import Image\n\nfrom api.schemas.request_schemas import (\n    ImageClassificationRequest, \n    ObjectDetectionRequest, \n    PreprocessingRequest\n)\nfrom api.schemas.response_schemas import (\n    ClassificationResponse, \n    DetectionResponse, \n    PreprocessingResponse, \n    ModelInfoResponse\n)\nfrom services.vision_service import VisionService\nfrom utils.image_utils import (\n    decode_base64_image, \n    load_image_from_url, \n    validate_image_content_type,\n    pil_to_numpy\n)\n\nrouter = APIRouter(prefix=\"/vision\", tags=[\"Vis\u00e3o Computacional\"])\n\n# Instanciar servi\u00e7o\nvision_service = VisionService()\n\n@router.post(\"/classify\", response_model=ClassificationResponse)\nasync def classify_image(\n    request: ImageClassificationRequest = None,\n    file: UploadFile = File(None)\n):\n    \"\"\"\n    Classifica uma imagem usando modelo de vis\u00e3o computacional\n    \"\"\"\n    try:\n        # Obter imagem de diferentes fontes\n        image = None\n\n        if file:\n            # Upload via multipart form\n            if not validate_image_content_type(file.content_type):\n                raise HTTPException(status_code=400, detail=\"Tipo de arquivo n\u00e3o suportado\")\n\n            contents = await file.read()\n            image = Image.open(io.BytesIO(contents))\n\n        elif request:\n            if request.image_base64:\n                # Imagem em base64\n                image = decode_base64_image(request.image_base64)\n            elif request.image_url:\n                # Imagem via URL\n                image = load_image_from_url(request.image_url)\n\n        if image is None:\n            raise HTTPException(status_code=400, detail=\"Nenhuma imagem fornecida\")\n\n        # Classificar imagem\n        result = vision_service.classify_image(image)\n\n        return ClassificationResponse(\n            success=True,\n            prediction=result['prediction'],\n            confidence=result['confidence'],\n            processing_time=result['processing_time'],\n            all_predictions=result.get('all_predictions')\n        )\n\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Erro interno: {str(e)}\")\n\n@router.post(\"/detect_objects\", response_model=DetectionResponse)\nasync def detect_objects(\n    request: ObjectDetectionRequest,\n    file: UploadFile = File(None)\n):\n    \"\"\"\n    Detecta objetos em uma imagem\n    \"\"\"\n    try:\n        # Obter imagem\n        image = None\n\n        if file:\n            if not validate_image_content_type(file.content_type):\n                raise HTTPException(status_code=400, detail=\"Tipo de arquivo n\u00e3o suportado\")\n\n            contents = await file.read()\n            image = Image.open(io.BytesIO(contents))\n        elif request.image_base64:\n            image = decode_base64_image(request.image_base64)\n        elif request.image_url:\n            image = load_image_from_url(request.image_url)\n\n        if image is None:\n            raise HTTPException(status_code=400, detail=\"Nenhuma imagem fornecida\")\n\n        # Detectar objetos\n        result = vision_service.detect_objects(image, request.confidence_threshold)\n\n        return DetectionResponse(\n            success=True,\n            detections=result['detections'],\n            count=result['count'],\n            processing_time=result['processing_time']\n        )\n\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Erro interno: {str(e)}\")\n\n@router.post(\"/preprocess\", response_model=PreprocessingResponse)\nasync def preprocess_image(request: PreprocessingRequest):\n    \"\"\"\n    Pr\u00e9-processa uma imagem conforme especifica\u00e7\u00f5es\n    \"\"\"\n    try:\n        # Obter imagem\n        image = None\n\n        if request.image_base64:\n            image = decode_base64_image(request.image_base64)\n        elif request.image_url:\n            image = load_image_from_url(request.image_url)\n\n        if image is None:\n            raise HTTPException(status_code=400, detail=\"Nenhuma imagem fornecida\")\n\n        # Pr\u00e9-processar imagem\n        result = vision_service.preprocess_image(\n            image=image,\n            target_format=request.target_format,\n            target_size=request.target_size,\n            apply_grayscale=request.apply_grayscale,\n            apply_blur=request.apply_blur,\n            blur_kernel_size=request.blur_kernel_size\n        )\n\n        # Converter imagem processada para base64 para resposta (opcional)\n        buffered = io.BytesIO()\n        result['processed_image'].save(buffered, format=\"JPEG\")\n        img_str = base64.b64encode(buffered.getvalue()).decode()\n\n        return PreprocessingResponse(\n            success=True,\n            processed_image_url=f\"data:image/jpeg;base64,{img_str}\",\n            processing_time=result['processing_time'],\n            original_size=result['original_size'],\n            processed_size=result['processed_size']\n        )\n\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Erro interno: {str(e)}\")\n\n@router.get(\"/model_info\", response_model=ModelInfoResponse)\nasync def get_model_info():\n    \"\"\"\n    Obt\u00e9m informa\u00e7\u00f5es sobre o modelo de vis\u00e3o computacional carregado\n    \"\"\"\n    model_info = vision_service.model_service.get_model_info()\n\n    return ModelInfoResponse(\n        success=True,\n        model_name=model_info['model_name'],\n        model_version=model_info['model_version'],\n        input_shape=model_info['input_shape'],\n        classes=model_info['classes'],\n        loaded_successfully=model_info['loaded_successfully']\n    )\n</code></pre> <pre><code># app/api/routers/health_router.py\nfrom fastapi import APIRouter\nfrom api.schemas.response_schemas import BaseResponse\n\nrouter = APIRouter(prefix=\"/health\", tags=[\"Health\"])\n\n@router.get(\"\", response_model=BaseResponse)\nasync def health_check():\n    \"\"\"\n    Verifica se o servi\u00e7o est\u00e1 ativo e saud\u00e1vel\n    \"\"\"\n    return BaseResponse(\n        success=True,\n        message=\"API de Vis\u00e3o Computacional est\u00e1 ativa!\",\n    )\n\n@router.get(\"/ready\")\nasync def readiness_check():\n    \"\"\"\n    Verifica se o servi\u00e7o est\u00e1 pronto para receber requisi\u00e7\u00f5es\n    \"\"\"\n    # Aqui voc\u00ea pode adicionar verifica\u00e7\u00f5es mais espec\u00edficas\n    # como conex\u00e3o com banco de dados, disponibilidade de modelos, etc.\n    return {\"status\": \"ready\"}\n</code></pre>"},{"location":"aulas/aula7/#ponto-de-entrada-principal","title":"Ponto de Entrada Principal","text":"<pre><code># app/main.py\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom api.routers import vision_router, health_router\nfrom config.settings import settings\n\n# Criar inst\u00e2ncia do FastAPI\napp = FastAPI(\n    title=settings.app_name,\n    version=settings.app_version,\n    debug=settings.debug,\n    description=\"API para servi\u00e7os de Vis\u00e3o Computacional\"\n)\n\n# Configurar CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Em produ\u00e7\u00e3o, substituir por dom\u00ednios espec\u00edficos\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Incluir roteadores\napp.include_router(health_router.router)\napp.include_router(vision_router.router)\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"\n    Ponto de entrada raiz da API\n    \"\"\"\n    return {\n        \"message\": \"Bem-vindo \u00e0 API de Vis\u00e3o Computacional!\",\n        \"version\": settings.app_version,\n        \"documentation\": \"/docs\",\n        \"redoc\": \"/redoc\"\n    }\n\n# Para rodar: uvicorn app.main:app --reload --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"aulas/aula7/#arquivo-de-requisitos","title":"Arquivo de Requisitos","text":"<pre><code># requirements.txt\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\npydantic==2.5.0\npydantic-settings==2.1.0\nPillow==10.1.0\nnumpy==1.24.3\nopencv-python==4.8.1.78\nscikit-learn==1.3.0\ntensorflow==2.15.0\njoblib==1.3.2\nrequests==2.31.0\npython-multipart==0.0.6\npython-dotenv==1.0.0\n</code></pre>"},{"location":"aulas/aula7/#exemplo-de-teste","title":"Exemplo de Teste","text":"<pre><code># tests/test_api.py\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom PIL import Image\nimport io\nimport numpy as np\nfrom app.main import app\n\nclient = TestClient(app)\n\ndef create_test_image(width=224, height=224):\n    \"\"\"Cria imagem de teste\"\"\"\n    image_array = np.random.randint(0, 255, (height, width, 3), dtype=np.uint8)\n    image = Image.fromarray(image_array)\n\n    img_byte_arr = io.BytesIO()\n    image.save(img_byte_arr, format='JPEG')\n    img_byte_arr.seek(0)\n\n    return img_byte_arr\n\ndef test_health_endpoint():\n    \"\"\"Testa endpoint de sa\u00fade\"\"\"\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"success\"] is True\n    assert \"message\" in data\n\ndef test_classify_endpoint_with_upload():\n    \"\"\"Testa endpoint de classifica\u00e7\u00e3o com upload de arquivo\"\"\"\n    test_image = create_test_image()\n\n    response = client.post(\n        \"/api/v1/vision/classify\",\n        files={\"file\": (\"test.jpg\", test_image, \"image/jpeg\")}\n    )\n\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"success\"] is True\n    assert \"prediction\" in data\n    assert \"confidence\" in data\n\ndef test_classify_endpoint_with_base64():\n    \"\"\"Testa endpoint de classifica\u00e7\u00e3o com imagem em base64\"\"\"\n    test_image = create_test_image()\n    import base64\n\n    # Converter imagem para base64\n    test_image.seek(0)\n    img_bytes = test_image.read()\n    img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n\n    response = client.post(\n        \"/api/v1/vision/classify\",\n        json={\"image_base64\": f\"data:image/jpeg;base64,{img_base64}\"}\n    )\n\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"success\"] is True\n\ndef test_model_info():\n    \"\"\"Testa endpoint de informa\u00e7\u00f5es do modelo\"\"\"\n    response = client.get(\"/api/v1/vision/model_info\")\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"success\"] is True\n    assert \"model_name\" in data\n    assert \"classes\" in data\n</code></pre>"},{"location":"aulas/aula7/#resultado-esperado","title":"Resultado Esperado","text":"<p>Nesta aula, voc\u00ea:</p> <ol> <li>Aprendeu a arquitetura de APIs para vis\u00e3o computacional</li> <li>Implementou uma API completa com FastAPI seguindo padr\u00f5es de engenharia</li> <li>Separou claramente as responsabilidades em Controller, Service e Model</li> <li>Criou modelos de requisi\u00e7\u00e3o e resposta com Pydantic</li> <li>Implementou servi\u00e7os para gerenciamento de modelos e processamento de imagens</li> <li>Adicionou endpoints para classifica\u00e7\u00e3o, detec\u00e7\u00e3o e pr\u00e9-processamento</li> <li>Configurou middlewares e tratamento de erros</li> <li>Criou testes para validar a funcionalidade da API</li> </ol> <p>Agora voc\u00ea tem uma API funcional que pode ser integrada a qualquer sistema, permitindo que outros servi\u00e7os consumam suas capacidades de vis\u00e3o computacional.</p>"},{"location":"aulas/aula8/","title":"Aula 8 - CP2","text":""},{"location":"aulas/aula8/#objetivo","title":"Objetivo","text":"<p>Avalia\u00e7\u00e3o do projeto entregue na CP2, com foco em arquitetura, funcionalidade e organiza\u00e7\u00e3o do c\u00f3digo.</p>"},{"location":"aulas/aula8/#criterios-de-avaliacao","title":"Crit\u00e9rios de Avalia\u00e7\u00e3o","text":"<p>Durante esta aula, ser\u00e3o avaliados os seguintes aspectos do projeto entregue:</p>"},{"location":"aulas/aula8/#arquitetura-30","title":"Arquitetura (30%)","text":"<ul> <li>Separa\u00e7\u00e3o adequada de responsabilidades (Controller, Service, Model)</li> <li>Estrutura de diret\u00f3rios organizada</li> <li>Uso apropriado de padr\u00f5es de projeto</li> </ul>"},{"location":"aulas/aula8/#funcionalidade-30","title":"Funcionalidade (30%)","text":"<ul> <li>API funcional e respondendo corretamente \u00e0s requisi\u00e7\u00f5es</li> <li>Classifica\u00e7\u00e3o de imagens funcionando adequadamente</li> <li>Tratamento adequado de erros</li> </ul>"},{"location":"aulas/aula8/#organizacao-do-codigo-20","title":"Organiza\u00e7\u00e3o do C\u00f3digo (20%)","text":"<ul> <li>C\u00f3digo limpo e bem comentado</li> <li>Nomenclatura consistente</li> <li>Seguimento de conven\u00e7\u00f5es de estilo</li> </ul>"},{"location":"aulas/aula8/#documentacao-20","title":"Documenta\u00e7\u00e3o (20%)","text":"<ul> <li>README com instru\u00e7\u00f5es claras</li> <li>Documenta\u00e7\u00e3o dos endpoints</li> <li>Explica\u00e7\u00e3o do funcionamento do sistema</li> </ul>"},{"location":"aulas/aula8/#atividade","title":"Atividade","text":"<p>Durante esta aula, os alunos dever\u00e3o:</p> <ol> <li>Apresentar sua API de classifica\u00e7\u00e3o</li> <li>Demonstrar o funcionamento dos endpoints</li> <li>Explicar a arquitetura escolhida</li> <li>Discutir desafios enfrentados e solu\u00e7\u00f5es implementadas</li> </ol>"},{"location":"aulas/aula9/","title":"Aula 9 - CNN na Pr\u00e1tica (Sem Matem\u00e1tica Excessiva)","text":""},{"location":"aulas/aula9/#objetivo-da-aula","title":"Objetivo da Aula","text":"<p>Compreender o funcionamento interno das Redes Neurais Convolucionais (CNNs) de forma visual e intuitiva, implementar transfer learning com modelos pr\u00e9-existentes e aplicar fine-tuning em datasets espec\u00edficos.</p>"},{"location":"aulas/aula9/#conteudo-teorico","title":"Conte\u00fado Te\u00f3rico","text":""},{"location":"aulas/aula9/#como-funcionam-as-cnns-visao-intuitiva","title":"Como Funcionam as CNNs (Vis\u00e3o Intuitiva)","text":"<p>As Redes Neurais Convolucionais s\u00e3o projetadas para reconhecer padr\u00f5es visuais em imagens. Podemos pensar nelas como um sistema com m\u00faltiplas etapas de \"detec\u00e7\u00e3o\":</p> <ol> <li>Camadas Iniciais: Detectam padr\u00f5es simples como linhas, bordas e cantos</li> <li>Camadas Intermedi\u00e1rias: Combinam padr\u00f5es simples para formar formas mais complexas</li> <li>Camadas Finais: Reconhecem objetos completos com base nos padr\u00f5es anteriores</li> </ol>"},{"location":"aulas/aula9/#transfer-learning","title":"Transfer Learning","text":"<p>O Transfer Learning \u00e9 uma t\u00e9cnica que permite reutilizar um modelo pr\u00e9-treinado em uma nova tarefa, economizando tempo e recursos computacionais.</p> <p>Vantagens: - Menos dados necess\u00e1rios - Tempo de treinamento reduzido - Melhor desempenho em datasets menores - Menor poder computacional exigido</p>"},{"location":"aulas/aula9/#arquiteturas-populares","title":"Arquiteturas Populares","text":"<ul> <li>VGG: Simples e eficaz, bom para aprendizado</li> <li>ResNet: Excelente desempenho, permite redes muito profundas</li> <li>MobileNet: Otimizado para dispositivos m\u00f3veis</li> <li>EfficientNet: Bom equil\u00edbrio entre acur\u00e1cia e efici\u00eancia</li> </ul>"},{"location":"aulas/aula9/#atividade-pratica","title":"Atividade Pr\u00e1tica","text":""},{"location":"aulas/aula9/#implementar-transfer-learning-com-tensorflowkeras","title":"Implementar Transfer Learning com TensorFlow/Keras","text":"<pre><code># src/models/cnn_transfer_learning.py\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass CNNTransferLearning:\n    def __init__(self, base_model_name='vgg16', input_shape=(224, 224, 3), num_classes=2):\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.base_model_name = base_model_name\n        self.model = None\n        self.history = None\n\n        # Carregar modelo base\n        self.base_model = self._load_base_model()\n\n    def _load_base_model(self):\n        \"\"\"Carrega modelo pr\u00e9-treinado\"\"\"\n        if self.base_model_name.lower() == 'vgg16':\n            base_model = VGG16(\n                weights='imagenet',\n                include_top=False,\n                input_shape=self.input_shape\n            )\n        elif self.base_model_name.lower() == 'resnet50':\n            base_model = ResNet50(\n                weights='imagenet',\n                include_top=False,\n                input_shape=self.input_shape\n            )\n        elif self.base_model_name.lower() == 'mobilenetv2':\n            base_model = MobileNetV2(\n                weights='imagenet',\n                include_top=False,\n                input_shape=self.input_shape\n            )\n        else:\n            raise ValueError(f\"Modelo {self.base_model_name} n\u00e3o suportado\")\n\n        # Congelar pesos do modelo base\n        base_model.trainable = False\n\n        return base_model\n\n    def build_model(self):\n        \"\"\"Constr\u00f3i modelo com cabe\u00e7a personalizada\"\"\"\n        inputs = tf.keras.Input(shape=self.input_shape)\n\n        # Pr\u00e9-processamento do modelo base\n        if self.base_model_name.lower() in ['vgg16', 'resnet50']:\n            x = tf.keras.applications.vgg16.preprocess_input(inputs)\n        elif self.base_model_name.lower() == 'mobilenetv2':\n            x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n\n        # Extrair features\n        x = self.base_model(x, training=False)\n\n        # Camadas personalizadas\n        x = layers.GlobalAveragePooling2D()(x)\n        x = layers.Dropout(0.2)(x)\n        x = layers.Dense(128, activation='relu')(x)\n        x = layers.Dropout(0.2)(x)\n        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n\n        self.model = models.Model(inputs, outputs)\n\n        return self.model\n\n    def compile_model(self, learning_rate=0.0001):\n        \"\"\"Compila o modelo\"\"\"\n        self.model.compile(\n            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n    def train_initial(self, train_dataset, validation_dataset, epochs=10):\n        \"\"\"Treina a cabe\u00e7a personalizada\"\"\"\n        self.history = self.model.fit(\n            train_dataset,\n            epochs=epochs,\n            validation_data=validation_dataset,\n            verbose=1\n        )\n        return self.history\n\n    def unfreeze_model(self, fine_tune_from_layer=100):\n        \"\"\"Descongela parte do modelo base para fine-tuning\"\"\"\n        self.base_model.trainable = True\n\n        # Congelar as primeiras camadas (menos espec\u00edficas)\n        fine_tune_at = len(self.base_model.layers) - fine_tune_from_layer\n        for layer in self.base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n\n    def fine_tune(self, train_dataset, validation_dataset, epochs=10, learning_rate=0.0001/10):\n        \"\"\"Realiza fine-tuning\"\"\"\n        # Recompilar com learning rate menor\n        self.model.compile(\n            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n        )\n\n        # Treinar novamente\n        fine_tune_history = self.model.fit(\n            train_dataset,\n            epochs=epochs,\n            validation_data=validation_dataset,\n            verbose=1\n        )\n\n        return fine_tune_history\n\n    def visualize_model_architecture(self):\n        \"\"\"Visualiza arquitetura do modelo\"\"\"\n        if self.model:\n            print(self.model.summary())\n        else:\n            print(\"Modelo n\u00e3o constru\u00eddo ainda\")\n\n    def plot_training_history(self):\n        \"\"\"Plota hist\u00f3rico de treinamento\"\"\"\n        if self.history:\n            acc = self.history.history['accuracy']\n            val_acc = self.history.history['val_accuracy']\n            loss = self.history.history['loss']\n            val_loss = self.history.history['val_loss']\n\n            epochs_range = range(len(acc))\n\n            plt.figure(figsize=(12, 4))\n\n            plt.subplot(1, 2, 1)\n            plt.plot(epochs_range, acc, label='Acur\u00e1cia de Treino')\n            plt.plot(epochs_range, val_acc, label='Acur\u00e1cia de Valida\u00e7\u00e3o')\n            plt.legend(loc='lower right')\n            plt.title('Acur\u00e1cia de Treino e Valida\u00e7\u00e3o')\n\n            plt.subplot(1, 2, 2)\n            plt.plot(epochs_range, loss, label='Perda de Treino')\n            plt.plot(epochs_range, val_loss, label='Perda de Valida\u00e7\u00e3o')\n            plt.legend(loc='upper right')\n            plt.title('Perda de Treino e Valida\u00e7\u00e3o')\n\n            plt.show()\n        else:\n            print(\"Nenhum hist\u00f3rico de treinamento dispon\u00edvel\")\n</code></pre>"},{"location":"aulas/aula9/#visualizacao-de-features-extraidas","title":"Visualiza\u00e7\u00e3o de Features Extra\u00eddas","text":"<pre><code># src/utils/cnn_visualization.py\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef visualize_feature_maps(model, image, layer_names=None):\n    \"\"\"Visualiza feature maps de camadas espec\u00edficas\"\"\"\n    # Preparar imagem\n    img = tf.expand_dims(image, axis=0)\n\n    # Criar modelo intermedi\u00e1rio para extrair features\n    if layer_names is None:\n        # Pegar as primeiras camadas convolucionais\n        layer_names = []\n        for layer in model.layers:\n            if 'conv' in layer.name:\n                layer_names.append(layer.name)\n                if len(layer_names) &gt;= 4:  # Limitar a 4 camadas\n                    break\n\n    # Criar modelo que retorna outputs das camadas selecionadas\n    outputs = [model.get_layer(name).output for name in layer_names]\n    feature_model = tf.keras.Model(inputs=model.input, outputs=outputs)\n\n    # Obter features\n    feature_maps = feature_model.predict(img)\n\n    # Visualizar\n    for i, feature_map in enumerate(feature_maps):\n        n_features = feature_map.shape[-1]\n        size = feature_map.shape[1]\n\n        # Limitar n\u00famero de features para visualiza\u00e7\u00e3o\n        n_cols = min(8, n_features)\n        n_rows = min(4, (n_features // n_cols) + (1 if n_features % n_cols else 0))\n\n        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n        fig.suptitle(f'Feature Maps - Camada: {layer_names[i]}')\n\n        for j in range(min(n_features, n_rows * n_cols)):\n            row, col = j // n_cols, j % n_cols\n\n            if n_rows == 1:\n                ax = axes[col] if n_cols &gt; 1 else axes\n            elif n_cols == 1:\n                ax = axes[row]\n            else:\n                ax = axes[row, col]\n\n            ax.imshow(feature_map[0, :, :, j], cmap='viridis')\n            ax.set_xticks([])\n            ax.set_yticks([])\n\n        # Ocultar eixos extras\n        for j in range(min(n_features, n_rows * n_cols), n_rows * n_cols):\n            row, col = j // n_cols, j % n_cols\n            if n_rows == 1:\n                axes[col].axis('off') if n_cols &gt; 1 else axes.axis('off')\n            elif n_cols == 1:\n                axes[row].axis('off')\n            else:\n                axes[row, col].axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\ndef visualize_filters(model, layer_name):\n    \"\"\"Visualiza filtros de uma camada convolucional\"\"\"\n    layer = model.get_layer(layer_name)\n\n    if 'conv' not in layer.name.lower():\n        print(f\"A camada {layer_name} n\u00e3o \u00e9 uma camada convolucional\")\n        return\n\n    # Obter pesos dos filtros\n    weights = layer.get_weights()[0]  # [filtros, biases]\n\n    print(f\"Formato dos filtros: {weights.shape}\")\n\n    # Visualizar filtros (assumindo formato [filter_height, filter_width, input_channels, output_channels])\n    n_filters = min(64, weights.shape[-1])  # Limitar visualiza\u00e7\u00e3o\n    n_cols = 8\n    n_rows = (n_filters // n_cols) + (1 if n_filters % n_cols else 0)\n\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 2*n_rows))\n    fig.suptitle(f'Filtros da camada: {layer_name}')\n\n    for i in range(n_filters):\n        row, col = i // n_cols, i % n_cols\n\n        if n_rows == 1:\n            ax = axes[col] if n_cols &gt; 1 else axes\n        elif n_cols == 1:\n            ax = axes[row]\n        else:\n            ax = axes[row, col]\n\n        # Pegar filtro (m\u00e9dia se tiver m\u00faltiplos canais de entrada)\n        if weights.shape[2] == 1:  # Escala de cinza\n            filter_img = weights[:, :, 0, i]\n        elif weights.shape[2] == 3:  # RGB\n            filter_img = np.mean(weights[:, :, :, i], axis=2)\n        else:  # Outros casos\n            filter_img = np.mean(weights[:, :, :, i], axis=2)\n\n        ax.imshow(filter_img, cmap='viridis')\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    # Ocultar eixos extras\n    for i in range(n_filters, n_rows * n_cols):\n        row, col = i // n_cols, i % n_cols\n        if n_rows == 1:\n            axes[col].axis('off') if n_cols &gt; 1 else axes.axis('off')\n        elif n_cols == 1:\n            axes[row].axis('off')\n        else:\n            axes[row, col].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"aulas/aula9/#exemplo-de-uso","title":"Exemplo de Uso","text":"<p>```python</p>"},{"location":"aulas/aula9/#srcexamplescnn_practical_examplepy","title":"src/examples/cnn_practical_example.py","text":"<p>from models.cnn_transfer_learning import CNNTransferLearning from utils.cnn_visualization import visualize_feature_maps, visualize_filters import tensorflow as tf import numpy as np</p> <p>def demonstrate_cnn_transfer_learning():     \"\"\"Demonstra transfer learning com CNN\"\"\"     print(\"=== Demonstra\u00e7\u00e3o de Transfer Learning com CNN ===\\n\")</p> <pre><code># Criar modelo com transfer learning\ncnn_tl = CNNTransferLearning(\n    base_model_name='mobilenetv2',\n    input_shape=(224, 224, 3),\n    num_classes=3  # Exemplo: 3 classes\n)\n\nprint(\"Modelo base carregado:\", cnn_tl.base_model_name)\nprint(\"Formato de entrada:\", cnn_tl.input_shape)\nprint(\"N\u00famero de classes:\", cnn_tl.num_classes)\n\n# Construir modelo\nmodel = cnn_tl.build_model()\nprint(\"\\nModelo constru\u00eddo com sucesso!\")\n\n# Compilar modelo\ncnn_tl.compile_model(learning_rate=0.0001)\nprint(\"Modelo compilado\")\n\n# Visualizar arquitetura\ncnn_tl.visualize_model_architecture()\n\n# Simular datasets (em uma aplica\u00e7\u00e3o real, voc\u00ea usaria seus dados reais)\nprint(\"\\nSimulando datasets para demonstra\u00e7\u00e3o...\")\n\n# Criar dados simulados para demonstra\u00e7\u00e3o\ntrain_images = tf.random.normal((100, 224, 224, 3))\ntrain_labels = tf.keras.utils.to_categorical(np.random.randint(0, 3, size=(100,)), num_classes=3)\nval_images = tf.random.normal((20, 224, 224, 3))\nval_labels = tf.keras.utils.to_categorical(np.random.randint(0, 3, size=(20,)), num_classes=3)\n\n# Criar datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(16)\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels)).batch(16)\n\nprint(\"Datasets criados\")\n\n# Treinar modelo inicialmente\nprint(\"\\nTreinando cabe\u00e7a personalizada...\")\nhistory = cnn_tl.train_initial(train_dataset, validation_dataset, epochs=2)  # Poucas \u00e9pocas para demonstra\u00e7\u00e3o\n\nprint(\"Treinamento inicial conclu\u00eddo!\")\n\n# Visualizar hist\u00f3rico de treinamento\ncnn_tl.plot_training_history()\n\n# Preparar para fine-tuning\nprint(\"\\nPreparando para fine-tuning...\")\ncnn_tl.unfreeze_model(fine_tune_from_layer=50)\n\n# Compilar novamente com learning rate menor\ncnn_tl.compile_model(learning_rate=0.0001/10)\n\n# Fine-tuning\nprint(\"Realizando fine-tuning...\")\nfine_tune_history = cnn_tl.fine_tune(train_dataset, validation_dataset, epochs=2)\n\nprint(\"Fine-tuning conclu\u00eddo!\")\n\nreturn cnn_tl\n</code></pre> <p>def visualize_cnn_features():     \"\"\"Visualiza features extra\u00eddas pela CNN\"\"\"     # Criar modelo simples para visualiza\u00e7\u00e3o     cnn_tl = CNNTransferLearning(         base_model_name='mobilenetv2',         input_shape=(224, 224, 3),         num_classes=3     )     model = cnn_tl.build_model()</p> <pre><code># Criar imagem de exemplo\nsample_image = tf.random.normal((224, 224, 3))\n\nprint(\"Visualizando feature maps...\")\n# Para esta demonstra\u00e7\u00e3o, vamos visualizar algumas camadas espec\u00edficas\nlayer_names = []\nfor layer in model.layers:\n    if 'conv' in layer.name and len(layer_names) &lt; 2:\n        layer_names.append(layer.name)\n\nif layer_names:\n    print(f\"Visualizando camadas: {layer_names}\")\n    # Nota: Esta chamada pode n\u00e3o funcionar com o modelo completo devido \u00e0 complexidade\n    # A visualiza\u00e7\u00e3o real seria feita com um modelo mais simples ou camadas espec\u00edficas\nelse:\n    print(\"Nenhuma camada convolucional encontrada para visualiza\u00e7\u00e3o\")\n</code></pre> <p>def compare_architectures():     \"\"\"Compara diferentes arquiteturas de CNN\"\"\"     architectures = ['vgg16', 'mobilenetv2']     results = {}</p> <pre><code>print(\"\\n=== Compara\u00e7\u00e3o de Arquiteturas ===\")\n\nfor arch in architectures:\n    print(f\"\\nTestando {arch}...\")\n\n    try:\n        cnn_tl = CNNTransferLearning(\n            base_model_name=arch,\n            input_shape=(224, 224, 3),\n            num_classes=3\n        )\n        model = cnn_tl.build_model()\n\n        params = model.count_params()\n        input_shape = model.input_shape\n\n        results[arch] = {\n            'params': params,\n            'input_shape': input_shape,\n            'layers': len(model.layers)\n        }\n\n        print(f\"  Par\u00e2metros: {params:,}\")\n        print(f\"  Camadas: {len(model.layers)}\")\n\n    except Exception as e:\n        print(f\"  Erro ao carregar {arch}: {e}\")\n\nprint(f\"\\nResultados da compara\u00e7\u00e3o:\")\nfor arch, data in results.items():\n    print(f\"  {arch}: {data['params']:,} par\u00e2metros, {data['layers']} camadas\")\n\nreturn results\n</code></pre> <p>if name == \"main\":     # Executar demonstra\u00e7\u00f5es     trained_model = demonstrate_cnn_transfer_learning()     visualize_cnn_features()     comparison_results = compare_architectures()</p> <pre><code>print(\"\\n=== Resumo da Aula ===\")\nprint(\"Hoje aprendemos:\")\nprint(\"- Como funcionam as CNNs de forma intuitiva\")\nprint(\"- Transfer learning com modelos pr\u00e9-treinados\")\nprint(\"- Fine-tuning para adapta\u00e7\u00e3o a novos dom\u00ednios\")\nprint(\"- Visualiza\u00e7\u00e3o de features e arquiteturas\")\nprint(\"- Compara\u00e7\u00e3o entre diferentes arquiteturas\")\n</code></pre>"},{"location":"aulas/cp1/","title":"CP1 - Pipeline de Processamento","text":""},{"location":"aulas/cp1/#objetivo","title":"Objetivo","text":"<p>Criar um pipeline funcional de processamento de imagem que receba uma imagem, aplique t\u00e9cnicas de pr\u00e9-processamento e segmenta\u00e7\u00e3o, detecte objetos e retorne bounding boxes.</p>"},{"location":"aulas/cp1/#requisitos-tecnicos","title":"Requisitos T\u00e9cnicos","text":"<p>Seu projeto deve incluir:</p> <ol> <li>Estrutura de projeto organizada seguindo a arquitetura vista nas aulas anteriores</li> <li>Pipeline de processamento que realize m\u00faltiplas etapas de transforma\u00e7\u00e3o</li> <li>Sistema de detec\u00e7\u00e3o baseado em regras (threshold, contornos, etc.)</li> <li>Visualiza\u00e7\u00e3o dos resultados com bounding boxes</li> <li>Documenta\u00e7\u00e3o b\u00e1sica do c\u00f3digo e do processo</li> </ol>"},{"location":"aulas/cp1/#etapas-do-projeto","title":"Etapas do Projeto","text":""},{"location":"aulas/cp1/#1-estruturacao-do-projeto","title":"1. Estrutura\u00e7\u00e3o do Projeto","text":"<p>Sua estrutura de projeto deve seguir o padr\u00e3o estabelecido:</p> <pre><code>cp1_pipeline/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 preprocessing/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 filters.py\n\u2502   \u2502   \u251c\u2500\u2500 morphology.py\n\u2502   \u2502   \u2514\u2500\u2500 pipeline.py\n\u2502   \u251c\u2500\u2500 features/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 thresholding.py\n\u2502   \u2502   \u251c\u2500\u2500 contours.py\n\u2502   \u2502   \u2514\u2500\u2500 bounding_boxes.py\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 io.py\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 input/\n\u2502   \u2514\u2500\u2500 output/\n\u251c\u2500\u2500 tests/\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 config.yaml\n</code></pre>"},{"location":"aulas/cp1/#2-implementacao-do-pipeline","title":"2. Implementa\u00e7\u00e3o do Pipeline","text":"<p>Crie um pipeline que combine as t\u00e9cnicas aprendidas:</p> <pre><code># src/main.py\nfrom preprocessing.pipeline import ImagePreprocessingPipeline\nfrom features.thresholding import otsu_threshold\nfrom features.contours import find_contours, filter_contours_by_area\nfrom features.bounding_boxes import draw_bounding_boxes\nfrom utils.io import load_image_rgb, show_image\nimport cv2\nimport os\n\ndef create_cv_pipeline():\n    \"\"\"Cria um pipeline completo de vis\u00e3o computacional\"\"\"\n    pipeline = ImagePreprocessingPipeline()\n\n    # Adiciona etapas ao pipeline\n    pipeline.add_grayscale() \\\n             .add_histogram_equalization(color_space='grayscale') \\\n             .add_blur(blur_type='gaussian', kernel_size=(3, 3)) \\\n             .add_edge_detection(edge_type='canny', low_threshold=50, high_threshold=150)\n\n    return pipeline\n\ndef process_image_pipeline(input_path, output_path):\n    \"\"\"Processa uma imagem usando o pipeline completo\"\"\"\n    # Carregar imagem\n    image = load_image_rgb(input_path)\n\n    # Criar e aplicar pipeline\n    pipeline = create_cv_pipeline()\n    processed_image = pipeline.process(image)\n\n    # Aplicar threshold para binariza\u00e7\u00e3o\n    binary_image, _ = otsu_threshold(processed_image)\n\n    # Encontrar contornos\n    contours, _ = find_contours(binary_image)\n\n    # Filtrar contornos por \u00e1rea\n    filtered_contours = filter_contours_by_area(contours, min_area=100)\n\n    # Desenhar bounding boxes\n    result_image = draw_bounding_boxes(image, filtered_contours)\n\n    # Salvar resultado\n    output_dir = os.path.dirname(output_path)\n    os.makedirs(output_dir, exist_ok=True)\n    cv2.imwrite(output_path, cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))\n\n    return result_image, len(filtered_contours)\n\ndef main():\n    \"\"\"Fun\u00e7\u00e3o principal para executar o pipeline\"\"\"\n    input_path = \"data/input/exemplo.jpg\"  # Caminho da imagem de entrada\n    output_path = \"data/output/result.jpg\"  # Caminho para salvar resultado\n\n    # Verificar se a imagem de entrada existe\n    if not os.path.exists(input_path):\n        print(f\"Erro: Imagem de entrada n\u00e3o encontrada em {input_path}\")\n        return\n\n    # Processar imagem\n    result_image, object_count = process_image_pipeline(input_path, output_path)\n\n    print(f\"Processamento conclu\u00eddo!\")\n    print(f\"Objetos detectados: {object_count}\")\n    print(f\"Resultado salvo em: {output_path}\")\n\n    # Mostrar resultado\n    show_image(result_image, f\"Resultado - {object_count} objetos detectados\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"aulas/cp1/#3-configuracao-do-projeto","title":"3. Configura\u00e7\u00e3o do Projeto","text":"<pre><code># config.yaml\npipeline:\n  grayscale: true\n  histogram_equalization: true\n  blur:\n    enabled: true\n    type: gaussian\n    kernel_size: [3, 3]\n  edge_detection:\n    enabled: true\n    type: canny\n    low_threshold: 50\n    high_threshold: 150\n\nsegmentation:\n  threshold_method: otsu\n  min_contour_area: 100\n  max_contour_area: 100000\n\noutput:\n  bounding_box_color: [0, 255, 0]\n  bounding_box_thickness: 2\n</code></pre>"},{"location":"aulas/cp1/#4-script-de-teste","title":"4. Script de Teste","text":"<pre><code># tests/test_cp1.py\nimport unittest\nimport cv2\nimport numpy as np\nimport os\nfrom src.main import process_image_pipeline\n\nclass TestCPPipeline(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Configura\u00e7\u00e3o antes de cada teste\"\"\"\n        # Criar imagem de teste\n        self.test_image = np.random.randint(0, 255, (300, 300, 3), dtype=np.uint8)\n        cv2.imwrite('data/input/test_image.jpg', cv2.cvtColor(self.test_image, cv2.COLOR_RGB2BGR))\n\n    def test_process_image_pipeline(self):\n        \"\"\"Testa o pipeline completo de processamento\"\"\"\n        input_path = 'data/input/test_image.jpg'\n        output_path = 'data/output/test_result.jpg'\n\n        # Executar pipeline\n        result_image, object_count = process_image_pipeline(input_path, output_path)\n\n        # Verificar se o resultado foi salvo\n        self.assertTrue(os.path.exists(output_path))\n\n        # Verificar se a imagem de resultado tem dimens\u00f5es v\u00e1lidas\n        self.assertEqual(len(result_image.shape), 3)  # RGB\n\n        # Verificar se algum objeto foi detectado (pode variar com imagem aleat\u00f3ria)\n        self.assertIsInstance(object_count, int)\n        self.assertGreaterEqual(object_count, 0)\n\n    def tearDown(self):\n        \"\"\"Limpeza ap\u00f3s cada teste\"\"\"\n        test_files = [\n            'data/input/test_image.jpg',\n            'data/output/test_result.jpg'\n        ]\n        for file_path in test_files:\n            if os.path.exists(file_path):\n                os.remove(file_path)\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"aulas/cp1/#5-documentacao","title":"5. Documenta\u00e7\u00e3o","text":"<pre><code># CP1 - Pipeline de Processamento\n\nEste projeto implementa um pipeline completo de processamento de imagem para detec\u00e7\u00e3o de objetos.\n\n## Funcionalidades\n\n- Pr\u00e9-processamento de imagem (equaliza\u00e7\u00e3o, desfoque, detec\u00e7\u00e3o de bordas)\n- Segmenta\u00e7\u00e3o por threshold\n- Detec\u00e7\u00e3o de contornos\n- Desenho de bounding boxes\n- Contagem de objetos\n\n## Como usar\n\n1. Coloque sua imagem em `data/input/`\n2. Execute `python src/main.py`\n3. O resultado ser\u00e1 salvo em `data/output/`\n\n## Configura\u00e7\u00e3o\n\nAs configura\u00e7\u00f5es do pipeline podem ser ajustadas em `config.yaml`.\n</code></pre>"},{"location":"aulas/cp1/#criterios-de-avaliacao","title":"Crit\u00e9rios de Avalia\u00e7\u00e3o","text":"<p>Seu projeto ser\u00e1 avaliado com base nos seguintes crit\u00e9rios:</p> <ul> <li>Corre\u00e7\u00e3o t\u00e9cnica (40%): O c\u00f3digo funciona corretamente e produz resultados esperados</li> <li>Organiza\u00e7\u00e3o do c\u00f3digo (25%): Estrutura bem organizada, modular e seguindo boas pr\u00e1ticas</li> <li>Funcionalidade do pipeline (20%): Implementa\u00e7\u00e3o completa do pipeline com m\u00faltiplas etapas</li> <li>Documenta\u00e7\u00e3o (15%): C\u00f3digo e projeto devidamente documentados</li> </ul>"},{"location":"aulas/cp1/#entrega","title":"Entrega","text":"<p>A entrega pode ser feita como:</p> <ul> <li>Script organizado com todas as funcionalidades implementadas</li> <li>Pequena API local (opcional) que aceita upload de imagem e retorna resultado</li> <li>Demonstra\u00e7\u00e3o funcional do pipeline</li> </ul> <p>Lembre-se de que o foco est\u00e1 na implementa\u00e7\u00e3o de um pipeline funcional que demonstre compreens\u00e3o das t\u00e9cnicas de vis\u00e3o computacional vistas at\u00e9 agora.</p>"},{"location":"aulas/cp2/","title":"Aula 8 - CP2","text":""},{"location":"aulas/cp2/#objetivo","title":"Objetivo","text":"<p>Avalia\u00e7\u00e3o do projeto entregue na CP2, com foco em arquitetura, funcionalidade e organiza\u00e7\u00e3o do c\u00f3digo.</p>"},{"location":"aulas/cp2/#criterios-de-avaliacao","title":"Crit\u00e9rios de Avalia\u00e7\u00e3o","text":"<p>Durante esta aula, ser\u00e3o avaliados os seguintes aspectos do projeto entregue:</p>"},{"location":"aulas/cp2/#arquitetura-30","title":"Arquitetura (30%)","text":"<ul> <li>Separa\u00e7\u00e3o adequada de responsabilidades (Controller, Service, Model)</li> <li>Estrutura de diret\u00f3rios organizada</li> <li>Uso apropriado de padr\u00f5es de projeto</li> </ul>"},{"location":"aulas/cp2/#funcionalidade-30","title":"Funcionalidade (30%)","text":"<ul> <li>API funcional e respondendo corretamente \u00e0s requisi\u00e7\u00f5es</li> <li>Classifica\u00e7\u00e3o de imagens funcionando adequadamente</li> <li>Tratamento adequado de erros</li> </ul>"},{"location":"aulas/cp2/#organizacao-do-codigo-20","title":"Organiza\u00e7\u00e3o do C\u00f3digo (20%)","text":"<ul> <li>C\u00f3digo limpo e bem comentado</li> <li>Nomenclatura consistente</li> <li>Seguimento de conven\u00e7\u00f5es de estilo</li> </ul>"},{"location":"aulas/cp2/#documentacao-20","title":"Documenta\u00e7\u00e3o (20%)","text":"<ul> <li>README com instru\u00e7\u00f5es claras</li> <li>Documenta\u00e7\u00e3o dos endpoints</li> <li>Explica\u00e7\u00e3o do funcionamento do sistema</li> </ul>"},{"location":"aulas/cp2/#atividade","title":"Atividade","text":"<p>Durante esta aula, os alunos dever\u00e3o:</p> <ol> <li>Apresentar sua API de classifica\u00e7\u00e3o</li> <li>Demonstrar o funcionamento dos endpoints</li> <li>Explicar a arquitetura escolhida</li> <li>Discutir desafios enfrentados e solu\u00e7\u00f5es implementadas</li> </ol>"},{"location":"aulas/cp3/","title":"CP3 - Projeto Final","text":""},{"location":"aulas/cp3/#objetivo","title":"Objetivo","text":"<p>Desenvolver um sistema completo de vis\u00e3o computacional que demonstre todas as habilidades aprendidas ao longo da disciplina, integrando pipeline de processamento, classifica\u00e7\u00e3o, detec\u00e7\u00e3o de objetos e disponibiliza\u00e7\u00e3o em uma API.</p>"},{"location":"aulas/cp3/#opcoes-de-projeto","title":"Op\u00e7\u00f5es de Projeto","text":"<p>Os alunos podem escolher entre as seguintes op\u00e7\u00f5es:</p> <ol> <li>Detector de EPI - Sistema para identifica\u00e7\u00e3o de equipamentos de prote\u00e7\u00e3o individual</li> <li>Classificador de Defeitos Industriais - Sistema para detec\u00e7\u00e3o de falhas em produtos</li> <li>Sistema de Contagem - Contador autom\u00e1tico de objetos em imagens</li> <li>Detector de Anomalias - Sistema para identifica\u00e7\u00e3o de padr\u00f5es an\u00f4malos</li> <li>OCR Simples - Sistema para reconhecimento \u00f3ptico de caracteres</li> <li>Reconhecimento Facial B\u00e1sico - Sistema para identifica\u00e7\u00e3o de faces</li> </ol>"},{"location":"aulas/cp3/#requisitos-minimos","title":"Requisitos M\u00ednimos","text":"<p>Seu projeto deve contemplar:</p>"},{"location":"aulas/cp3/#arquitetura-30","title":"Arquitetura (30%)","text":"<ul> <li>C\u00f3digo estruturado seguindo boas pr\u00e1ticas de engenharia de software</li> <li>Separa\u00e7\u00e3o clara de responsabilidades (modelos, servi\u00e7os, utilit\u00e1rios)</li> <li>Documenta\u00e7\u00e3o adequada do c\u00f3digo e da arquitetura</li> </ul>"},{"location":"aulas/cp3/#funcionalidade-30","title":"Funcionalidade (30%)","text":"<ul> <li>Sistema completo funcionando</li> <li>Pipeline de vis\u00e3o computacional implementado</li> <li>Integra\u00e7\u00e3o com modelo de ML/DL</li> <li>API para consumo do sistema</li> </ul>"},{"location":"aulas/cp3/#organizacao-20","title":"Organiza\u00e7\u00e3o (20%)","text":"<ul> <li>Estrutura de projeto bem organizada</li> <li>C\u00f3digo limpo e leg\u00edvel</li> <li>Seguimento de conven\u00e7\u00f5es de nomenclatura</li> </ul>"},{"location":"aulas/cp3/#clareza-tecnica-20","title":"Clareza T\u00e9cnica (20%)","text":"<ul> <li>Apresenta\u00e7\u00e3o t\u00e9cnica do projeto</li> <li>Explica\u00e7\u00e3o clara das t\u00e9cnicas utilizadas</li> <li>Justificativa das escolhas t\u00e9cnicas</li> </ul>"},{"location":"aulas/cp3/#estrutura-do-projeto","title":"Estrutura do Projeto","text":"<pre><code>projeto_final/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 routers/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 cv_router.py\n\u2502   \u2502   \u2514\u2500\u2500 schemas/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 request_schemas.py\n\u2502   \u2502       \u2514\u2500\u2500 response_schemas.py\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 cv_service.py\n\u2502   \u2502   \u2514\u2500\u2500 model_service.py\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 cv_models.py\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 image_utils.py\n\u2502   \u2502   \u2514\u2500\u2500 preprocessing.py\n\u2502   \u2514\u2500\u2500 config/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 settings.py\n\u251c\u2500\u2500 models/\n\u2502   \u2514\u2500\u2500 trained_model.h5 (ou .pkl)\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 processed/\n\u2502   \u2514\u2500\u2500 external/\n\u251c\u2500\u2500 notebooks/\n\u251c\u2500\u2500 tests/\n\u251c\u2500\u2500 docs/\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 docker-compose.yml\n</code></pre>"},{"location":"aulas/cp3/#implementacao-detalhada","title":"Implementa\u00e7\u00e3o Detalhada","text":""},{"location":"aulas/cp3/#1-configuracao-do-projeto","title":"1. Configura\u00e7\u00e3o do Projeto","text":"<pre><code># app/config/settings.py\nfrom pydantic_settings import Settings\nfrom typing import List\n\nclass Settings(Settings):\n    app_name: str = \"Projeto Final - Vis\u00e3o Computacional\"\n    app_version: str = \"1.0.0\"\n    debug: bool = False\n    model_path: str = \"models/trained_model.h5\"\n    allowed_image_types: List[str] = [\"image/jpeg\", \"image/jpg\", \"image/png\", \"image/webp\"]\n    max_file_size: int = 10 * 1024 * 1024  # 10MB\n    api_prefix: str = \"/api/v1\"\n    project_type: str = \"epi_detector\"  # Pode ser alterado conforme o projeto\n\nsettings = Settings()\n</code></pre>"},{"location":"aulas/cp3/#2-modelos-de-requisicao-e-resposta","title":"2. Modelos de Requisi\u00e7\u00e3o e Resposta","text":"<pre><code># app/api/schemas/request_schemas.py\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass CVRequest(BaseModel):\n    \"\"\"Schema base para requisi\u00e7\u00f5es de vis\u00e3o computacional\"\"\"\n    image_url: Optional[str] = Field(None, description=\"URL da imagem\")\n    image_base64: Optional[str] = Field(None, description=\"Imagem em base64\")\n\nclass EpiDetectionRequest(CVRequest):\n    \"\"\"Schema para requisi\u00e7\u00e3o de detec\u00e7\u00e3o de EPI\"\"\"\n    confidence_threshold: float = Field(default=0.5, ge=0.0, le=1.0)\n\nclass DefectClassificationRequest(CVRequest):\n    \"\"\"Schema para requisi\u00e7\u00e3o de classifica\u00e7\u00e3o de defeitos\"\"\"\n    pass\n\nclass CountingRequest(CVRequest):\n    \"\"\"Schema para requisi\u00e7\u00e3o de contagem\"\"\"\n    pass\n</code></pre> <pre><code># app/api/schemas/response_schemas.py\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Optional\nfrom datetime import datetime\n\nclass BaseResponse(BaseModel):\n    \"\"\"Resposta base para todas as opera\u00e7\u00f5es\"\"\"\n    success: bool\n    message: Optional[str] = None\n    timestamp: datetime = datetime.now()\n\nclass EpiDetectionResponse(BaseResponse):\n    \"\"\"Resposta para detec\u00e7\u00e3o de EPI\"\"\"\n    detections: List[Dict]\n    has_protective_equipment: bool\n    equipment_types: List[str]\n    processing_time: float\n\nclass DefectClassificationResponse(BaseResponse):\n    \"\"\"Resposta para classifica\u00e7\u00e3o de defeitos\"\"\"\n    defect_type: str\n    confidence: float\n    severity: str\n    processing_time: float\n\nclass CountingResponse(BaseResponse):\n    \"\"\"Resposta para contagem\"\"\"\n    count: int\n    objects: List[Dict]\n    processing_time: float\n</code></pre>"},{"location":"aulas/cp3/#3-servicos-de-visao-computacional","title":"3. Servi\u00e7os de Vis\u00e3o Computacional","text":"<pre><code># app/services/cv_service.py\nimport numpy as np\nfrom PIL import Image\nimport time\nfrom typing import Dict, List\nfrom utils.image_utils import pil_to_numpy\nfrom services.model_service import ModelService\n\nclass CVService:\n    def __init__(self):\n        self.model_service = ModelService()\n\n    def detect_epi(self, image: Image.Image, confidence_threshold: float = 0.5) -&gt; Dict:\n        \"\"\"Detecta EPI em imagem\"\"\"\n        start_time = time.time()\n\n        # Converter imagem para array\n        image_array = pil_to_numpy(image)\n\n        # Em uma implementa\u00e7\u00e3o real, usaria um modelo treinado para detec\u00e7\u00e3o de EPI\n        # Aqui est\u00e1 uma simula\u00e7\u00e3o\n        height, width = image_array.shape[:2]\n\n        # Simular detec\u00e7\u00f5es de EPI\n        mock_detections = [\n            {\n                \"type\": \"capacete\",\n                \"confidence\": 0.89,\n                \"bbox\": [int(0.4*width), int(0.1*height), int(0.6*width), int(0.3*height)],\n                \"coordinates\": {\n                    \"x_min\": int(0.4*width),\n                    \"y_min\": int(0.1*height),\n                    \"x_max\": int(0.6*width),\n                    \"y_max\": int(0.3*height)\n                }\n            },\n            {\n                \"type\": \"colete\",\n                \"confidence\": 0.76,\n                \"bbox\": [int(0.3*width), int(0.3*height), int(0.7*width), int(0.7*height)],\n                \"coordinates\": {\n                    \"x_min\": int(0.3*width),\n                    \"y_min\": int(0.3*height),\n                    \"x_max\": int(0.7*width),\n                    \"y_max\": int(0.7*height)\n                }\n            }\n        ]\n\n        # Filtrar por threshold\n        filtered_detections = [\n            det for det in mock_detections \n            if det[\"confidence\"] &gt;= confidence_threshold\n        ]\n\n        has_protective_equipment = len(filtered_detections) &gt; 0\n        equipment_types = list(set([det[\"type\"] for det in filtered_detections]))\n\n        processing_time = time.time() - start_time\n\n        return {\n            'detections': filtered_detections,\n            'has_protective_equipment': has_protective_equipment,\n            'equipment_types': equipment_types,\n            'processing_time': processing_time\n        }\n\n    def classify_defect(self, image: Image.Image) -&gt; Dict:\n        \"\"\"Classifica defeito industrial\"\"\"\n        start_time = time.time()\n\n        # Converter imagem para array\n        image_array = pil_to_numpy(image)\n\n        # Simular classifica\u00e7\u00e3o de defeito\n        # Em uma implementa\u00e7\u00e3o real, usaria modelo treinado\n        defect_types = [\"trinca\", \"falta_material\", \"deformacao\", \"sujeira\", \"nenhum\"]\n        defect_probs = np.random.dirichlet(np.ones(len(defect_types)))  # Distribui\u00e7\u00e3o aleat\u00f3ria\n\n        max_idx = np.argmax(defect_probs)\n        defect_type = defect_types[max_idx]\n        confidence = float(defect_probs[max_idx])\n\n        # Determinar severidade com base no tipo e confian\u00e7a\n        severity = \"baixa\" if confidence &lt; 0.5 else \"media\" if confidence &lt; 0.8 else \"alta\"\n\n        processing_time = time.time() - start_time\n\n        return {\n            'defect_type': defect_type,\n            'confidence': confidence,\n            'severity': severity,\n            'processing_time': processing_time\n        }\n\n    def count_objects(self, image: Image.Image) -&gt; Dict:\n        \"\"\"Conta objetos em imagem\"\"\"\n        start_time = time.time()\n\n        # Converter imagem para array\n        image_array = pil_to_numpy(image)\n\n        # Simular contagem de objetos\n        # Em uma implementa\u00e7\u00e3o real, usaria detec\u00e7\u00e3o de objetos ou segmenta\u00e7\u00e3o\n        height, width = image_array.shape[:2]\n\n        # Simular detec\u00e7\u00e3o de objetos\n        num_objects = np.random.randint(1, 10)  # Contagem aleat\u00f3ria para simula\u00e7\u00e3o\n\n        # Gerar bounding boxes simuladas\n        objects = []\n        for i in range(num_objects):\n            x = np.random.randint(0, width//2)\n            y = np.random.randint(0, height//2)\n            w = np.random.randint(width//8, width//4)\n            h = np.random.randint(height//8, height//4)\n\n            objects.append({\n                \"id\": i+1,\n                \"bbox\": [x, y, x+w, y+h],\n                \"coordinates\": {\n                    \"x_min\": x,\n                    \"y_min\": y,\n                    \"x_max\": x+w,\n                    \"y_max\": y+h\n                }\n            })\n\n        processing_time = time.time() - start_time\n\n        return {\n            'count': num_objects,\n            'objects': objects,\n            'processing_time': processing_time\n        }\n</code></pre>"},{"location":"aulas/cp3/#4-servico-de-modelos","title":"4. Servi\u00e7o de Modelos","text":"<pre><code># app/services/model_service.py\nimport tensorflow as tf\nimport joblib\nimport pickle\nimport os\nimport numpy as np\nfrom typing import Any, Dict\nfrom config.settings import settings\n\nclass ModelService:\n    def __init__(self):\n        self.model = None\n        self.model_info = {}\n        self._load_model()\n\n    def _load_model(self):\n        \"\"\"Carrega modelo treinado\"\"\"\n        try:\n            model_path = settings.model_path\n\n            if not os.path.exists(model_path):\n                print(f\"Aviso: Modelo n\u00e3o encontrado em {model_path}. Criando modelo mock.\")\n                self.model = self._create_mock_model()\n                self.model_info = {\n                    'model_name': 'Mock Model',\n                    'model_version': '1.0.0',\n                    'input_shape': (224, 224, 3),\n                    'classes': ['classe_a', 'classe_b', 'classe_c'],\n                    'loaded_successfully': False\n                }\n                return\n\n            # Tentar carregar como modelo TensorFlow/Keras primeiro\n            if model_path.endswith(('.h5', '.keras')):\n                self.model = tf.keras.models.load_model(model_path)\n                # Extrair informa\u00e7\u00f5es do modelo\n                self.model_info = {\n                    'model_name': os.path.basename(model_path),\n                    'model_version': '1.0.0',\n                    'input_shape': self.model.input_shape,\n                    'classes': getattr(self.model, 'classes', ['classe_a', 'classe_b', 'classe_c']),\n                    'loaded_successfully': True\n                }\n            else:\n                # Carregar com joblib ou pickle\n                try:\n                    self.model = joblib.load(model_path)\n                except:\n                    with open(model_path, 'rb') as f:\n                        self.model = pickle.load(f)\n\n                # Informa\u00e7\u00f5es para modelos sklearn\n                self.model_info = {\n                    'model_name': os.path.basename(model_path),\n                    'model_version': '1.0.0',\n                    'input_shape': 'Vari\u00e1vel (depende do modelo)',\n                    'classes': list(getattr(self.model, 'classes_', ['classe_a', 'classe_b', 'classe_c'])),\n                    'loaded_successfully': True\n                }\n\n        except Exception as e:\n            print(f\"Erro ao carregar modelo: {e}\")\n            # Criar modelo mock\n            self.model = self._create_mock_model()\n            self.model_info = {\n                'model_name': 'Mock Model',\n                'model_version': '1.0.0',\n                'input_shape': (224, 224, 3),\n                'classes': ['classe_a', 'classe_b', 'classe_c'],\n                'loaded_successfully': False\n            }\n\n    def _create_mock_model(self):\n        \"\"\"Cria modelo mock para demonstra\u00e7\u00e3o\"\"\"\n        class MockModel:\n            def predict(self, X):\n                # Simular predi\u00e7\u00f5es\n                n_samples = X.shape[0]\n                return np.random.rand(n_samples, 3)  # 3 classes\n\n            def predict_proba(self, X):\n                # Simular probabilidades\n                n_samples = X.shape[0]\n                probs = np.random.rand(n_samples, 3)\n                probs = probs / probs.sum(axis=1, keepdims=True)  # Normalizar\n                return probs\n\n        return MockModel()\n\n    def get_model_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Retorna informa\u00e7\u00f5es sobre o modelo carregado\"\"\"\n        return self.model_info\n\n    def predict(self, image_array: np.ndarray) -&gt; Dict[str, Any]:\n        \"\"\"Faz predi\u00e7\u00e3o com o modelo\"\"\"\n        import time\n        start_time = time.time()\n\n        # Redimensionar imagem para o tamanho esperado pelo modelo\n        from PIL import Image\n        img = Image.fromarray(image_array.astype('uint8'))\n        img = img.resize((224, 224))\n        processed_input = np.array(img).astype(np.float32) / 255.0\n        processed_input = np.expand_dims(processed_input, axis=0)\n\n        # Fazer predi\u00e7\u00e3o\n        if hasattr(self.model, 'predict_proba'):\n            probabilities = self.model.predict_proba(processed_input)[0]\n        else:\n            predictions = self.model.predict(processed_input)[0]\n            if predictions.ndim == 0:  # Se for um escalar\n                probabilities = np.zeros(len(self.model_info['classes']))\n                probabilities[int(predictions)] = 1.0\n            else:\n                probabilities = predictions\n\n        # Obter classe com maior probabilidade\n        predicted_class_idx = np.argmax(probabilities)\n        predicted_class = self.model_info['classes'][predicted_class_idx]\n        confidence = float(probabilities[predicted_class_idx])\n\n        # Obter todas as predi\u00e7\u00f5es com probabilidades\n        all_predictions = [\n            {\"class\": cls, \"probability\": float(prob)}\n            for cls, prob in zip(self.model_info['classes'], probabilities)\n        ]\n\n        processing_time = time.time() - start_time\n\n        return {\n            'prediction': predicted_class,\n            'confidence': confidence,\n            'all_predictions': all_predictions,\n            'processing_time': processing_time\n        }\n</code></pre>"},{"location":"aulas/cp3/#5-router-de-visao-computacional","title":"5. Router de Vis\u00e3o Computacional","text":"<pre><code># app/api/routers/cv_router.py\nfrom fastapi import APIRouter, File, UploadFile, HTTPException\nfrom fastapi.responses import JSONResponse\nfrom typing import Optional\nimport base64\nimport io\nfrom PIL import Image\n\nfrom api.schemas.request_schemas import (\n    EpiDetectionRequest, \n    DefectClassificationRequest, \n    CountingRequest\n)\nfrom api.schemas.response_schemas import (\n    EpiDetectionResponse, \n    DefectClassificationResponse, \n    CountingResponse\n)\nfrom services.cv_service import CVService\nfrom utils.image_utils import (\n    decode_base64_image, \n    load_image_from_url, \n    validate_image_content_type\n)\n\nrouter = APIRouter(prefix=\"/cv\", tags=[\"Vis\u00e3o Computacional\"])\n\n# Instanciar servi\u00e7o\ncv_service = CVService()\n\n@router.post(\"/detect-epi\", response_model=EpiDetectionResponse)\nasync def detect_epi(\n    request: EpiDetectionRequest = None,\n    file: UploadFile = File(None)\n):\n    \"\"\"\n    Detecta equipamentos de prote\u00e7\u00e3o individual em imagem\n    \"\"\"\n    try:\n        # Obter imagem\n        image = None\n\n        if file:\n            if not validate_image_content_type(file.content_type):\n                raise HTTPException(status_code=400, detail=\"Tipo de arquivo n\u00e3o suportado\")\n\n            contents = await file.read()\n            image = Image.open(io.BytesIO(contents))\n        elif request:\n            if request.image_base64:\n                image = decode_base64_image(request.image_base64)\n            elif request.image_url:\n                image = load_image_from_url(request.image_url)\n\n        if image is None:\n            raise HTTPException(status_code=400, detail=\"Nenhuma imagem fornecida\")\n\n        # Detectar EPI\n        result = cv_service.detect_epi(image, request.confidence_threshold)\n\n        return EpiDetectionResponse(\n            success=True,\n            detections=result['detections'],\n            has_protective_equipment=result['has_protective_equipment'],\n            equipment_types=result['equipment_types'],\n            processing_time=result['processing_time']\n        )\n\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Erro interno: {str(e)}\")\n\n@router.post(\"/classify-defect\", response_model=DefectClassificationResponse)\nasync def classify_defect(\n    request: DefectClassificationRequest = None,\n    file: UploadFile = File(None)\n):\n    \"\"\"\n    Classifica defeitos industriais em imagem\n    \"\"\"\n    try:\n        # Obter imagem\n        image = None\n\n        if file:\n            if not validate_image_content_type(file.content_type):\n                raise HTTPException(status_code=400, detail=\"Tipo de arquivo n\u00e3o suportado\")\n\n            contents = await file.read()\n            image = Image.open(io.BytesIO(contents))\n        elif request:\n            if request.image_base64:\n                image = decode_base64_image(request.image_base64)\n            elif request.image_url:\n                image = load_image_from_url(request.image_url)\n\n        if image is None:\n            raise HTTPException(status_code=400, detail=\"Nenhuma imagem fornecida\")\n\n        # Classificar defeito\n        result = cv_service.classify_defect(image)\n\n        return DefectClassificationResponse(\n            success=True,\n            defect_type=result['defect_type'],\n            confidence=result['confidence'],\n            severity=result['severity'],\n            processing_time=result['processing_time']\n        )\n\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Erro interno: {str(e)}\")\n\n@router.post(\"/count-objects\", response_model=CountingResponse)\nasync def count_objects(\n    request: CountingRequest = None,\n    file: UploadFile = File(None)\n):\n    \"\"\"\n    Conta objetos em imagem\n    \"\"\"\n    try:\n        # Obter imagem\n        image = None\n\n        if file:\n            if not validate_image_content_type(file.content_type):\n                raise HTTPException(status_code=400, detail=\"Tipo de arquivo n\u00e3o suportado\")\n\n            contents = await file.read()\n            image = Image.open(io.BytesIO(contents))\n        elif request:\n            if request.image_base64:\n                image = decode_base64_image(request.image_base64)\n            elif request.image_url:\n                image = load_image_from_url(request.image_url)\n\n        if image is None:\n            raise HTTPException(status_code=400, detail=\"Nenhuma imagem fornecida\")\n\n        # Contar objetos\n        result = cv_service.count_objects(image)\n\n        return CountingResponse(\n            success=True,\n            count=result['count'],\n            objects=result['objects'],\n            processing_time=result['processing_time']\n        )\n\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Erro interno: {str(e)}\")\n\n@router.get(\"/model-info\")\nasync def get_model_info():\n    \"\"\"\n    Retorna informa\u00e7\u00f5es sobre o modelo carregado\n    \"\"\"\n    model_info = cv_service.model_service.get_model_info()\n    return {\n        \"success\": True,\n        \"model_info\": model_info\n    }\n</code></pre>"},{"location":"aulas/cp3/#6-arquivo-principal","title":"6. Arquivo Principal","text":"<pre><code># app/main.py\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom api.routers.cv_router import router as cv_router\nfrom config.settings import settings\n\n# Criar inst\u00e2ncia do FastAPI\napp = FastAPI(\n    title=settings.app_name,\n    version=settings.app_version,\n    debug=settings.debug,\n    description=\"Projeto Final - Sistema Completo de Vis\u00e3o Computacional\"\n)\n\n# Configurar CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Em produ\u00e7\u00e3o, substituir por dom\u00ednios espec\u00edficos\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Incluir roteadores\napp.include_router(cv_router)\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"\n    Ponto de entrada raiz da API\n    \"\"\"\n    return {\n        \"message\": \"Projeto Final - Vis\u00e3o Computacional\",\n        \"version\": settings.app_version,\n        \"documentation\": \"/docs\",\n        \"available_routes\": [\n            \"/api/v1/cv/detect-epi\",\n            \"/api/v1/cv/classify-defect\",\n            \"/api/v1/cv/count-objects\"\n        ]\n    }\n\n@app.get(\"/health\")\nasync def health():\n    \"\"\"\n    Endpoint de sa\u00fade do sistema\n    \"\"\"\n    return {\"status\": \"healthy\", \"service\": \"cv-project-api\"}\n\n# Para rodar: uvicorn app.main:app --reload --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"aulas/cp3/#7-arquivo-de-requisitos","title":"7. Arquivo de Requisitos","text":"<pre><code># requirements.txt\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\npydantic==2.5.0\npydantic-settings==2.1.0\nPillow==10.1.0\nnumpy==1.24.3\nopencv-python==4.8.1.78\nscikit-learn==1.3.0\ntensorflow==2.15.0\njoblib==1.3.2\nrequests==2.31.0\npython-multipart==0.0.6\npython-dotenv==1.0.0\n</code></pre>"},{"location":"aulas/cp3/#8-documentacao-do-readme","title":"8. Documenta\u00e7\u00e3o do README","text":"<pre><code># Projeto Final - Vis\u00e3o Computacional Aplicada\n\nEste projeto implementa um sistema completo de vis\u00e3o computacional com m\u00faltiplas funcionalidades.\n\n## Funcionalidades\n\n- Detec\u00e7\u00e3o de Equipamentos de Prote\u00e7\u00e3o Individual (EPI)\n- Classifica\u00e7\u00e3o de Defeitos Industriais\n- Contagem de Objetos em Imagens\n\n## Tecnologias Utilizadas\n\n- FastAPI: Framework web moderno e r\u00e1pido\n- TensorFlow: Modelos de Deep Learning\n- OpenCV: Processamento de imagens\n- Docker: Containeriza\u00e7\u00e3o\n- Pydantic: Valida\u00e7\u00e3o de dados\n\n## Instala\u00e7\u00e3o\n\n1. Clone o reposit\u00f3rio\n2. Crie um ambiente virtual:\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # Linux/Mac\n   # ou\n   venv\\Scripts\\activate  # Windows\n   ```\n3. Instale as depend\u00eancias:\n   ```bash\n   pip install -r requirements.txt\n   ```\n4. Coloque seu modelo treinado em `models/trained_model.h5`\n5. Atualize as configura\u00e7\u00f5es em `app/config/settings.py`\n\n## Execu\u00e7\u00e3o\n\n```bash\nuvicorn app.main:app --reload --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"aulas/cp3/#endpoints","title":"Endpoints","text":"<ul> <li><code>GET /</code> - P\u00e1gina inicial</li> <li><code>GET /health</code> - Status de sa\u00fade da API</li> <li><code>POST /api/v1/cv/detect-epi</code> - Detec\u00e7\u00e3o de EPI</li> <li><code>POST /api/v1/cv/classify-defect</code> - Classifica\u00e7\u00e3o de defeitos</li> <li><code>POST /api/v1/cv/count-objects</code> - Contagem de objetos</li> <li><code>GET /api/v1/cv/model-info</code> - Informa\u00e7\u00f5es sobre o modelo</li> </ul>"},{"location":"aulas/cp3/#documentacao-automatica","title":"Documenta\u00e7\u00e3o Autom\u00e1tica","text":"<p>Acesse <code>/docs</code> ou <code>/redoc</code> para documenta\u00e7\u00e3o interativa da API.</p>"},{"location":"aulas/cp3/#avaliacao","title":"Avalia\u00e7\u00e3o","text":"<p>O projeto ser\u00e1 avaliado com base nos seguintes crit\u00e9rios:</p> <ul> <li>Arquitetura (30%): C\u00f3digo estruturado e seguindo boas pr\u00e1ticas</li> <li>Funcionalidade (30%): Sistema completo funcionando</li> <li>Organiza\u00e7\u00e3o (20%): C\u00f3digo limpo e bem organizado</li> <li>Clareza T\u00e9cnica (20%): Apresenta\u00e7\u00e3o e explica\u00e7\u00e3o clara das t\u00e9cnicas</li> </ul>"},{"location":"aulas/cp3/#autores","title":"Autores","text":"<p>[Seu nome] [Seu email] ```</p>"},{"location":"aulas/cp3/#criterios-de-avaliacao","title":"Crit\u00e9rios de Avalia\u00e7\u00e3o","text":""},{"location":"aulas/cp3/#arquitetura-30_1","title":"Arquitetura (30%)","text":"<ul> <li>Organiza\u00e7\u00e3o do c\u00f3digo em m\u00f3dulos bem definidos</li> <li>Separa\u00e7\u00e3o de responsabilidades (models, services, utils, etc.)</li> <li>Seguimento de padr\u00f5es de projeto</li> <li>Documenta\u00e7\u00e3o do c\u00f3digo</li> </ul>"},{"location":"aulas/cp3/#funcionamento-30","title":"Funcionamento (30%)","text":"<ul> <li>Sistema completo e funcional</li> <li>Integra\u00e7\u00e3o adequada entre componentes</li> <li>API respondendo corretamente \u00e0s requisi\u00e7\u00f5es</li> <li>Modelos de ML/DL integrados corretamente</li> </ul>"},{"location":"aulas/cp3/#organizacao-20_1","title":"Organiza\u00e7\u00e3o (20%)","text":"<ul> <li>Estrutura de diret\u00f3rios clara e l\u00f3gica</li> <li>C\u00f3digo limpo e leg\u00edvel</li> <li>Conven\u00e7\u00f5es de nomenclatura consistentes</li> <li>Arquivos de configura\u00e7\u00e3o adequados</li> </ul>"},{"location":"aulas/cp3/#clareza-tecnica-20_1","title":"Clareza T\u00e9cnica (20%)","text":"<ul> <li>Apresenta\u00e7\u00e3o clara do projeto</li> <li>Explica\u00e7\u00e3o das t\u00e9cnicas utilizadas</li> <li>Justificativa das escolhas t\u00e9cnicas</li> <li>Demonstra\u00e7\u00e3o de compreens\u00e3o dos conceitos</li> </ul>"},{"location":"aulas/cp3/#entrega","title":"Entrega","text":"<p>A entrega deve incluir:</p> <ol> <li>C\u00f3digo-fonte completo e funcional</li> <li>Documenta\u00e7\u00e3o adequada</li> <li>README com instru\u00e7\u00f5es de execu\u00e7\u00e3o</li> <li>Apresenta\u00e7\u00e3o t\u00e9cnica do projeto</li> <li>Demonstra\u00e7\u00e3o do sistema em funcionamento</li> </ol>"},{"location":"aulas/intro/","title":"Introdu\u00e7\u00e3o","text":"<p>Fa\u00e7a o download do PDF de Introdu\u00e7\u00e3o:</p> <ul> <li>Arquivo PDF: Introdu\u00e7\u00e3o (slides.pdf)</li> </ul> <p>Dica</p> <p>Abra o PDF em uma aba e acompanhe junto com os exemplos abaixo. Voc\u00ea vai reconhecer os conceitos quando eles aparecerem nos laborat\u00f3rios.</p>"},{"location":"aulas/intro/#aplicacoes-em-visao-computacional-videos","title":"Aplica\u00e7\u00f5es em Vis\u00e3o Computacional (v\u00eddeos)","text":"<p>A ideia destes v\u00eddeos \u00e9 mostrar onde a vis\u00e3o computacional aparece no mundo real e o tipo de problema que vamos atacar na disciplina.</p> <ul> <li> <p>Aplica\u00e7\u00e3o 1</p> <p>Assistir no YouTube</p> </li> <li> <p>Aplica\u00e7\u00e3o 2</p> <p>Assistir no YouTube</p> </li> <li> <p>Aplica\u00e7\u00e3o 3</p> <p>Assistir no YouTube</p> </li> </ul> Ver v\u00eddeos embutidos (player) <p></p> <p></p> <p></p>"},{"location":"aulas/intro/#projetos-passados-inspiracao","title":"Projetos passados (inspira\u00e7\u00e3o)","text":"<p>A seguir est\u00e3o playlists/v\u00eddeos de projetos de turmas anteriores. Use como refer\u00eancia para entender o n\u00edvel esperado de prot\u00f3tipo e apresenta\u00e7\u00e3o.</p> <ul> <li> <p>Projeto 1</p> <p>Assistir</p> </li> <li> <p>Projeto 2</p> <p>Assistir</p> </li> <li> <p>Projeto 3</p> <p>Assistir</p> </li> <li> <p>Projeto 4</p> <p>Assistir</p> </li> </ul> Ver projetos embutidos (player) <p></p> <p></p> <p></p> <p></p>"},{"location":"recursos/bibliografia/","title":"Bibliografia","text":""},{"location":"recursos/bibliografia/#bibliografia-basica","title":"Bibliografia B\u00e1sica","text":"<ol> <li> <p>GOODFELLOW, Ian; BENGIO, Yoshua; COURVILLE, Aaron. Deep Learning. Cambridge, MA: MIT Press, 2016.</p> </li> <li> <p>GONZALEZ, Rafael C.; WOODS, Richard E. Digital Image Processing. 4. ed. Boston: Pearson, 2018.</p> </li> <li> <p>SZELISKI, Richard. Computer Vision: Algorithms and Applications. 2. ed. Cham: Springer, 2022.</p> </li> </ol>"},{"location":"recursos/bibliografia/#bibliografia-complementar","title":"Bibliografia Complementar","text":"<ol> <li> <p>HARTLEY, Richard; ZISSERMAN, Andrew. Multiple View Geometry in Computer Vision. 2. ed. Cambridge: Cambridge University Press, 2004.</p> </li> <li> <p>CHOLLET, Fran\u00e7ois. Deep Learning with Python. 2. ed. Shelter Island, NY: Manning, 2021.</p> </li> <li> <p>HOWSE, Joseph; MINICHINO, Joe. Learning OpenCV 4: Computer Vision with Python 3. 3. ed. Birmingham: Packt, 2020.</p> </li> </ol>"},{"location":"recursos/ferramentas/","title":"Ferramentas e Instala\u00e7\u00e3o \u2014 Vis\u00e3o Computacional","text":"<p>Escolha seu caminho</p> <p>Voc\u00ea pode trabalhar localmente (recomendado) ou usar Google Colab (sem instala\u00e7\u00e3o).</p> <p>Baixar Python VS Code Google Colab</p>"},{"location":"recursos/ferramentas/#1-opcao-a-instalacao-local-recomendada","title":"1) Op\u00e7\u00e3o A \u2014 Instala\u00e7\u00e3o local (recomendada)","text":""},{"location":"recursos/ferramentas/#11-python","title":"1.1 Python","text":"<p>Vers\u00e3o recomendada</p> <p>Use Python 3.11 ou 3.12 para melhor compatibilidade com o ecossistema do curso.</p> WindowsmacOSLinux (Ubuntu/Debian) <ol> <li>Baixe o instalador em: <code>https://www.python.org/downloads/</code></li> <li>Durante a instala\u00e7\u00e3o, marque Add Python to PATH.</li> <li>Verifique no terminal:     <pre><code>python --version\n</code></pre></li> </ol> <p>Homebrew (recomendado, se voc\u00ea j\u00e1 usa): <pre><code>brew install python\npython3 --version\n</code></pre></p> <pre><code>sudo apt update\nsudo apt install -y python3 python3-venv python3-pip\npython3 --version\n</code></pre>"},{"location":"recursos/ferramentas/#12-ide-ambiente-de-desenvolvimento","title":"1.2 IDE (ambiente de desenvolvimento)","text":"<p>Recomendado para o curso</p> <ul> <li>Visual Studio Code (principal)</li> <li>JupyterLab (notebooks)</li> <li>Google Colab (alternativa em nuvem)</li> </ul> <p>VS Code (passo a passo)</p> <ol> <li>Instale: https://code.visualstudio.com/</li> <li>Abra o VS Code e instale a extens\u00e3o Python (Microsoft).</li> <li>(Opcional) Instale tamb\u00e9m Jupyter (para notebooks).</li> </ol>"},{"location":"recursos/ferramentas/#13-crie-um-ambiente-virtual-venv","title":"1.3 Crie um ambiente virtual (venv)","text":"<p>Por que usar venv?</p> <p>Mant\u00e9m as depend\u00eancias do curso isoladas do restante do seu sistema.</p> macOS / LinuxWindows (PowerShell) <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\npython -m pip install --upgrade pip\n</code></pre> <pre><code>python -m venv .venv\n.\\.venv\\Scripts\\Activate.ps1\npython -m pip install --upgrade pip\n</code></pre> <p>PowerShell bloqueando ativa\u00e7\u00e3o?</p> <p>Execute uma vez: <pre><code>Set-ExecutionPolicy -Scope CurrentUser RemoteSigned\n</code></pre></p>"},{"location":"recursos/ferramentas/#14-instale-as-bibliotecas-do-curso","title":"1.4 Instale as bibliotecas do curso","text":"<p>Pacote m\u00ednimo do curso (primeiras aulas)</p> <pre><code>pip install numpy matplotlib scikit-learn jupyterlab opencv-python\n</code></pre> <p>Deep Learning (quando o curso entrar em redes neurais)</p> <pre><code>pip install tensorflow\n</code></pre> macOS Apple Silicon (opcional: acelera\u00e7\u00e3o por GPU/Metal) <pre><code>pip install tensorflow-metal\n</code></pre>"},{"location":"recursos/ferramentas/#15-teste-rapido-do-ambiente-sanity-check","title":"1.5 Teste r\u00e1pido do ambiente (sanity check)","text":"<p>Crie um arquivo <code>check_env.py</code> com o conte\u00fado abaixo:</p> <pre><code>import sys\nimport cv2\nimport numpy as np\nimport matplotlib\nimport sklearn\n\nprint(\"Python:\", sys.version.split()[0])\nprint(\"OpenCV:\", cv2.__version__)\nprint(\"NumPy:\", np.__version__)\nprint(\"Matplotlib:\", matplotlib.__version__)\nprint(\"Scikit-learn:\", sklearn.__version__)\nprint(\"OK \u2705\")\n</code></pre> <p>Rode no terminal (com o <code>.venv</code> ativado): <pre><code>python check_env.py\n</code></pre></p> <p>Se apareceu OK \u2705</p> <p>Seu ambiente est\u00e1 pronto para as aulas pr\u00e1ticas.</p>"},{"location":"recursos/ferramentas/#2-opcao-b-google-colab-sem-instalacao","title":"2) Op\u00e7\u00e3o B \u2014 Google Colab (sem instala\u00e7\u00e3o)","text":"<p>Quando usar</p> <p>Use o Colab se voc\u00ea estiver sem permiss\u00e3o de instalar coisas no computador ou se precisar de GPU.</p> <ol> <li>Acesse: https://colab.research.google.com/</li> <li>Crie um notebook e rode (se necess\u00e1rio):     <pre><code>!pip install opencv-python\n</code></pre></li> </ol> <p>Observa\u00e7\u00e3o</p> <p>No Colab, NumPy/Matplotlib geralmente j\u00e1 v\u00eam instalados.</p>"},{"location":"recursos/ferramentas/#3-abrir-o-jupyterlab","title":"3) Abrir o JupyterLab","text":"<p>Com o ambiente virtual ativado: <pre><code>jupyter lab\n</code></pre></p>"},{"location":"sobre/avaliacao/","title":"Avalia\u00e7\u00e3o","text":""},{"location":"sobre/avaliacao/#atencao-integridade-academica","title":"Aten\u00e7\u00e3o: integridade acad\u00eamica","text":"<ul> <li>Desonestidade intelectual n\u00e3o ser\u00e1 tolerada.</li> <li>Autoavalia\u00e7\u00e3o: uma autoavalia\u00e7\u00e3o incompat\u00edvel com o material entregue ser\u00e1 interpretada como desonestidade intelectual.</li> <li>Pl\u00e1gio: atividades podem incluir pesquisa e cita\u00e7\u00f5es, mas todo conte\u00fado (texto, imagem, c\u00f3digo, gr\u00e1fico, etc.) que n\u00e3o seja autoral deve ter refer\u00eancias.  </li> <li>Inclua uma se\u00e7\u00e3o \u201cRefer\u00eancias\u201d com os hiperlinks utilizados (ou refer\u00eancias bibliogr\u00e1ficas, quando aplic\u00e1vel).</li> <li>C\u00f3pia entre grupos: n\u00e3o \u00e9 permitida a troca de trabalhos. Cada grupo \u00e9 respons\u00e1vel pelo pr\u00f3prio desenvolvimento intelectual.  </li> <li>Atividades duplicadas resultar\u00e3o na anula\u00e7\u00e3o de ambas.</li> </ul>"},{"location":"sobre/avaliacao/#como-funcionam-os-checkpoints","title":"Como funcionam os checkpoints","text":""},{"location":"sobre/avaliacao/#o-que-sao","title":"O que s\u00e3o","text":"<p>Os checkpoints s\u00e3o instrumentos de avalia\u00e7\u00e3o da disciplina, desenhados para verificar a consolida\u00e7\u00e3o dos conte\u00fados e habilidades trabalhados ao longo do semestre.</p>"},{"location":"sobre/avaliacao/#quantos-existem","title":"Quantos existem","text":"<p>S\u00e3o 3 checkpoints por semestre.</p>"},{"location":"sobre/avaliacao/#formato","title":"Formato","text":"<p>Cada checkpoint pode assumir um dos formatos abaixo (conforme comunicado na publica\u00e7\u00e3o): - Prova (avalia\u00e7\u00e3o individual) - Projeto (desafio pr\u00e1tico, individual ou em grupo \u2014 conforme a regra do checkpoint)</p> <p>O formato, regras de colabora\u00e7\u00e3o e crit\u00e9rios de corre\u00e7\u00e3o ser\u00e3o sempre informados no enunciado do checkpoint.</p>"},{"location":"sobre/avaliacao/#quando-acontecem","title":"Quando acontecem","text":"<ul> <li>Os checkpoints s\u00e3o divulgados com anteced\u00eancia, de acordo com a agenda da disciplina.</li> <li>Em geral, devem ser realizados fora do hor\u00e1rio de aula, exceto quando o checkpoint for explicitamente aplicado em sala.</li> </ul>"},{"location":"sobre/avaliacao/#onde-e-como-sera-feito","title":"Onde e como ser\u00e1 feito","text":"<ul> <li>Quando for projeto, a entrega normalmente ocorrer\u00e1 via GitHub Classroom (reposit\u00f3rio privado), onde o estudante/grupo dever\u00e1 registrar a evolu\u00e7\u00e3o por meio de commits.</li> <li>Quando for prova, a aplica\u00e7\u00e3o e o formato de entrega seguir\u00e3o as instru\u00e7\u00f5es publicadas no checkpoint.</li> </ul>"},{"location":"sobre/avaliacao/#criterios-e-rubrica","title":"Crit\u00e9rios e rubrica","text":"<ul> <li>Cada checkpoint vem acompanhado de uma rubrica de avalia\u00e7\u00e3o (crit\u00e9rios e pesos), pensada para guiar o desenvolvimento e tornar a corre\u00e7\u00e3o transparente.</li> <li>O estudante deve usar a rubrica como \u201cchecklist\u201d do que \u00e9 esperado.</li> </ul>"},{"location":"sobre/avaliacao/#entrega-quando-aplicavel","title":"Entrega (quando aplic\u00e1vel)","text":""},{"location":"sobre/avaliacao/#prazo","title":"Prazo","text":"<ul> <li>A entrega deve ser feita at\u00e9 a data/hora definida no enunciado.</li> <li>Entregas fora do prazo seguem a pol\u00edtica definida pelo professor/institui\u00e7\u00e3o (quando aplic\u00e1vel).</li> </ul>"},{"location":"sobre/avaliacao/#autoavaliacao","title":"Autoavalia\u00e7\u00e3o","text":"<ul> <li>No momento da entrega, o estudante poder\u00e1 responder um formul\u00e1rio de autoavalia\u00e7\u00e3o (ex.: Google Forms), descrevendo:</li> <li>o que foi implementado,</li> <li>o que ficou pendente,</li> <li>evid\u00eancias (links/prints/commits) quando solicitado.</li> </ul>"},{"location":"sobre/avaliacao/#como-sera-avaliado","title":"Como ser\u00e1 avaliado","text":"<ul> <li>A avalia\u00e7\u00e3o pode ocorrer:</li> <li>em sala, na data prevista na agenda, e/ou</li> <li>por an\u00e1lise do reposit\u00f3rio/entrega, conforme regras do checkpoint.</li> <li>Quando houver avalia\u00e7\u00e3o pr\u00e1tica/oral, o estudante dever\u00e1 demonstrar:</li> <li>a evolu\u00e7\u00e3o do trabalho (ex.: hist\u00f3rico de commits),</li> <li>o funcionamento do que foi implementado,</li> <li>dom\u00ednio t\u00e9cnico do que foi entregue.</li> <li>A autoavalia\u00e7\u00e3o ser\u00e1 utilizada como apoio, mas n\u00e3o substitui as evid\u00eancias do reposit\u00f3rio/entrega.</li> </ul>"},{"location":"sobre/avaliacao/#o-que-devo-fazer-agora","title":"O que devo fazer agora?","text":"<p>Acompanhe a agenda da disciplina e aguarde a libera\u00e7\u00e3o do pr\u00f3ximo checkpoint. Assim que publicado, leia o enunciado completo e a rubrica antes de iniciar.</p>"},{"location":"sobre/conteudo/","title":"Conte\u00fado Program\u00e1tico","text":""},{"location":"sobre/conteudo/#carga-horaria","title":"Carga Hor\u00e1ria","text":"<ul> <li>2h aula por semana</li> </ul>"},{"location":"sobre/conteudo/#unidade-1-fundamentos-e-pipeline-de-visao-computacional","title":"Unidade 1 \u2013 Fundamentos e Pipeline de Vis\u00e3o Computacional","text":"<ul> <li>Conceitos fundamentais</li> <li>Representa\u00e7\u00e3o digital de imagens</li> <li>Pipeline de sistemas de CV</li> <li>Estrutura\u00e7\u00e3o de projetos</li> </ul>"},{"location":"sobre/conteudo/#unidade-2-processamento-e-segmentacao","title":"Unidade 2 \u2013 Processamento e Segmenta\u00e7\u00e3o","text":"<ul> <li>Espa\u00e7os de cor</li> <li>Histogramas</li> <li>Filtros e convolu\u00e7\u00e3o</li> <li>Binariza\u00e7\u00e3o e segmenta\u00e7\u00e3o</li> <li>Contornos e bounding boxes</li> </ul>"},{"location":"sobre/conteudo/#unidade-3-extracao-de-caracteristicas-e-machine-learning","title":"Unidade 3 \u2013 Extra\u00e7\u00e3o de Caracter\u00edsticas e Machine Learning","text":"<ul> <li>Keypoints e descritores</li> <li>Vetoriza\u00e7\u00e3o de imagens</li> <li>Classifica\u00e7\u00e3o supervisionada</li> <li>Avalia\u00e7\u00e3o de modelos</li> </ul>"},{"location":"sobre/conteudo/#unidade-4-deep-learning-aplicado","title":"Unidade 4 \u2013 Deep Learning Aplicado","text":"<ul> <li>Introdu\u00e7\u00e3o \u00e0s CNNs</li> <li>Transfer Learning</li> <li>Detec\u00e7\u00e3o de objetos (YOLO)</li> <li>Otimiza\u00e7\u00e3o e deploy</li> </ul>"},{"location":"sobre/conteudo/#unidade-5-integracao-e-engenharia","title":"Unidade 5 \u2013 Integra\u00e7\u00e3o e Engenharia","text":"<ul> <li>Constru\u00e7\u00e3o de APIs (FastAPI/Flask)</li> <li>Estrutura arquitetural</li> <li>Containeriza\u00e7\u00e3o</li> <li>Boas pr\u00e1ticas e organiza\u00e7\u00e3o</li> </ul>"},{"location":"sobre/metodologia/","title":"Metodologia","text":""},{"location":"sobre/metodologia/#abordagem-pedagogica","title":"Abordagem Pedag\u00f3gica","text":"<p>A disciplina adota uma abordagem te\u00f3rico-pr\u00e1tica com foco em aprendizagem baseada em projetos (Project-Based Learning). </p> <p>Os alunos trabalhar\u00e3o em ciclos de desenvolvimento incremental, construindo solu\u00e7\u00f5es reais de vis\u00e3o computacional.</p>"},{"location":"sobre/metodologia/#estrategias-de-ensino","title":"Estrat\u00e9gias de Ensino","text":"<ul> <li>Aulas expositivas dialogadas: Apresenta\u00e7\u00e3o dos conceitos te\u00f3ricos com intera\u00e7\u00e3o entre professor e alunos</li> <li>Demonstra\u00e7\u00f5es pr\u00e1ticas: Exemplos em tempo real de implementa\u00e7\u00e3o de algoritmos de vis\u00e3o computacional</li> <li>Implementa\u00e7\u00e3o incremental de projeto: Desenvolvimento cont\u00ednuo de um projeto ao longo da disciplina</li> <li>Estudos de caso reais: An\u00e1lise de aplica\u00e7\u00f5es reais de vis\u00e3o computacional em diferentes dom\u00ednios</li> <li>Desenvolvimento de sistema aplicado: Cria\u00e7\u00e3o de solu\u00e7\u00f5es funcionais para problemas espec\u00edficos</li> </ul>"},{"location":"sobre/objetivos/","title":"Objetivos da Disciplina","text":""},{"location":"sobre/objetivos/#objetivo-geral","title":"Objetivo Geral","text":"<p>Capacitar o aluno a projetar, implementar e integrar solu\u00e7\u00f5es de vis\u00e3o computacional em sistemas de software, aplicando t\u00e9cnicas modernas de processamento de imagens e aprendizado de m\u00e1quina.</p>"},{"location":"sobre/objetivos/#objetivos-especificos","title":"Objetivos Espec\u00edficos","text":"<p>Ao final da disciplina o aluno ser\u00e1 capaz de:</p> <ul> <li>Compreender o pipeline completo de um sistema de vis\u00e3o computacional</li> <li>Aplicar t\u00e9cnicas de pr\u00e9-processamento e segmenta\u00e7\u00e3o de imagens</li> <li>Implementar classificadores baseados em machine learning tradicional</li> <li>Utilizar redes neurais convolucionais com transfer learning</li> <li>Implementar detec\u00e7\u00e3o de objetos com modelos modernos</li> <li>Estruturar projetos de vis\u00e3o computacional seguindo boas pr\u00e1ticas de engenharia de software</li> <li>Criar APIs para disponibiliza\u00e7\u00e3o de modelos</li> <li>Avaliar desempenho e otimizar infer\u00eancia</li> <li>Documentar e apresentar solu\u00e7\u00f5es aplicadas</li> </ul>"},{"location":"unidades/unidade1/","title":"Unidade 1 - Fundamentos e Pipeline de Vis\u00e3o Computacional","text":""},{"location":"unidades/unidade1/#conceitos-fundamentais","title":"Conceitos Fundamentais","text":"<p>Vis\u00e3o Computacional (Computer Vision - CV) \u00e9 um campo da intelig\u00eancia artificial que busca capacitar m\u00e1quinas com a habilidade de \"ver\" e interpretar o mundo visual da mesma forma que os humanos fazem. Esta \u00e1rea combina conhecimentos de processamento de imagens, aprendizado de m\u00e1quina e intelig\u00eancia artificial para extrair informa\u00e7\u00f5es significativas de imagens digitais.</p>"},{"location":"unidades/unidade1/#definicoes-importantes","title":"Defini\u00e7\u00f5es Importantes","text":"<ul> <li>Processamento de Imagens: Manipula\u00e7\u00e3o de imagens para melhorar qualidade, real\u00e7ar caracter\u00edsticas ou preparar para outras etapas</li> <li>An\u00e1lise de Imagens: Extra\u00e7\u00e3o de informa\u00e7\u00f5es sem\u00e2nticas de imagens para tomada de decis\u00e3o</li> <li>Percep\u00e7\u00e3o Visual: Interpreta\u00e7\u00e3o de cenas e objetos em um contexto mais amplo</li> </ul>"},{"location":"unidades/unidade1/#representacao-digital-de-imagens","title":"Representa\u00e7\u00e3o Digital de Imagens","text":""},{"location":"unidades/unidade1/#tipos-de-imagens","title":"Tipos de Imagens","text":""},{"location":"unidades/unidade1/#imagens-em-nivel-de-cinza","title":"Imagens em N\u00edvel de Cinza","text":"<ul> <li>Cada pixel representa intensidade luminosa</li> <li>Valores tipicamente variam de 0 (preto) a 255 (branco)</li> <li>Menor volume de dados comparado a imagens coloridas</li> </ul>"},{"location":"unidades/unidade1/#imagens-coloridas","title":"Imagens Coloridas","text":"<ul> <li>RGB (Red, Green, Blue): Combina\u00e7\u00e3o de tr\u00eas canais de cores prim\u00e1rias</li> <li>HSV (Hue, Saturation, Value): Separar cor, satura\u00e7\u00e3o e brilho</li> <li>CMYK: Usado principalmente em impress\u00e3o</li> </ul>"},{"location":"unidades/unidade1/#estrutura-de-dados","title":"Estrutura de Dados","text":"<p>Uma imagem digital \u00e9 representada como uma matriz (ou tensor) de valores:</p> <pre><code># Exemplo de representa\u00e7\u00e3o de imagem\nimport numpy as np\n\n# Imagem em escala de cinza 100x100 pixels\nimagem_cinza = np.zeros((100, 100), dtype=np.uint8)\n\n# Imagem RGB 100x100 pixels\nimagem_rgb = np.zeros((100, 100, 3), dtype=np.uint8)\n</code></pre>"},{"location":"unidades/unidade1/#pipeline-de-sistemas-de-visao-computacional","title":"Pipeline de Sistemas de Vis\u00e3o Computacional","text":"<p>Um pipeline de vis\u00e3o computacional t\u00edpico consiste nas seguintes etapas:</p>"},{"location":"unidades/unidade1/#1-aquisicao-de-imagem","title":"1. Aquisi\u00e7\u00e3o de Imagem","text":"<ul> <li>Captura de imagens por c\u00e2meras, scanners ou fontes digitais</li> <li>Considera\u00e7\u00f5es: resolu\u00e7\u00e3o, taxa de quadros, condi\u00e7\u00f5es de ilumina\u00e7\u00e3o</li> </ul>"},{"location":"unidades/unidade1/#2-pre-processamento","title":"2. Pr\u00e9-processamento","text":"<ul> <li>Redimensionamento e normaliza\u00e7\u00e3o</li> <li>Corre\u00e7\u00e3o de distor\u00e7\u00f5es</li> <li>Melhoria de contraste</li> <li>Remo\u00e7\u00e3o de ru\u00eddo</li> </ul>"},{"location":"unidades/unidade1/#3-extracao-de-caracteristicas","title":"3. Extra\u00e7\u00e3o de Caracter\u00edsticas","text":"<ul> <li>Detec\u00e7\u00e3o de bordas, cantos e pontos de interesse</li> <li>Extra\u00e7\u00e3o de descritores visuais</li> <li>Transforma\u00e7\u00f5es espaciais e espectrais</li> </ul>"},{"location":"unidades/unidade1/#4-modelagem-e-analise","title":"4. Modelagem e An\u00e1lise","text":"<ul> <li>Classifica\u00e7\u00e3o de objetos</li> <li>Segmenta\u00e7\u00e3o de regi\u00f5es</li> <li>Detec\u00e7\u00e3o de padr\u00f5es</li> <li>Reconhecimento de formas</li> </ul>"},{"location":"unidades/unidade1/#5-pos-processamento","title":"5. P\u00f3s-processamento","text":"<ul> <li>Filtragem de resultados</li> <li>Fus\u00e3o de informa\u00e7\u00f5es</li> <li>Tomada de decis\u00e3o baseada em regras</li> </ul>"},{"location":"unidades/unidade1/#6-deploy-e-integracao","title":"6. Deploy e Integra\u00e7\u00e3o","text":"<ul> <li>Implementa\u00e7\u00e3o em sistemas reais</li> <li>Otimiza\u00e7\u00e3o de desempenho</li> <li>Monitoramento e manuten\u00e7\u00e3o</li> </ul>"},{"location":"unidades/unidade1/#arquitetura-de-projetos-de-visao-computacional","title":"Arquitetura de Projetos de Vis\u00e3o Computacional","text":""},{"location":"unidades/unidade1/#estruturacao-de-codigo","title":"Estrutura\u00e7\u00e3o de C\u00f3digo","text":"<p>Para garantir manutenibilidade e escalabilidade, projetos de vis\u00e3o computacional devem seguir boas pr\u00e1ticas de engenharia de software:</p> <pre><code>projeto_visao_computacional/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 preprocessing/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 filters.py\n\u2502   \u2502   \u2514\u2500\u2500 normalization.py\n\u2502   \u251c\u2500\u2500 features/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 detectors.py\n\u2502   \u2502   \u2514\u2500\u2500 descriptors.py\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 classifiers.py\n\u2502   \u2502   \u2514\u2500\u2500 neural_networks.py\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 visualization.py\n\u2502   \u2502   \u2514\u2500\u2500 io.py\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 processed/\n\u2502   \u2514\u2500\u2500 external/\n\u251c\u2500\u2500 notebooks/\n\u251c\u2500\u2500 tests/\n\u251c\u2500\u2500 docs/\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"unidades/unidade1/#principios-de-design","title":"Princ\u00edpios de Design","text":"<ul> <li>Modularidade: Separa\u00e7\u00e3o clara de responsabilidades</li> <li>Reusabilidade: Componentes reutiliz\u00e1veis em diferentes contextos</li> <li>Testabilidade: Facilidade de testar individualmente cada componente</li> <li>Documenta\u00e7\u00e3o: C\u00f3digo bem documentado e explicativo</li> </ul>"},{"location":"unidades/unidade1/#aplicacoes-reais-de-visao-computacional","title":"Aplica\u00e7\u00f5es Reais de Vis\u00e3o Computacional","text":""},{"location":"unidades/unidade1/#industria","title":"Ind\u00fastria","text":"<ul> <li>Inspe\u00e7\u00e3o de qualidade de produtos</li> <li>Rob\u00f4s aut\u00f4nomos em f\u00e1bricas</li> <li>Controle de processos produtivos</li> </ul>"},{"location":"unidades/unidade1/#saude","title":"Sa\u00fade","text":"<ul> <li>Diagn\u00f3stico por imagem (raio-X, resson\u00e2ncia magn\u00e9tica)</li> <li>An\u00e1lise de c\u00e9lulas e tecidos</li> <li>Cirurgia assistida por computador</li> </ul>"},{"location":"unidades/unidade1/#financas","title":"Finan\u00e7as","text":"<ul> <li>Leitura \u00f3ptica de cheques</li> <li>Verifica\u00e7\u00e3o biom\u00e9trica</li> <li>An\u00e1lise de documentos</li> </ul>"},{"location":"unidades/unidade1/#varejo","title":"Varejo","text":"<ul> <li>Checkout autom\u00e1tico</li> <li>An\u00e1lise de comportamento do consumidor</li> <li>Reposi\u00e7\u00e3o autom\u00e1tica de estoque</li> </ul>"},{"location":"unidades/unidade1/#desafios-em-visao-computacional","title":"Desafios em Vis\u00e3o Computacional","text":"<ul> <li>Varia\u00e7\u00e3o de ilumina\u00e7\u00e3o: Afeta a apar\u00eancia dos objetos</li> <li>Oclus\u00e3o: Partes dos objetos podem estar escondidas</li> <li>Deforma\u00e7\u00e3o: Objetos podem ter formas variadas</li> <li>Escala e rota\u00e7\u00e3o: Mesmo objeto pode aparecer em diferentes tamanhos e orienta\u00e7\u00f5es</li> <li>Ru\u00eddo: Imperfei\u00e7\u00f5es na captura de imagem</li> <li>Velocidade: Necessidade de processamento em tempo real</li> </ul>"},{"location":"unidades/unidade1/#consideracoes-eticas-e-de-privacidade","title":"Considera\u00e7\u00f5es \u00c9ticas e de Privacidade","text":"<p>Ao desenvolver sistemas de vis\u00e3o computacional, \u00e9 importante considerar:</p> <ul> <li>Vi\u00e9s algor\u00edtmico: Garantir que os modelos n\u00e3o discriminem injustamente</li> <li>Privacidade: Proteger dados pessoais e sens\u00edveis</li> <li>Transpar\u00eancia: Explicar como as decis\u00f5es s\u00e3o tomadas</li> <li>Consentimento: Informar e obter permiss\u00e3o para uso de imagens</li> </ul>"},{"location":"unidades/unidade2/","title":"Unidade 2 - Processamento e Segmenta\u00e7\u00e3o","text":""},{"location":"unidades/unidade2/#espacos-de-cor","title":"Espa\u00e7os de Cor","text":"<p>O espa\u00e7o de cor define como as cores s\u00e3o representadas em uma imagem digital. Cada espa\u00e7o de cor tem suas vantagens para diferentes tipos de opera\u00e7\u00f5es de processamento de imagem.</p>"},{"location":"unidades/unidade2/#rgb-red-green-blue","title":"RGB (Red, Green, Blue)","text":"<ul> <li>Modelo aditivo baseado na combina\u00e7\u00e3o de luz vermelha, verde e azul</li> <li>Adequado para dispositivos de exibi\u00e7\u00e3o</li> <li>Limita\u00e7\u00f5es para separa\u00e7\u00e3o de cor e brilho</li> </ul> <pre><code>import cv2\nimport matplotlib.pyplot as plt\n\n# Carregar imagem\nimg = cv2.imread('imagem.jpg')\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Visualizar canais RGB separadamente\nfig, axes = plt.subplots(1, 4, figsize=(15, 4))\naxes[0].imshow(img_rgb)\naxes[0].set_title('Imagem Original')\naxes[1].imshow(img_rgb[:,:,0], cmap='Reds_r')\naxes[1].set_title('Canal R')\naxes[2].imshow(img_rgb[:,:,1], cmap='Greens_r')\naxes[2].set_title('Canal G')\naxes[3].imshow(img_rgb[:,:,2], cmap='Blues_r')\naxes[3].set_title('Canal B')\nplt.show()\n</code></pre>"},{"location":"unidades/unidade2/#hsv-hue-saturation-value","title":"HSV (Hue, Saturation, Value)","text":"<ul> <li>Hue (Matiz): Cor pura (0-360\u00b0)</li> <li>Saturation (Satura\u00e7\u00e3o): Pureza da cor (0-100%)</li> <li>Value (Valor): Brilho (0-100%)</li> </ul> <p>Ideal para opera\u00e7\u00f5es baseadas em cor, como detec\u00e7\u00e3o de objetos por cor espec\u00edfica.</p> <pre><code>import cv2\nimport numpy as np\n\n# Converter RGB para HSV\nimg_hsv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)\n\n# Detectar objetos vermelhos\nlower_red = np.array([0, 50, 50])\nupper_red = np.array([10, 255, 255])\nmask1 = cv2.inRange(img_hsv, lower_red, upper_red)\n\n# Para vermelho tamb\u00e9m pode haver transi\u00e7\u00e3o no limite do c\u00edrculo crom\u00e1tico\nlower_red2 = np.array([170, 50, 50])\nupper_red2 = np.array([180, 255, 255])\nmask2 = cv2.inRange(img_hsv, lower_red2, upper_red2)\n\n# Combinar m\u00e1scaras\nred_mask = mask1 + mask2\n</code></pre>"},{"location":"unidades/unidade2/#outros-espacos-de-cor","title":"Outros Espa\u00e7os de Cor","text":"<ul> <li>LAB: Separar lumin\u00e2ncia de cor (L*a*b*)</li> <li>YUV/YCrCb: Separar lumin\u00e2ncia de cromin\u00e2ncia</li> <li>Grayscale: Convers\u00e3o para tons de cinza</li> </ul>"},{"location":"unidades/unidade2/#histogramas","title":"Histogramas","text":"<p>O histograma de uma imagem mostra a distribui\u00e7\u00e3o de intensidades de pixel, \u00fatil para an\u00e1lise e corre\u00e7\u00e3o de contraste.</p>"},{"location":"unidades/unidade2/#histograma-em-escala-de-cinza","title":"Histograma em Escala de Cinza","text":"<pre><code>import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Calcular histograma\ngray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\nhistogram = cv2.calcHist([gray], [0], None, [256], [0, 256])\n\n# Plotar histograma\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.imshow(gray, cmap='gray')\nplt.title('Imagem em Escala de Cinza')\nplt.subplot(1, 2, 2)\nplt.plot(histogram)\nplt.title('Histograma')\nplt.xlabel('Intensidade')\nplt.ylabel('N\u00famero de Pixels')\nplt.show()\n</code></pre>"},{"location":"unidades/unidade2/#equalizacao-de-histograma","title":"Equaliza\u00e7\u00e3o de Histograma","text":"<p>Melhora o contraste de imagens com baixo contraste:</p> <pre><code># Equaliza\u00e7\u00e3o de histograma\nequalized = cv2.equalizeHist(gray)\n\n# Comparar antes e depois\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\naxes[0,0].imshow(gray, cmap='gray')\naxes[0,0].set_title('Original')\naxes[0,1].plot(cv2.calcHist([gray], [0], None, [256], [0, 256]))\naxes[0,1].set_title('Histograma Original')\n\naxes[1,0].imshow(equalized, cmap='gray')\naxes[1,0].set_title('Equalizada')\naxes[1,1].plot(cv2.calcHist([equalized], [0], None, [256], [0, 256]))\naxes[1,1].set_title('Histograma Equalizado')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"unidades/unidade2/#filtros-e-convolucao","title":"Filtros e Convolu\u00e7\u00e3o","text":"<p>A convolu\u00e7\u00e3o \u00e9 uma opera\u00e7\u00e3o fundamental em processamento de imagem, usada para aplicar filtros e transforma\u00e7\u00f5es locais.</p>"},{"location":"unidades/unidade2/#tipos-de-filtros","title":"Tipos de Filtros","text":""},{"location":"unidades/unidade2/#filtros-passa-baixa-suavizacao","title":"Filtros Passa-Baixa (Suaviza\u00e7\u00e3o)","text":"<ul> <li>Reduzem ru\u00eddo e detalhes finos</li> <li>M\u00e9dia, gaussiano, mediana</li> </ul> <pre><code># Filtro de m\u00e9dia\nblurred_avg = cv2.blur(gray, (5, 5))\n\n# Filtro Gaussiano\nblurred_gaussian = cv2.GaussianBlur(gray, (5, 5), 0)\n\n# Filtro de mediana (bom para ru\u00eddo sal e pimenta)\nblurred_median = cv2.medianBlur(gray, 5)\n\n# Comparar resultados\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\naxes[0,0].imshow(gray, cmap='gray')\naxes[0,0].set_title('Original')\naxes[0,1].imshow(blurred_avg, cmap='gray')\naxes[0,1].set_title('Filtro M\u00e9dia')\naxes[1,0].imshow(blurred_gaussian, cmap='gray')\naxes[1,0].set_title('Filtro Gaussiano')\naxes[1,1].imshow(blurred_median, cmap='gray')\naxes[1,1].set_title('Filtro Mediana')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"unidades/unidade2/#filtros-passa-alta-realce-de-bordas","title":"Filtros Passa-Alta (Realce de Bordas)","text":"<ul> <li>Destacam varia\u00e7\u00f5es abruptas de intensidade</li> <li>Laplaciano, Sobel, Canny</li> </ul> <pre><code># Detector de bordas Sobel\nsobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\nsobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\nsobel_combined = np.sqrt(sobel_x**2 + sobel_y**2)\n\n# Detector de bordas Laplaciano\nlaplacian = cv2.Laplacian(gray, cv2.CV_64F)\n\n# Detector de bordas Canny\ncanny = cv2.Canny(gray, 100, 200)\n\n# Comparar resultados\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\naxes[0,0].imshow(gray, cmap='gray')\naxes[0,0].set_title('Original')\naxes[0,1].imshow(np.absolute(sobel_combined), cmap='gray')\naxes[0,1].set_title('Sobel')\naxes[1,0].imshow(np.absolute(laplacian), cmap='gray')\naxes[1,0].set_title('Laplaciano')\naxes[1,1].imshow(canny, cmap='gray')\naxes[1,1].set_title('Canny')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"unidades/unidade2/#binarizacao-e-segmentacao","title":"Binariza\u00e7\u00e3o e Segmenta\u00e7\u00e3o","text":""},{"location":"unidades/unidade2/#binarizacao-global","title":"Binariza\u00e7\u00e3o Global","text":"<p>Converte imagem em escala de cinza para preto e branco com base em um limiar (threshold):</p> <pre><code># Binariza\u00e7\u00e3o global\n_, binary_thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n\n# Binariza\u00e7\u00e3o adaptativa\nbinary_adaptive = cv2.adaptiveThreshold(\n    gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2\n)\n\n# Otsu's binarization\n_, binary_otsu = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n\n# Comparar m\u00e9todos\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\naxes[0,0].imshow(gray, cmap='gray')\naxes[0,0].set_title('Original')\naxes[0,1].imshow(binary_thresh, cmap='gray')\naxes[0,1].set_title('Limiar Fixo')\naxes[1,0].imshow(binary_adaptive, cmap='gray')\naxes[1,0].set_title('Adaptativo')\naxes[1,1].imshow(binary_otsu, cmap='gray')\naxes[1,1].set_title('Otsu')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"unidades/unidade2/#operacoes-morfologicas","title":"Opera\u00e7\u00f5es Morfol\u00f3gicas","text":"<p>Opera\u00e7\u00f5es morfol\u00f3gicas manipulam a estrutura geom\u00e9trica dos objetos em uma imagem bin\u00e1ria:</p> <pre><code># Elemento estruturante\nkernel = np.ones((5,5), np.uint8)\n\n# Eros\u00e3o: reduz objetos brancos\nerosion = cv2.erode(binary_thresh, kernel, iterations=1)\n\n# Dilata\u00e7\u00e3o: aumenta objetos brancos\ndilation = cv2.dilate(binary_thresh, kernel, iterations=1)\n\n# Abertura: eros\u00e3o seguida de dilata\u00e7\u00e3o (remove ru\u00eddo)\nopening = cv2.morphologyEx(binary_thresh, cv2.MORPH_OPEN, kernel)\n\n# Fechamento: dilata\u00e7\u00e3o seguida de eros\u00e3o (fecha lacunas)\nclosing = cv2.morphologyEx(binary_thresh, cv2.MORPH_CLOSE, kernel)\n\n# Comparar opera\u00e7\u00f5es\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes[0,0].imshow(binary_thresh, cmap='gray')\naxes[0,0].set_title('Original')\naxes[0,1].imshow(erosion, cmap='gray')\naxes[0,1].set_title('Eros\u00e3o')\naxes[0,2].imshow(dilation, cmap='gray')\naxes[0,2].set_title('Dilata\u00e7\u00e3o')\naxes[1,0].imshow(opening, cmap='gray')\naxes[1,0].set_title('Abertura')\naxes[1,1].imshow(closing, cmap='gray')\naxes[1,1].set_title('Fechamento')\naxes[1,2].axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"unidades/unidade2/#contornos-e-bounding-boxes","title":"Contornos e Bounding Boxes","text":"<p>Contornos s\u00e3o curvas que conectam pontos cont\u00ednuos de mesma intensidade, \u00fateis para detec\u00e7\u00e3o de formas e segmenta\u00e7\u00e3o:</p> <pre><code>import cv2\nimport numpy as np\n\n# Encontrar contornos\ncontours, hierarchy = cv2.findContours(\n    binary_thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n)\n\n# Desenhar contornos\nimg_with_contours = img_rgb.copy()\ncv2.drawContours(img_with_contours, contours, -1, (0, 255, 0), 2)\n\n# Calcular bounding boxes\nimg_bboxes = img_rgb.copy()\nfor contour in contours:\n    x, y, w, h = cv2.boundingRect(contour)\n    cv2.rectangle(img_bboxes, (x, y), (x+w, y+h), (255, 0, 0), 2)\n\n# Calcular bounding boxes rotacionadas\nimg_rotated_bboxes = img_rgb.copy()\nfor contour in contours:\n    rect = cv2.minAreaRect(contour)\n    box = cv2.boxPoints(rect)\n    box = np.int0(box)\n    cv2.drawContours(img_rotated_bboxes, [box], 0, (0, 255, 255), 2)\n\n# Visualizar resultados\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\naxes[0].imshow(img_rgb)\naxes[0].set_title('Original')\naxes[1].imshow(img_with_contours)\naxes[1].set_title('Contornos')\naxes[2].imshow(img_bboxes)\naxes[2].set_title('Bounding Boxes')\nplt.tight_layout()\nplt.show()\n\nprint(f\"N\u00famero de contornos encontrados: {len(contours)}\")\n</code></pre>"},{"location":"unidades/unidade2/#tecnicas-avancadas-de-segmentacao","title":"T\u00e9cnicas Avan\u00e7adas de Segmenta\u00e7\u00e3o","text":""},{"location":"unidades/unidade2/#segmentacao-por-crescimento-de-regiao-region-growing","title":"Segmenta\u00e7\u00e3o por Crescimento de Regi\u00e3o (Region Growing)","text":"<p>Agrupa pixels com caracter\u00edsticas semelhantes:</p> <pre><code>from skimage.segmentation import felzenszwalb\nfrom skimage.util import img_as_float\n\n# Segmenta\u00e7\u00e3o de Felzenszwalb\nsegments = felzenszwalb(img_as_float(img_rgb), scale=100, sigma=0.5, min_size=50)\n\n# Visualizar segmentos\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.imshow(img_rgb)\nplt.title('Original')\nplt.subplot(1, 2, 2)\nplt.imshow(segments, cmap='nipy_spectral')\nplt.title('Segmentos')\nplt.show()\n</code></pre>"},{"location":"unidades/unidade2/#segmentacao-por-watershed","title":"Segmenta\u00e7\u00e3o por Watershed","text":"<p>T\u00e9cnica baseada em morfologia matem\u00e1tica:</p> <pre><code>from scipy import ndimage\nfrom skimage.feature import peak_local_max\nfrom skimage.segmentation import watershed\n\n# Converter para escala de cinza\ngray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n\n# Calcular dist\u00e2ncia euclidiana\ndistance = ndimage.distance_transform_edt(binary_thresh)\n\n# Encontrar picos locais\nlocal_maxima = peak_local_max(distance, min_distance=20, labels=binary_thresh)\n\n# Criar marcadores\nmarkers, _ = ndimage.label(local_maxima)\nsegmented = watershed(-distance, markers, mask=binary_thresh)\n\n# Visualizar resultado\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 3, 1)\nplt.imshow(gray, cmap='gray')\nplt.title('Original')\nplt.subplot(1, 3, 2)\nplt.imshow(distance, cmap='hot')\nplt.title('Dist\u00e2ncia')\nplt.subplot(1, 3, 3)\nplt.imshow(segmented, cmap='nipy_spectral')\nplt.title('Watershed')\nplt.show()\n</code></pre>"},{"location":"unidades/unidade2/#aplicacoes-praticas","title":"Aplica\u00e7\u00f5es Pr\u00e1ticas","text":""},{"location":"unidades/unidade2/#deteccao-de-objetos-por-cor","title":"Detec\u00e7\u00e3o de Objetos por Cor","text":"<pre><code>def detect_objects_by_color(image, lower_color, upper_color):\n    \"\"\"Detecta objetos de uma determinada cor em uma imagem\"\"\"\n    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n\n    # Criar m\u00e1scara para a cor\n    mask = cv2.inRange(hsv, lower_color, upper_color)\n\n    # Aplicar opera\u00e7\u00f5es morfol\u00f3gicas para limpar a m\u00e1scara\n    kernel = np.ones((5,5), np.uint8)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n    # Encontrar contornos\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Desenhar bounding boxes\n    result = image.copy()\n    for contour in contours:\n        if cv2.contourArea(contour) &gt; 100:  # Filtrar pequenos contornos\n            x, y, w, h = cv2.boundingRect(contour)\n            cv2.rectangle(result, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n    return result, len(contours)\n\n# Exemplo: detectar objetos vermelhos\nlower_red = np.array([0, 50, 50])\nupper_red = np.array([10, 255, 255])\nresult_img, count = detect_objects_by_color(img_rgb, lower_red, upper_red)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(img_rgb)\nplt.title('Original')\nplt.subplot(1, 2, 2)\nplt.imshow(result_img)\nplt.title(f'Detec\u00e7\u00e3o de Objetos Vermelhos ({count} encontrados)')\nplt.show()\n</code></pre>"},{"location":"unidades/unidade2/#consideracoes-praticas","title":"Considera\u00e7\u00f5es Pr\u00e1ticas","text":""},{"location":"unidades/unidade2/#escolha-de-tecnicas","title":"Escolha de T\u00e9cnicas","text":"<ul> <li>Ilumina\u00e7\u00e3o: Use espa\u00e7os de cor adequados (HSV para varia\u00e7\u00f5es de brilho)</li> <li>Ru\u00eddo: Aplique filtros antes de opera\u00e7\u00f5es sens\u00edveis</li> <li>Desempenho: Considere complexidade computacional para aplica\u00e7\u00f5es em tempo real</li> <li>Precis\u00e3o: Ajuste par\u00e2metros com base nas caracter\u00edsticas espec\u00edficas do problema</li> </ul>"},{"location":"unidades/unidade2/#avaliacao-de-resultados","title":"Avalia\u00e7\u00e3o de Resultados","text":"<ul> <li>Precis\u00e3o: Propor\u00e7\u00e3o de pixels corretamente classificados</li> <li>Recall: Propor\u00e7\u00e3o de pixels relevantes detectados</li> <li>IoU (Intersection over Union): Medida de sobreposi\u00e7\u00e3o entre segmenta\u00e7\u00e3o predita e real</li> </ul>"},{"location":"unidades/unidade3/","title":"Unidade 3 - Extra\u00e7\u00e3o de Caracter\u00edsticas e Machine Learning","text":""},{"location":"unidades/unidade3/#introducao-a-extracao-de-caracteristicas","title":"Introdu\u00e7\u00e3o \u00e0 Extra\u00e7\u00e3o de Caracter\u00edsticas","text":"<p>A extra\u00e7\u00e3o de caracter\u00edsticas \u00e9 um passo cr\u00edtico em sistemas de vis\u00e3o computacional, onde informa\u00e7\u00f5es relevantes s\u00e3o extra\u00eddas de imagens para serem usadas em tarefas de classifica\u00e7\u00e3o, detec\u00e7\u00e3o ou reconhecimento. Uma boa representa\u00e7\u00e3o de caracter\u00edsticas permite que algoritmos de aprendizado de m\u00e1quina identifiquem padr\u00f5es de forma eficiente.</p>"},{"location":"unidades/unidade3/#o-que-sao-caracteristicas-features","title":"O que s\u00e3o Caracter\u00edsticas (Features)?","text":"<p>Caracter\u00edsticas s\u00e3o propriedades quantific\u00e1veis de uma imagem que representam aspectos importantes para uma tarefa espec\u00edfica. Podem ser:</p> <ul> <li>Caracter\u00edsticas de baixo n\u00edvel: Bordas, texturas, cores, gradientes</li> <li>Caracter\u00edsticas de m\u00e9dio n\u00edvel: Cantos, blobs, keypoints</li> <li>Caracter\u00edsticas de alto n\u00edvel: Objetos, partes de objetos, rela\u00e7\u00f5es espaciais</li> </ul>"},{"location":"unidades/unidade3/#keypoints-e-descritores","title":"Keypoints e Descritores","text":""},{"location":"unidades/unidade3/#keypoints-pontos-de-interesse","title":"Keypoints (Pontos de Interesse)","text":"<p>Keypoints s\u00e3o pontos espec\u00edficos em uma imagem que possuem propriedades distintivas, como cantos, bordas ou regi\u00f5es com alta varia\u00e7\u00e3o de intensidade. S\u00e3o invariantes a transforma\u00e7\u00f5es como rota\u00e7\u00e3o, escala e ilumina\u00e7\u00e3o.</p>"},{"location":"unidades/unidade3/#descritores","title":"Descritores","text":"<p>Descritores s\u00e3o vetores num\u00e9ricos que codificam informa\u00e7\u00f5es sobre a vizinhan\u00e7a de um keypoint, permitindo compara\u00e7\u00e3o entre diferentes keypoints.</p>"},{"location":"unidades/unidade3/#algoritmos-populares","title":"Algoritmos Populares","text":""},{"location":"unidades/unidade3/#harris-corner-detector","title":"Harris Corner Detector","text":"<p>Detecta cantos em imagens com base na varia\u00e7\u00e3o de intensidade em diferentes dire\u00e7\u00f5es:</p> <pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef detect_harris_corners(image, blockSize=2, ksize=3, k=0.04):\n    \"\"\"Detecta cantos usando o detector de Harris\"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY).astype(np.float32)\n\n    # Calcular derivadas\n    dx2 = cv2.cornerHarris(gray, blockSize, ksize, k)\n\n    # Dilatar para marcar cantos claramente\n    dx2 = cv2.dilate(dx2, None)\n\n    # Retornar imagem com cantos destacados\n    result = image.copy()\n    result[dx2 &gt; 0.01*dx2.max()] = [255, 0, 0]  # Marcar cantos em vermelho\n\n    return result, dx2\n\n# Exemplo de uso\nimg = cv2.imread('exemplo.jpg')\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\ncorners_img, corners_map = detect_harris_corners(img_rgb)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(img_rgb)\nplt.title('Imagem Original')\nplt.subplot(1, 2, 2)\nplt.imshow(corners_img)\nplt.title('Cantos Detectados (Harris)')\nplt.show()\n</code></pre>"},{"location":"unidades/unidade3/#shi-tomasi-corner-detector","title":"Shi-Tomasi Corner Detector","text":"<p>Melhoria do detector de Harris, selecionando cantos com maior precis\u00e3o:</p> <pre><code>def detect_shi_tomasi_corners(image, maxCorners=100, qualityLevel=0.01, minDistance=10):\n    \"\"\"Detecta cantos usando o detector Shi-Tomasi\"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n    corners = cv2.goodFeaturesToTrack(\n        gray, maxCorners, qualityLevel, minDistance\n    )\n\n    # Converter para int\n    corners = np.int0(corners)\n\n    # Desenhar cantos\n    result = image.copy()\n    for corner in corners:\n        x, y = corner.ravel()\n        cv2.circle(result, (x, y), 5, [255, 0, 0], -1)\n\n    return result, corners\n\n# Exemplo de uso\nshi_tomasi_img, shi_tomasi_corners = detect_shi_tomasi_corners(img_rgb)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(img_rgb)\nplt.title('Imagem Original')\nplt.subplot(1, 2, 2)\nplt.imshow(shi_tomasi_img)\nplt.title(f'Cantos Detectados (Shi-Tomasi) - {len(shi_tomasi_corners)} encontrados')\nplt.show()\n</code></pre>"},{"location":"unidades/unidade3/#sift-scale-invariant-feature-transform","title":"SIFT (Scale-Invariant Feature Transform)","text":"<p>Algoritmo robusto para detec\u00e7\u00e3o e descri\u00e7\u00e3o de keypoints invariantes a escala e rota\u00e7\u00e3o:</p> <pre><code>def detect_sift_features(image):\n    \"\"\"Detecta e descreve keypoints usando SIFT\"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n    # Criar detector SIFT\n    sift = cv2.SIFT_create()\n\n    # Detectar keypoints e calcular descritores\n    keypoints, descriptors = sift.detectAndCompute(gray, None)\n\n    # Desenhar keypoints\n    result = cv2.drawKeypoints(image, keypoints, None)\n\n    return result, keypoints, descriptors\n\n# Exemplo de uso (se o OpenCV tiver SIFT dispon\u00edvel)\ntry:\n    sift_img, sift_kp, sift_desc = detect_sift_features(img_rgb)\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(img_rgb)\n    plt.title('Imagem Original')\n    plt.subplot(1, 2, 2)\n    plt.imshow(sift_img)\n    plt.title(f'SIFT Features - {len(sift_kp) if sift_kp else 0} keypoints')\n    plt.show()\nexcept cv2.error:\n    print(\"SIFT n\u00e3o dispon\u00edvel nesta vers\u00e3o do OpenCV\")\n</code></pre>"},{"location":"unidades/unidade3/#orb-oriented-fast-and-rotated-brief","title":"ORB (Oriented FAST and Rotated BRIEF)","text":"<p>Vers\u00e3o mais r\u00e1pida e livre de patentes do SIFT:</p> <pre><code>def detect_orb_features(image):\n    \"\"\"Detecta e descreve keypoints usando ORB\"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n    # Criar detector ORB\n    orb = cv2.ORB_create()\n\n    # Detectar keypoints e calcular descritores\n    keypoints, descriptors = orb.detectAndCompute(gray, None)\n\n    # Desenhar keypoints\n    result = cv2.drawKeypoints(image, keypoints, None, color=(0,255,0), flags=0)\n\n    return result, keypoints, descriptors\n\n# Exemplo de uso\norb_img, orb_kp, orb_desc = detect_orb_features(img_rgb)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(img_rgb)\nplt.title('Imagem Original')\nplt.subplot(1, 2, 2)\nplt.imshow(orb_img)\nplt.title(f'ORB Features - {len(orb_kp) if orb_kp else 0} keypoints')\nplt.show()\n</code></pre>"},{"location":"unidades/unidade3/#vetorizacao-de-imagens","title":"Vetoriza\u00e7\u00e3o de Imagens","text":"<p>Converter imagens em vetores num\u00e9ricos \u00e9 essencial para aplicar algoritmos de machine learning. Existem v\u00e1rias abordagens para isso:</p>"},{"location":"unidades/unidade3/#vetorizacao-simples","title":"Vetoriza\u00e7\u00e3o Simples","text":"<p>Converter pixels diretamente em um vetor:</p> <pre><code>def flatten_image(image):\n    \"\"\"Converte imagem em vetor 1D\"\"\"\n    if len(image.shape) == 3:  # RGB\n        flattened = image.reshape(-1)\n    else:  # Grayscale\n        flattened = image.flatten()\n    return flattened\n\n# Exemplo\nsample_img = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\nflattened_vector = flatten_image(sample_img)\nprint(f\"Tamanho original: {sample_img.shape}\")\nprint(f\"Tamanho vetorializado: {flattened_vector.shape}\")\n</code></pre>"},{"location":"unidades/unidade3/#histogramas-como-vetores-de-caracteristicas","title":"Histogramas como Vetores de Caracter\u00edsticas","text":"<pre><code>def compute_color_histogram(image, bins=32):\n    \"\"\"Calcula histograma de cores como vetor de caracter\u00edsticas\"\"\"\n    # Converter para HSV para melhor representa\u00e7\u00e3o de cor\n    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n\n    # Calcular histograma para cada canal\n    hist_h = cv2.calcHist([hsv], [0], None, [bins], [0, 180])\n    hist_s = cv2.calcHist([hsv], [1], None, [bins], [0, 256])\n    hist_v = cv2.calcHist([hsv], [2], None, [bins], [0, 256])\n\n    # Concatenar histogramas\n    feature_vector = np.concatenate([hist_h.flatten(), hist_s.flatten(), hist_v.flatten()])\n\n    return feature_vector\n\n# Exemplo\nhist_features = compute_color_histogram(img_rgb)\nprint(f\"Vetor de caracter\u00edsticas (histograma): {hist_features.shape}\")\n</code></pre>"},{"location":"unidades/unidade3/#descritores-de-textura","title":"Descritores de Textura","text":"<pre><code>from skimage.feature import greycomatrix, greycoprops\n\ndef compute_texture_features(image):\n    \"\"\"Calcula descritores de textura GLCM\"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n    # Calcular matriz de co-ocorr\u00eancia de n\u00edveis de cinza\n    glcm = greycomatrix(gray, [1], [0, 45, 90, 135], levels=256, symmetric=True, normed=True)\n\n    # Extrair propriedades\n    contrast = greycoprops(glcm, 'contrast').flatten()\n    energy = greycoprops(glcm, 'energy').flatten()\n    homogeneity = greycoprops(glcm, 'homogeneity').flatten()\n    correlation = greycoprops(glcm, 'correlation').flatten()\n\n    # Concatenar todas as propriedades\n    texture_features = np.concatenate([contrast, energy, homogeneity, correlation])\n\n    return texture_features\n\n# Exemplo\ntexture_features = compute_texture_features(img_rgb)\nprint(f\"Vetor de caracter\u00edsticas (textura): {texture_features.shape}\")\n</code></pre>"},{"location":"unidades/unidade3/#classificacao-supervisionada","title":"Classifica\u00e7\u00e3o Supervisionada","text":""},{"location":"unidades/unidade3/#pipeline-de-classificacao","title":"Pipeline de Classifica\u00e7\u00e3o","text":"<ol> <li>Extra\u00e7\u00e3o de caracter\u00edsticas: Converter imagens em vetores</li> <li>Divis\u00e3o dos dados: Treino, valida\u00e7\u00e3o e teste</li> <li>Treinamento do modelo: Ajustar par\u00e2metros</li> <li>Avalia\u00e7\u00e3o: Medir desempenho</li> <li>Otimiza\u00e7\u00e3o: Ajustar hiperpar\u00e2metros</li> </ol>"},{"location":"unidades/unidade3/#exemplo-completo-de-classificacao","title":"Exemplo Completo de Classifica\u00e7\u00e3o","text":"<pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport cv2\nimport os\n\nclass ImageClassifier:\n    def __init__(self, classifier_type='random_forest'):\n        self.classifier_type = classifier_type\n        self.scaler = StandardScaler()\n\n        if classifier_type == 'random_forest':\n            self.classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n        elif classifier_type == 'svm':\n            self.classifier = SVC(kernel='rbf', random_state=42)\n\n    def extract_features(self, image):\n        \"\"\"Extrai caracter\u00edsticas de uma \u00fanica imagem\"\"\"\n        # Histograma de cores\n        color_hist = compute_color_histogram(image)\n\n        # Caracter\u00edsticas de textura\n        texture_features = compute_texture_features(image)\n\n        # Concatenar todas as caracter\u00edsticas\n        features = np.concatenate([color_hist, texture_features])\n\n        return features\n\n    def prepare_dataset(self, image_paths, labels):\n        \"\"\"Prepara o dataset completo\"\"\"\n        features_list = []\n\n        for img_path in image_paths:\n            img = cv2.imread(img_path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            features = self.extract_features(img)\n            features_list.append(features)\n\n        X = np.array(features_list)\n        y = np.array(labels)\n\n        return X, y\n\n    def train(self, X_train, y_train):\n        \"\"\"Treina o classificador\"\"\"\n        # Normalizar caracter\u00edsticas\n        X_train_scaled = self.scaler.fit_transform(X_train)\n\n        # Treinar modelo\n        self.classifier.fit(X_train_scaled, y_train)\n\n    def predict(self, X_test):\n        \"\"\"Faz previs\u00f5es\"\"\"\n        X_test_scaled = self.scaler.transform(X_test)\n        return self.classifier.predict(X_test_scaled)\n\n    def evaluate(self, X_test, y_test):\n        \"\"\"Avalia o modelo\"\"\"\n        predictions = self.predict(X_test)\n        accuracy = accuracy_score(y_test, predictions)\n        report = classification_report(y_test, predictions)\n\n        return accuracy, report\n\n# Exemplo de uso (com dados hipot\u00e9ticos)\n# Suponha que temos caminhos de imagens e r\u00f3tulos\n# image_paths = ['img1.jpg', 'img2.jpg', ...]\n# labels = ['classe_a', 'classe_b', ...]\n\n# classifier = ImageClassifier(classifier_type='random_forest')\n# X, y = classifier.prepare_dataset(image_paths, labels)\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# classifier.train(X_train, y_train)\n# accuracy, report = classifier.evaluate(X_test, y_test)\n# print(f\"Acur\u00e1cia: {accuracy}\")\n# print(report)\n</code></pre>"},{"location":"unidades/unidade3/#avaliacao-de-modelos","title":"Avalia\u00e7\u00e3o de Modelos","text":""},{"location":"unidades/unidade3/#metricas-comuns","title":"M\u00e9tricas Comuns","text":"<pre><code>from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\nimport seaborn as sns\n\ndef plot_confusion_matrix(y_true, y_pred, classes):\n    \"\"\"Plota matriz de confus\u00e3o\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=classes, yticklabels=classes)\n    plt.title('Matriz de Confus\u00e3o')\n    plt.ylabel('Verdadeiro')\n    plt.xlabel('Previsto')\n    plt.show()\n\n    # Calcular m\u00e9tricas\n    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n\n    print(f\"Precis\u00e3o: {precision:.3f}\")\n    print(f\"Recall: {recall:.3f}\")\n    print(f\"F1-Score: {f1:.3f}\")\n\n# Exemplo de uso\n# plot_confusion_matrix(y_test, predictions, classes=['Classe A', 'Classe B', 'Classe C'])\n</code></pre>"},{"location":"unidades/unidade3/#curva-roc-e-auc","title":"Curva ROC e AUC","text":"<p>Para problemas bin\u00e1rios:</p> <pre><code>from sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\ndef plot_roc_curves(y_true, y_scores, n_classes):\n    \"\"\"Plota curvas ROC para classifica\u00e7\u00e3o multiclasse\"\"\"\n    # Binarizar r\u00f3tulos\n    y_true_bin = label_binarize(y_true, classes=range(n_classes))\n\n    # Calcular curvas ROC para cada classe\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_scores[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Plotar curvas\n    plt.figure(figsize=(10, 8))\n    for i in range(n_classes):\n        plt.plot(fpr[i], tpr[i], label=f'Classe {i} (AUC = {roc_auc[i]:.2f})')\n\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('Taxa de Falsos Positivos')\n    plt.ylabel('Taxa de Verdadeiros Positivos')\n    plt.title('Curvas ROC')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n</code></pre>"},{"location":"unidades/unidade3/#tecnicas-avancadas-de-extracao-de-caracteristicas","title":"T\u00e9cnicas Avan\u00e7adas de Extra\u00e7\u00e3o de Caracter\u00edsticas","text":""},{"location":"unidades/unidade3/#bag-of-visual-words-bovw","title":"Bag of Visual Words (BOVW)","text":"<p>Similar ao Bag of Words em processamento de linguagem natural:</p> <pre><code>from sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\nimport numpy as np\n\nclass BagOfVisualWords:\n    def __init__(self, vocab_size=100, descriptor_extractor=None):\n        self.vocab_size = vocab_size\n        self.kmeans = KMeans(n_clusters=vocab_size, random_state=42)\n        self.descriptor_extractor = descriptor_extractor or self.default_descriptor_extractor\n\n    def default_descriptor_extractor(self, image):\n        \"\"\"Extrator de descritores padr\u00e3o (ORB)\"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        orb = cv2.ORB_create()\n        keypoints, descriptors = orb.detectAndCompute(gray, None)\n        return descriptors\n\n    def build_vocabulary(self, images):\n        \"\"\"Constr\u00f3i vocabul\u00e1rio a partir de descritores de m\u00faltiplas imagens\"\"\"\n        all_descriptors = []\n\n        for img in images:\n            descriptors = self.descriptor_extractor(img)\n            if descriptors is not None:\n                all_descriptors.append(descriptors)\n\n        # Concatenar todos os descritores\n        all_descriptors = np.vstack(all_descriptors)\n\n        # Treinar K-means para formar vocabul\u00e1rio\n        self.kmeans.fit(all_descriptors)\n\n    def encode_image(self, image):\n        \"\"\"Codifica uma imagem como histograma de palavras visuais\"\"\"\n        descriptors = self.descriptor_extractor(image)\n\n        if descriptors is None:\n            return np.zeros(self.vocab_size)\n\n        # Atribuir descritores aos clusters mais pr\u00f3ximos\n        distances = cdist(descriptors, self.kmeans.cluster_centers_)\n        assignments = np.argmin(distances, axis=1)\n\n        # Criar histograma\n        histogram = np.bincount(assignments, minlength=self.vocab_size)\n\n        # Normalizar\n        histogram = histogram.astype(float) / histogram.sum()\n\n        return histogram\n\n    def encode_dataset(self, images):\n        \"\"\"Codifica todas as imagens do dataset\"\"\"\n        histograms = []\n\n        for img in images:\n            hist = self.encode_image(img)\n            histograms.append(hist)\n\n        return np.array(histograms)\n\n# Exemplo de uso\n# bovw = BagOfVisualWords(vocab_size=200)\n# bovw.build_vocabulary(training_images)\n# train_histograms = bovw.encode_dataset(training_images)\n# test_histograms = bovw.encode_dataset(test_images)\n</code></pre>"},{"location":"unidades/unidade3/#hog-histogram-of-oriented-gradients","title":"HOG (Histogram of Oriented Gradients)","text":"<p>Eficiente para detec\u00e7\u00e3o de objetos:</p> <pre><code>from skimage.feature import hog\nfrom skimage import exposure\n\ndef extract_hog_features(image, visualize=False):\n    \"\"\"Extrai caracter\u00edsticas HOG de uma imagem\"\"\"\n    # Converter para escala de cinza\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n    # Calcular HOG\n    features, hog_image = hog(\n        gray,\n        orientations=9,\n        pixels_per_cell=(8, 8),\n        cells_per_block=(2, 2),\n        visualize=True,\n        block_norm='L2-Hys'\n    )\n\n    if visualize:\n        # Rescale histogram for better display\n        hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n        ax1.imshow(gray, cmap='gray')\n        ax1.set_title('Imagem Original')\n        ax1.axis('off')\n\n        ax2.imshow(hog_image_rescaled, cmap='gray')\n        ax2.set_title('HOG Features')\n        ax2.axis('off')\n\n        plt.show()\n\n    return features\n\n# Exemplo\nhog_features = extract_hog_features(img_rgb, visualize=True)\nprint(f\"Vetor de caracter\u00edsticas HOG: {hog_features.shape}\")\n</code></pre>"},{"location":"unidades/unidade3/#consideracoes-praticas","title":"Considera\u00e7\u00f5es Pr\u00e1ticas","text":""},{"location":"unidades/unidade3/#selecao-de-caracteristicas","title":"Sele\u00e7\u00e3o de Caracter\u00edsticas","text":"<ul> <li>Relev\u00e2ncia: Escolher caracter\u00edsticas que realmente contribuam para a tarefa</li> <li>Redund\u00e2ncia: Evitar caracter\u00edsticas altamente correlacionadas</li> <li>Dimensionalidade: Equilibrar quantidade de caracter\u00edsticas com desempenho</li> </ul>"},{"location":"unidades/unidade3/#preprocessamento","title":"Preprocessamento","text":"<ul> <li>Normaliza\u00e7\u00e3o: Padronizar caracter\u00edsticas para evitar dom\u00ednio de algumas sobre outras</li> <li>Redu\u00e7\u00e3o de dimensionalidade: Usar PCA ou LDA para simplificar o espa\u00e7o de caracter\u00edsticas</li> </ul>"},{"location":"unidades/unidade3/#validacao-cruzada","title":"Valida\u00e7\u00e3o Cruzada","text":"<p>Importante para estimar o desempenho real do modelo:</p> <pre><code>from sklearn.model_selection import cross_val_score\n\ndef evaluate_model_cv(model, X, y, cv=5):\n    \"\"\"Avalia modelo com valida\u00e7\u00e3o cruzada\"\"\"\n    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n\n    print(f\"Acur\u00e1cias na valida\u00e7\u00e3o cruzada: {scores}\")\n    print(f\"M\u00e9dia: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n\n    return scores\n\n# Exemplo\n# evaluate_model_cv(RandomForestClassifier(), X, y)\n</code></pre>"},{"location":"unidades/unidade3/#overfitting-e-regularizacao","title":"Overfitting e Regulariza\u00e7\u00e3o","text":"<p>O overfitting ocorre quando o modelo aprende demais os dados de treinamento, perdendo capacidade de generaliza\u00e7\u00e3o:</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator, X, y, title=\"Learning Curve\"):\n    \"\"\"Plota curva de aprendizado para diagnosticar overfitting\"\"\"\n    train_sizes, train_scores, val_scores = learning_curve(\n        estimator, X, y, cv=5, n_jobs=-1,\n        train_sizes=np.linspace(0.1, 1.0, 10),\n        scoring='accuracy'\n    )\n\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    val_mean = np.mean(val_scores, axis=1)\n    val_std = np.std(val_scores, axis=1)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Treino')\n    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n\n    plt.plot(train_sizes, val_mean, 'o-', color='red', label='Valida\u00e7\u00e3o')\n    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n\n    plt.title(title)\n    plt.xlabel('Tamanho do Conjunto de Treinamento')\n    plt.ylabel('Acur\u00e1cia')\n    plt.legend(loc='best')\n    plt.grid(True)\n    plt.show()\n\n# Exemplo\n# plot_learning_curve(RandomForestClassifier(), X, y)\n</code></pre> <p>Essas t\u00e9cnicas de extra\u00e7\u00e3o de caracter\u00edsticas e classifica\u00e7\u00e3o supervisionada formam a base para muitas aplica\u00e7\u00f5es de vis\u00e3o computacional, especialmente quando n\u00e3o h\u00e1 dados suficientes para usar deep learning ou quando \u00e9 necess\u00e1rio um modelo mais interpret\u00e1vel.</p>"},{"location":"unidades/unidade4/","title":"Unidade 4 - Deep Learning Aplicado","text":""},{"location":"unidades/unidade4/#introducao-as-redes-neurais-convolucionais-cnns","title":"Introdu\u00e7\u00e3o \u00e0s Redes Neurais Convolucionais (CNNs)","text":"<p>As Redes Neurais Convolucionais (Convolutional Neural Networks - CNNs) s\u00e3o uma classe especializada de redes neurais projetadas especificamente para processar dados com uma topologia de grade, como imagens. Elas t\u00eam sido extremamente bem-sucedidas em tarefas de vis\u00e3o computacional.</p>"},{"location":"unidades/unidade4/#estrutura-basica-de-uma-cnn","title":"Estrutura B\u00e1sica de uma CNN","text":"<p>Uma CNN t\u00edpica \u00e9 composta por:</p> <ol> <li>Camadas Convolucionais: Extraem caracter\u00edsticas da imagem</li> <li>Camadas de Pooling: Reduzem a dimensionalidade espacial</li> <li>Camadas totalmente conectadas (Dense): Realizam a classifica\u00e7\u00e3o final</li> <li>Fun\u00e7\u00f5es de ativa\u00e7\u00e3o: Introduzem n\u00e3o-linearidade</li> </ol>"},{"location":"unidades/unidade4/#como-funciona-uma-camada-convolucional","title":"Como Funciona uma Camada Convolucional","text":"<p>A convolu\u00e7\u00e3o \u00e9 uma opera\u00e7\u00e3o matem\u00e1tica que aplica um filtro (ou kernel) sobre uma imagem para detectar caracter\u00edsticas espec\u00edficas:</p> <pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Exemplo de camada convolucional\ndef visualize_conv_layer():\n    # Criar uma imagem de exemplo (1 imagem, 28x28 pixels, 1 canal)\n    sample_image = np.random.rand(1, 28, 28, 1).astype(np.float32)\n\n    # Criar camada convolucional\n    conv_layer = layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1))\n\n    # Aplicar convolu\u00e7\u00e3o\n    output = conv_layer(sample_image)\n\n    print(f\"Entrada: {sample_image.shape}\")\n    print(f\"Sa\u00edda: {output.shape}\")\n\n    return conv_layer\n\nconv_layer = visualize_conv_layer()\n</code></pre>"},{"location":"unidades/unidade4/#arquitetura-tipica-de-uma-cnn","title":"Arquitetura T\u00edpica de uma CNN","text":"<pre><code>def create_basic_cnn(input_shape, num_classes):\n    \"\"\"Cria uma CNN b\u00e1sica para classifica\u00e7\u00e3o\"\"\"\n    model = models.Sequential([\n        # Primeira camada convolucional\n        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n        layers.MaxPooling2D((2, 2)),\n\n        # Segunda camada convolucional\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n\n        # Terceira camada convolucional\n        layers.Conv2D(64, (3, 3), activation='relu'),\n\n        # Camadas densas para classifica\u00e7\u00e3o\n        layers.Flatten(),\n        layers.Dense(64, activation='relu'),\n        layers.Dropout(0.5),  # Regulariza\u00e7\u00e3o\n        layers.Dense(num_classes, activation='softmax')\n    ])\n\n    return model\n\n# Exemplo de uso\nmodel = create_basic_cnn((32, 32, 3), 10)  # CIFAR-10\nmodel.summary()\n</code></pre>"},{"location":"unidades/unidade4/#transfer-learning","title":"Transfer Learning","text":"<p>O Transfer Learning \u00e9 uma t\u00e9cnica poderosa que permite reutilizar um modelo pr\u00e9-treinado em uma nova tarefa, economizando tempo e recursos computacionais.</p>"},{"location":"unidades/unidade4/#por-que-usar-transfer-learning","title":"Por que usar Transfer Learning?","text":"<ul> <li>Efici\u00eancia: Treinar uma CNN do zero requer grandes conjuntos de dados e tempo computacional</li> <li>Performance: Modelos pr\u00e9-treinados j\u00e1 aprenderam caracter\u00edsticas gen\u00e9ricas de imagens</li> <li>Adaptabilidade: Ajustar um modelo existente para novas classes espec\u00edficas</li> </ul>"},{"location":"unidades/unidade4/#implementacao-de-transfer-learning","title":"Implementa\u00e7\u00e3o de Transfer Learning","text":"<pre><code>from tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\n\ndef create_transfer_model(num_classes, input_shape=(224, 224, 3)):\n    \"\"\"Cria modelo usando transfer learning com VGG16\"\"\"\n    # Carregar modelo pr\u00e9-treinado sem a cabe\u00e7a de classifica\u00e7\u00e3o\n    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n\n    # Congelar pesos do modelo base\n    base_model.trainable = False\n\n    # Adicionar camadas personalizadas\n    inputs = tf.keras.Input(shape=input_shape)\n    x = base_model(inputs, training=False)\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.2)(x)\n    outputs = Dense(num_classes, activation='softmax')(x)\n\n    model = Model(inputs, outputs)\n\n    return model, base_model\n\n# Exemplo de uso\ntransfer_model, base_model = create_transfer_model(num_classes=5)\ntransfer_model.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\ntransfer_model.summary()\n</code></pre>"},{"location":"unidades/unidade4/#fine-tuning","title":"Fine-tuning","text":"<p>Depois de treinar a cabe\u00e7a de classifica\u00e7\u00e3o personalizada, podemos desfrizar parte do modelo base e continuar o treinamento:</p> <pre><code>def fine_tune_model(model, base_model, train_dataset, validation_dataset, epochs=10):\n    \"\"\"Realiza fine-tuning do modelo\"\"\"\n    # Desfrizar parte do modelo base\n    base_model.trainable = True\n\n    # Congelar as primeiras camadas (menos espec\u00edficas)\n    fine_tune_at = len(base_model.layers) // 2\n    for layer in base_model.layers[:fine_tune_at]:\n        layer.trainable = False\n\n    # Compilar com learning rate menor\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(1e-5/10),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    # Treinar novamente\n    history_fine = model.fit(\n        train_dataset,\n        epochs=epochs,\n        validation_data=validation_dataset\n    )\n\n    return history_fine\n\n# Exemplo de uso (ap\u00f3s treinamento inicial)\n# history_fine = fine_tune_model(transfer_model, base_model, train_ds, val_ds, epochs=5)\n</code></pre>"},{"location":"unidades/unidade4/#deteccao-de-objetos-com-yolo","title":"Detec\u00e7\u00e3o de Objetos com YOLO","text":"<p>YOLO (You Only Look Once) \u00e9 uma arquitetura popular para detec\u00e7\u00e3o de objetos em tempo real.</p>"},{"location":"unidades/unidade4/#conceitos-de-deteccao-de-objetos","title":"Conceitos de Detec\u00e7\u00e3o de Objetos","text":"<p>Ao contr\u00e1rio da classifica\u00e7\u00e3o de imagens, a detec\u00e7\u00e3o de objetos envolve:</p> <ul> <li>Localiza\u00e7\u00e3o: Identificar onde os objetos est\u00e3o (bounding boxes)</li> <li>Classifica\u00e7\u00e3o: Determinar que tipo de objeto \u00e9</li> <li>Confian\u00e7a: Estimar a certeza da detec\u00e7\u00e3o</li> </ul>"},{"location":"unidades/unidade4/#estrutura-de-dados-para-deteccao","title":"Estrutura de Dados para Detec\u00e7\u00e3o","text":"<pre><code>import numpy as np\n\ndef create_detection_example():\n    \"\"\"Exemplo de estrutura de dados para detec\u00e7\u00e3o de objetos\"\"\"\n    # Suponha uma imagem 416x416 com 2 objetos detectados\n    detections = {\n        'boxes': np.array([  # [x_min, y_min, x_max, y_max]\n            [50, 60, 150, 200],   # Objeto 1\n            [200, 100, 300, 250]  # Objeto 2\n        ]),\n        'labels': np.array(['pessoa', 'carro']),\n        'scores': np.array([0.95, 0.87])  # Confian\u00e7a da detec\u00e7\u00e3o\n    }\n\n    return detections\n\ndetections = create_detection_example()\nprint(\"Exemplo de detec\u00e7\u00f5es:\")\nfor i in range(len(detections['boxes'])):\n    box = detections['boxes'][i]\n    label = detections['labels'][i]\n    score = detections['scores'][i]\n    print(f\"  {label}: {box} (confian\u00e7a: {score:.2f})\")\n</code></pre>"},{"location":"unidades/unidade4/#implementacao-basica-de-deteccao-com-tensorflowkeras","title":"Implementa\u00e7\u00e3o B\u00e1sica de Detec\u00e7\u00e3o com TensorFlow/Keras","text":"<pre><code>def create_object_detection_model(num_classes, input_shape=(416, 416, 3)):\n    \"\"\"Cria modelo b\u00e1sico para detec\u00e7\u00e3o de objetos\"\"\"\n    inputs = tf.keras.Input(shape=input_shape)\n\n    # Backbone CNN (feature extractor)\n    backbone = tf.keras.applications.MobileNetV2(\n        input_shape=input_shape, \n        include_top=False, \n        weights='imagenet'\n    )(inputs)\n\n    # Adicionar camadas para detec\u00e7\u00e3o\n    x = layers.GlobalAveragePooling2D()(backbone)\n    x = layers.Dense(1024, activation='relu')(x)\n\n    # Sa\u00edda para bounding boxes (4 coordenadas)\n    bbox_output = layers.Dense(4, name='bbox')(x)\n\n    # Sa\u00edda para classifica\u00e7\u00e3o\n    class_output = layers.Dense(num_classes, activation='softmax', name='classification')(x)\n\n    model = Model(inputs=inputs, outputs=[bbox_output, class_output])\n\n    return model\n\n# Exemplo de uso\ndetection_model = create_object_detection_model(num_classes=10)\ndetection_model.compile(\n    optimizer='adam',\n    loss={\n        'bbox': 'mse',\n        'classification': 'categorical_crossentropy'\n    },\n    metrics={\n        'classification': 'accuracy'\n    }\n)\n\ndetection_model.summary()\n</code></pre>"},{"location":"unidades/unidade4/#uso-de-modelos-pre-treinados-para-deteccao","title":"Uso de Modelos Pr\u00e9-Treinados para Detec\u00e7\u00e3o","text":"<pre><code>import cv2\nimport numpy as np\n\ndef load_pretrained_yolo():\n    \"\"\"Carrega modelo YOLO pr\u00e9-treinado (exemplo conceitual)\"\"\"\n    # Na pr\u00e1tica, usaria uma biblioteca como ultralytics/yolov5 ou tensorflow-models\n    print(\"Carregando modelo YOLO pr\u00e9-treinado...\")\n\n    # Exemplo com OpenCV DNN (modelo YOLO carregado como Darknet)\n    # net = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')\n    # layer_names = net.getLayerNames()\n    # output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n\n    # Para este exemplo, retornaremos um placeholder\n    class YOLODetector:\n        def detect(self, image):\n            # Simular detec\u00e7\u00e3o\n            height, width = image.shape[:2]\n\n            # Gerar detec\u00e7\u00f5es simuladas\n            boxes = [\n                [int(0.1 * width), int(0.1 * height), int(0.3 * width), int(0.3 * height)],\n                [int(0.6 * width), int(0.4 * height), int(0.8 * width), int(0.7 * height)]\n            ]\n            confidences = [0.9, 0.75]\n            class_ids = [0, 1]  # Ex: pessoa, carro\n\n            return boxes, confidences, class_ids\n\n    return YOLODetector()\n\ndef draw_detections(image, boxes, confidences, class_ids, classes):\n    \"\"\"Desenha detec\u00e7\u00f5es na imagem\"\"\"\n    colors = np.random.uniform(0, 255, size=(len(classes), 3))\n\n    for i in range(len(boxes)):\n        x, y, w, h = boxes[i]\n        label = f\"{classes[class_ids[i]]}: {confidences[i]:.2f}\"\n        color = colors[class_ids[i]]\n\n        cv2.rectangle(image, (x, y), (x+w, y+h), color, 2)\n        cv2.putText(image, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    return image\n\n# Exemplo de uso\ndetector = load_pretrained_yolo()\nclasses = ['pessoa', 'carro', 'bicicleta', 'cachorro', 'gato']\n\n# Simular imagem\nsample_img = np.random.randint(0, 255, (416, 416, 3), dtype=np.uint8)\n\n# Detectar objetos\nboxes, confidences, class_ids = detector.detect(sample_img)\n\n# Desenhar detec\u00e7\u00f5es\nresult_img = draw_detections(sample_img.copy(), boxes, confidences, class_ids, classes)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(sample_img)\nplt.title('Imagem Original')\nplt.subplot(1, 2, 2)\nplt.imshow(result_img)\nplt.title('Detec\u00e7\u00f5es')\nplt.show()\n</code></pre>"},{"location":"unidades/unidade4/#otimizacao-e-deploy","title":"Otimiza\u00e7\u00e3o e Deploy","text":""},{"location":"unidades/unidade4/#quantizacao-de-modelos","title":"Quantiza\u00e7\u00e3o de Modelos","text":"<p>A quantiza\u00e7\u00e3o reduz o tamanho e melhora a velocidade de execu\u00e7\u00e3o de modelos, especialmente importante para deploy em dispositivos com recursos limitados:</p> <pre><code>def quantize_model(saved_model_dir):\n    \"\"\"Converte modelo para vers\u00e3o quantizada\"\"\"\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n\n    # Quantiza\u00e7\u00e3o inteira\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n    # Converter para TensorFlow Lite\n    quantized_model = converter.convert()\n\n    return quantized_model\n\ndef save_quantized_model(quantized_model, filename):\n    \"\"\"Salva modelo quantizado\"\"\"\n    with open(filename, 'wb') as f:\n        f.write(quantized_model)\n\n# Exemplo de uso (ap\u00f3s treinamento do modelo)\n# quantized_tflite_model = quantize_model('saved_model_dir')\n# save_quantized_model(quantized_tflite_model, 'model_quantized.tflite')\n</code></pre>"},{"location":"unidades/unidade4/#exportacao-para-tensorflow-lite","title":"Exporta\u00e7\u00e3o para TensorFlow Lite","text":"<p>TensorFlow Lite \u00e9 otimizado para infer\u00eancia em dispositivos m\u00f3veis e embarcados:</p> <pre><code>def export_to_tflite(model, filename):\n    \"\"\"Exporta modelo para TensorFlow Lite\"\"\"\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n    # Opcional: habilitar otimiza\u00e7\u00f5es\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n    tflite_model = converter.convert()\n\n    # Salvar modelo\n    with open(filename, 'wb') as f:\n        f.write(tflite_model)\n\n    print(f\"Modelo TFLite salvo como {filename}\")\n\n# Exemplo de uso\n# export_to_tflite(transfer_model, 'my_model.tflite')\n</code></pre>"},{"location":"unidades/unidade4/#inferencia-com-tensorflow-lite","title":"Infer\u00eancia com TensorFlow Lite","text":"<pre><code>def run_tflite_inference(interpreter, input_data):\n    \"\"\"Executa infer\u00eancia com modelo TFLite\"\"\"\n    # Obter entrada e sa\u00edda do modelo\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Preparar entrada\n    input_shape = input_details[0]['shape']\n    input_data = input_data.reshape(input_shape).astype(np.float32)\n\n    # Definir entrada\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Executar infer\u00eancia\n    interpreter.invoke()\n\n    # Obter sa\u00edda\n    output_data = interpreter.get_tensor(output_details[0]['index'])\n\n    return output_data\n\n# Exemplo de uso (ap\u00f3s carregar modelo TFLite)\n# interpreter = tf.lite.Interpreter(model_path=\"model_quantized.tflite\")\n# interpreter.allocate_tensors()\n# prediction = run_tflite_inference(interpreter, sample_input)\n</code></pre>"},{"location":"unidades/unidade4/#avaliacao-de-modelos-de-deep-learning","title":"Avalia\u00e7\u00e3o de Modelos de Deep Learning","text":""},{"location":"unidades/unidade4/#metricas-para-classificacao","title":"M\u00e9tricas para Classifica\u00e7\u00e3o","text":"<pre><code>from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\ndef evaluate_classification_model(model, test_dataset):\n    \"\"\"Avalia modelo de classifica\u00e7\u00e3o\"\"\"\n    # Obter previs\u00f5es\n    predictions = model.predict(test_dataset)\n    predicted_classes = np.argmax(predictions, axis=1)\n\n    # Obter r\u00f3tulos verdadeiros\n    true_classes = []\n    for _, labels in test_dataset:\n        true_classes.extend(labels.numpy())\n    true_classes = np.array(true_classes)\n\n    # Calcular m\u00e9tricas\n    report = classification_report(true_classes, predicted_classes)\n    cm = confusion_matrix(true_classes, predicted_classes)\n\n    print(\"Relat\u00f3rio de Classifica\u00e7\u00e3o:\")\n    print(report)\n\n    # Plotar matriz de confus\u00e3o\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title('Matriz de Confus\u00e3o')\n    plt.ylabel('Verdadeiro')\n    plt.xlabel('Previsto')\n    plt.show()\n\n    return report, cm\n\n# Exemplo de uso\n# report, cm = evaluate_classification_model(transfer_model, test_dataset)\n</code></pre>"},{"location":"unidades/unidade4/#metricas-para-deteccao-de-objetos","title":"M\u00e9tricas para Detec\u00e7\u00e3o de Objetos","text":"<pre><code>def calculate_iou(box1, box2):\n    \"\"\"Calcula Intersection over Union entre duas bounding boxes\"\"\"\n    # Coordenadas: [x_min, y_min, x_max, y_max]\n    x1_min, y1_min, x1_max, y1_max = box1\n    x2_min, y2_min, x2_max, y2_max = box2\n\n    # Calcular interse\u00e7\u00e3o\n    inter_x_min = max(x1_min, x2_min)\n    inter_y_min = max(y1_min, y2_min)\n    inter_x_max = min(x1_max, x2_max)\n    inter_y_max = min(y1_max, y2_max)\n\n    # \u00c1rea de interse\u00e7\u00e3o\n    inter_width = max(0, inter_x_max - inter_x_min)\n    inter_height = max(0, inter_y_max - inter_y_min)\n    intersection_area = inter_width * inter_height\n\n    # \u00c1reas das bounding boxes\n    box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n    box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n\n    # Uni\u00e3o\n    union_area = box1_area + box2_area - intersection_area\n\n    # IoU\n    iou = intersection_area / union_area if union_area != 0 else 0\n\n    return iou\n\ndef evaluate_object_detection(predictions, ground_truth, iou_threshold=0.5):\n    \"\"\"Avalia modelo de detec\u00e7\u00e3o de objetos\"\"\"\n    tp = 0  # True Positives\n    fp = 0  # False Positives\n    fn = 0  # False Negatives\n\n    for pred_box, pred_label in predictions:\n        matched = False\n        for gt_box, gt_label in ground_truth:\n            if pred_label == gt_label:\n                iou = calculate_iou(pred_box, gt_box)\n                if iou &gt;= iou_threshold:\n                    tp += 1\n                    matched = True\n                    break\n\n        if not matched:\n            fp += 1\n\n    # Calcular FN\n    fn = len(ground_truth) - tp\n\n    # Calcular m\u00e9tricas\n    precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0\n\n    print(f\"Precision: {precision:.3f}\")\n    print(f\"Recall: {recall:.3f}\")\n    print(f\"F1-Score: {f1_score:.3f}\")\n\n    return precision, recall, f1_score\n\n# Exemplo de uso\n# predictions = [([50, 60, 150, 200], 'pessoa'), ([200, 100, 300, 250], 'carro')]\n# ground_truth = [([55, 65, 145, 195], 'pessoa'), ([195, 95, 305, 255], 'carro')]\n# precision, recall, f1 = evaluate_object_detection(predictions, ground_truth)\n</code></pre>"},{"location":"unidades/unidade4/#consideracoes-praticas","title":"Considera\u00e7\u00f5es Pr\u00e1ticas","text":""},{"location":"unidades/unidade4/#escolha-de-arquitetura","title":"Escolha de Arquitetura","text":"<ul> <li>VGG: Simples mas pesado, bom para aprendizado</li> <li>ResNet: Excelente desempenho, permite redes muito profundas</li> <li>MobileNet: Otimizado para dispositivos m\u00f3veis</li> <li>EfficientNet: Bom equil\u00edbrio entre acur\u00e1cia e efici\u00eancia</li> </ul>"},{"location":"unidades/unidade4/#data-augmentation","title":"Data Augmentation","text":"<p>T\u00e9cnica importante para aumentar o conjunto de treinamento e melhorar a generaliza\u00e7\u00e3o:</p> <pre><code>def create_data_augmentation():\n    \"\"\"Cria camadas de augmentation\"\"\"\n    data_augmentation = tf.keras.Sequential([\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.1),\n        layers.RandomZoom(0.1),\n        layers.RandomContrast(0.1),\n        layers.RandomBrightness(0.1)\n    ])\n\n    return data_augmentation\n\n# Exemplo de uso\naugmentation_layer = create_data_augmentation()\n\n# Aplicar durante treinamento\n# model = tf.keras.Sequential([\n#     augmentation_layer,\n#     base_model,\n#     # ... outras camadas\n# ])\n</code></pre>"},{"location":"unidades/unidade4/#estrategias-de-treinamento","title":"Estrat\u00e9gias de Treinamento","text":"<ul> <li>Learning Rate Scheduling: Ajustar taxa de aprendizado durante o treinamento</li> <li>Early Stopping: Parar treinamento quando o desempenho estagnar</li> <li>Checkpointing: Salvar melhores vers\u00f5es do modelo durante treinamento</li> </ul> <pre><code>def setup_training_callbacks():\n    \"\"\"Configura callbacks para treinamento\"\"\"\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=10,\n            restore_best_weights=True\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=5,\n            min_lr=1e-7\n        ),\n        tf.keras.callbacks.ModelCheckpoint(\n            'best_model.h5',\n            monitor='val_accuracy',\n            save_best_only=True,\n            verbose=1\n        )\n    ]\n\n    return callbacks\n\n# Exemplo de uso\n# callbacks = setup_training_callbacks()\n# model.fit(train_ds, validation_data=val_ds, epochs=100, callbacks=callbacks)\n</code></pre> <p>O Deep Learning revolucionou a vis\u00e3o computacional, permitindo solu\u00e7\u00f5es mais precisas e robustas para uma ampla variedade de tarefas. O uso de t\u00e9cnicas como transfer learning torna poss\u00edvel aplicar essas tecnologias mesmo com conjuntos de dados menores.</p>"},{"location":"unidades/unidade5/","title":"Unidade 5 - Integra\u00e7\u00e3o e Engenharia","text":""},{"location":"unidades/unidade5/#construcao-de-apis-para-modelos-de-visao-computacional","title":"Constru\u00e7\u00e3o de APIs para Modelos de Vis\u00e3o Computacional","text":"<p>A integra\u00e7\u00e3o de modelos de vis\u00e3o computacional em sistemas reais geralmente envolve a cria\u00e7\u00e3o de APIs (Application Programming Interfaces) que permitem que outros sistemas consumam os servi\u00e7os de an\u00e1lise visual.</p>"},{"location":"unidades/unidade5/#escolha-de-framework-para-api","title":"Escolha de Framework para API","text":"<p>Duas op\u00e7\u00f5es populares para criar APIs em Python s\u00e3o:</p> <ul> <li>FastAPI: Moderno, r\u00e1pido e com suporte a tipagem</li> <li>Flask: Leve e flex\u00edvel, ideal para prototipagem</li> </ul>"},{"location":"unidades/unidade5/#estrutura-de-uma-api-para-visao-computacional","title":"Estrutura de uma API para Vis\u00e3o Computacional","text":"<p>Uma API bem estruturada para vis\u00e3o computacional deve seguir princ\u00edpios de engenharia de software:</p> <pre><code>api_vision/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py              # Ponto de entrada da API\n\u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 routes/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 vision.py    # Rotas para vis\u00e3o computacional\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 health.py    # Rotas de sa\u00fade do sistema\n\u2502   \u2502   \u2514\u2500\u2500 models/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 request.py   # Modelos de requisi\u00e7\u00e3o\n\u2502   \u2502       \u2514\u2500\u2500 response.py  # Modelos de resposta\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 vision_service.py  # L\u00f3gica de neg\u00f3cio\n\u2502   \u2502   \u2514\u2500\u2500 model_loader.py    # Carregamento de modelos\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 image_utils.py     # Fun\u00e7\u00f5es auxiliares para imagens\n\u2502   \u2502   \u2514\u2500\u2500 validation.py      # Fun\u00e7\u00f5es de valida\u00e7\u00e3o\n\u2502   \u2514\u2500\u2500 config/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 settings.py        # Configura\u00e7\u00f5es do sistema\n\u251c\u2500\u2500 models/                   # Diret\u00f3rio para modelos treinados\n\u251c\u2500\u2500 tests/                    # Testes da API\n\u251c\u2500\u2500 requirements.txt          # Depend\u00eancias\n\u2514\u2500\u2500 Dockerfile               # Para containeriza\u00e7\u00e3o\n</code></pre>"},{"location":"unidades/unidade5/#exemplo-de-api-com-fastapi","title":"Exemplo de API com FastAPI","text":"<pre><code># app/main.py\nfrom fastapi import FastAPI, File, UploadFile, HTTPException\nfrom fastapi.responses import JSONResponse\nfrom PIL import Image\nimport io\nimport numpy as np\nimport cv2\nfrom typing import Optional\n\n# Importar servi\u00e7os\nfrom app.services.vision_service import VisionService\nfrom app.utils.image_utils import validate_image\nfrom app.api.models.request import ClassificationRequest\nfrom app.api.models.response import ClassificationResponse\n\napp = FastAPI(\n    title=\"API de Vis\u00e3o Computacional\",\n    description=\"Servi\u00e7o para classifica\u00e7\u00e3o e detec\u00e7\u00e3o de objetos em imagens\",\n    version=\"1.0.0\"\n)\n\n# Instanciar servi\u00e7o de vis\u00e3o computacional\nvision_service = VisionService()\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"API de Vis\u00e3o Computacional est\u00e1 ativa!\"}\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Verifica se o servi\u00e7o est\u00e1 ativo e saud\u00e1vel\"\"\"\n    return {\"status\": \"healthy\"}\n\n@app.post(\"/classify\", response_model=ClassificationResponse)\nasync def classify_image(file: UploadFile = File(...)):\n    \"\"\"\n    Classifica uma imagem usando modelo de vis\u00e3o computacional\n    \"\"\"\n    try:\n        # Validar tipo de arquivo\n        if not validate_image(file.content_type):\n            raise HTTPException(status_code=400, detail=\"Tipo de arquivo inv\u00e1lido\")\n\n        # Ler imagem\n        contents = await file.read()\n        image = Image.open(io.BytesIO(contents))\n\n        # Converter para formato necess\u00e1rio\n        image_array = np.array(image)\n\n        # Processar imagem e obter classifica\u00e7\u00e3o\n        result = vision_service.classify_image(image_array)\n\n        return ClassificationResponse(\n            success=True,\n            prediction=result['prediction'],\n            confidence=result['confidence'],\n            processing_time=result['processing_time']\n        )\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/detect_objects\")\nasync def detect_objects(file: UploadFile = File(...)):\n    \"\"\"\n    Detecta objetos em uma imagem\n    \"\"\"\n    try:\n        if not validate_image(file.content_type):\n            raise HTTPException(status_code=400, detail=\"Tipo de arquivo inv\u00e1lido\")\n\n        contents = await file.read()\n        image = Image.open(io.BytesIO(contents))\n        image_array = np.array(image)\n\n        # Detectar objetos\n        detections = vision_service.detect_objects(image_array)\n\n        return JSONResponse(content={\n            \"success\": True,\n            \"detections\": detections,\n            \"count\": len(detections)\n        })\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Rodar com: uvicorn app.main:app --reload\n</code></pre>"},{"location":"unidades/unidade5/#modelos-de-requisicao-e-resposta","title":"Modelos de Requisi\u00e7\u00e3o e Resposta","text":"<pre><code># app/api/models/request.py\nfrom pydantic import BaseModel\nfrom typing import Optional\n\nclass ClassificationRequest(BaseModel):\n    image_url: Optional[str] = None\n    image_base64: Optional[str] = None\n\n# app/api/models/response.py\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any\nimport time\n\nclass ClassificationResponse(BaseModel):\n    success: bool\n    prediction: str\n    confidence: float\n    processing_time: float\n    timestamp: str = str(time.time())\n\nclass DetectionResponse(BaseModel):\n    success: bool\n    detections: List[Dict[str, Any]]\n    count: int\n    processing_time: float\n</code></pre>"},{"location":"unidades/unidade5/#servico-de-visao-computacional","title":"Servi\u00e7o de Vis\u00e3o Computacional","text":"<pre><code># app/services/vision_service.py\nimport time\nimport numpy as np\nfrom typing import Dict, Any, List\nimport tensorflow as tf\nfrom PIL import Image\n\nclass VisionService:\n    def __init__(self):\n        \"\"\"Inicializa o servi\u00e7o carregando modelos necess\u00e1rios\"\"\"\n        self.model = self.load_model()\n        self.classes = self.load_class_names()\n\n    def load_model(self):\n        \"\"\"Carrega modelo treinado\"\"\"\n        # Substituir pelo caminho real do modelo\n        try:\n            model = tf.keras.models.load_model('models/classifier.h5')\n            return model\n        except:\n            # Modelo mock para demonstra\u00e7\u00e3o\n            print(\"Usando modelo mock - substitua pelo modelo real\")\n            return None\n\n    def load_class_names(self):\n        \"\"\"Carrega nomes das classes\"\"\"\n        # Substituir pela lista real de classes\n        return ['classe_a', 'classe_b', 'classe_c']\n\n    def preprocess_image(self, image_array):\n        \"\"\"Pr\u00e9-processa imagem para entrada do modelo\"\"\"\n        # Redimensionar imagem\n        image = Image.fromarray(image_array)\n        image = image.resize((224, 224))\n        image_array = np.array(image)\n\n        # Normalizar\n        image_array = image_array.astype(np.float32) / 255.0\n\n        # Expandir dimens\u00e3o para batch\n        image_array = np.expand_dims(image_array, axis=0)\n\n        return image_array\n\n    def classify_image(self, image_array):\n        \"\"\"Classifica uma imagem\"\"\"\n        start_time = time.time()\n\n        # Pr\u00e9-processar imagem\n        processed_image = self.preprocess_image(image_array)\n\n        # Fazer predi\u00e7\u00e3o (mock se modelo n\u00e3o estiver carregado)\n        if self.model is not None:\n            predictions = self.model.predict(processed_image)\n            predicted_class_idx = np.argmax(predictions[0])\n            confidence = float(predictions[0][predicted_class_idx])\n            predicted_class = self.classes[predicted_class_idx]\n        else:\n            # Simular predi\u00e7\u00e3o\n            predicted_class = \"classe_a\"\n            confidence = 0.85\n\n        processing_time = time.time() - start_time\n\n        return {\n            'prediction': predicted_class,\n            'confidence': confidence,\n            'processing_time': processing_time\n        }\n\n    def detect_objects(self, image_array):\n        \"\"\"Detecta objetos em uma imagem\"\"\"\n        start_time = time.time()\n\n        # Converter para OpenCV\n        image_cv = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n\n        # Simular detec\u00e7\u00f5es (substituir por modelo real de detec\u00e7\u00e3o)\n        height, width = image_array.shape[:2]\n        detections = [\n            {\n                \"label\": \"objeto\",\n                \"confidence\": 0.92,\n                \"bbox\": [int(0.1 * width), int(0.1 * height), \n                         int(0.3 * width), int(0.3 * height)],\n                \"coordinates\": {\n                    \"x_min\": int(0.1 * width),\n                    \"y_min\": int(0.1 * height),\n                    \"x_max\": int(0.3 * width),\n                    \"y_max\": int(0.3 * height)\n                }\n            }\n        ]\n\n        processing_time = time.time() - start_time\n\n        return {\n            'detections': detections,\n            'processing_time': processing_time,\n            'count': len(detections)\n        }\n</code></pre>"},{"location":"unidades/unidade5/#utilitarios","title":"Utilit\u00e1rios","text":"<pre><code># app/utils/image_utils.py\nfrom typing import Optional\n\ndef validate_image(content_type: str) -&gt; bool:\n    \"\"\"Valida se o tipo de conte\u00fado \u00e9 uma imagem suportada\"\"\"\n    valid_types = [\n        'image/jpeg',\n        'image/jpg', \n        'image/png',\n        'image/gif',\n        'image/webp'\n    ]\n    return content_type.lower() in valid_types\n\ndef resize_image(image_array, target_size=(224, 224)):\n    \"\"\"Redimensiona imagem mantendo propor\u00e7\u00e3o\"\"\"\n    from PIL import Image\n\n    image = Image.fromarray(image_array)\n    image = image.resize(target_size, Image.ANTIALIAS)\n    return np.array(image)\n\ndef normalize_image(image_array):\n    \"\"\"Normaliza imagem para valores entre 0 e 1\"\"\"\n    return image_array.astype(np.float32) / 255.0\n</code></pre>"},{"location":"unidades/unidade5/#estrutura-arquitetural","title":"Estrutura Arquitetural","text":""},{"location":"unidades/unidade5/#padroes-de-arquitetura","title":"Padr\u00f5es de Arquitetura","text":"<p>Para sistemas de vis\u00e3o computacional, \u00e9 importante seguir padr\u00f5es arquiteturais que promovam:</p> <ul> <li>Separation of Concerns: Separar diferentes responsabilidades</li> <li>Scalability: Permitir expans\u00e3o horizontal</li> <li>Maintainability: Facilitar manuten\u00e7\u00e3o e evolu\u00e7\u00e3o</li> <li>Testability: Permitir testes unit\u00e1rios e de integra\u00e7\u00e3o</li> </ul>"},{"location":"unidades/unidade5/#arquitetura-em-camadas","title":"Arquitetura em Camadas","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Presentation  \u2502  \u2190 API REST, interfaces web\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Application   \u2502  \u2190 L\u00f3gica de neg\u00f3cio, regras de neg\u00f3cio\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    Domain       \u2502  \u2190 Modelos de dom\u00ednio, entidades\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Infrastructure\u2502  \u2190 Persist\u00eancia, servi\u00e7os externos\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"unidades/unidade5/#padrao-repository","title":"Padr\u00e3o Repository","text":"<pre><code># app/repositories/model_repository.py\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\nimport pickle\nimport joblib\n\nclass ModelRepository(ABC):\n    @abstractmethod\n    def save_model(self, model, model_path: str):\n        pass\n\n    @abstractmethod\n    def load_model(self, model_path: str):\n        pass\n\n    @abstractmethod\n    def get_model_info(self, model_path: str) -&gt; dict:\n        pass\n\nclass LocalModelRepository(ModelRepository):\n    def save_model(self, model, model_path: str):\n        \"\"\"Salva modelo localmente\"\"\"\n        if hasattr(model, 'save'):\n            model.save(model_path)\n        else:\n            joblib.dump(model, model_path)\n\n    def load_model(self, model_path: str):\n        \"\"\"Carrega modelo localmente\"\"\"\n        try:\n            # Tentar carregar como Keras model primeiro\n            import tensorflow as tf\n            return tf.keras.models.load_model(model_path)\n        except:\n            # Caso contr\u00e1rio, usar joblib\n            return joblib.load(model_path)\n\n    def get_model_info(self, model_path: str) -&gt; dict:\n        \"\"\"Obt\u00e9m informa\u00e7\u00f5es sobre o modelo\"\"\"\n        import os\n        stat = os.stat(model_path)\n        return {\n            'size_mb': stat.st_size / (1024 * 1024),\n            'modified': stat.st_mtime\n        }\n</code></pre>"},{"location":"unidades/unidade5/#injecao-de-dependencia","title":"Inje\u00e7\u00e3o de Depend\u00eancia","text":"<pre><code># app/dependencies.py\nfrom app.services.vision_service import VisionService\nfrom app.repositories.model_repository import LocalModelRepository\n\ndef get_vision_service():\n    \"\"\"Dependency injection para VisionService\"\"\"\n    model_repo = LocalModelRepository()\n    # Aqui poderia injetar o reposit\u00f3rio no servi\u00e7o\n    return VisionService()\n\n# Em rotas:\n# vision_service = Depends(get_vision_service)\n</code></pre>"},{"location":"unidades/unidade5/#containerizacao","title":"Containeriza\u00e7\u00e3o","text":""},{"location":"unidades/unidade5/#dockerfile-para-api-de-visao-computacional","title":"Dockerfile para API de Vis\u00e3o Computacional","text":"<pre><code># Dockerfile\nFROM python:3.9-slim\n\n# Definir diret\u00f3rio de trabalho\nWORKDIR /app\n\n# Copiar depend\u00eancias\nCOPY requirements.txt .\n\n# Instalar depend\u00eancias\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copiar c\u00f3digo da aplica\u00e7\u00e3o\nCOPY . .\n\n# Criar diret\u00f3rio para modelos\nRUN mkdir -p models\n\n# Expor porta\nEXPOSE 8000\n\n# Comando para rodar a aplica\u00e7\u00e3o\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"unidades/unidade5/#arquivo-de-requisitos","title":"Arquivo de Requisitos","text":"<pre><code># requirements.txt\nfastapi==0.104.1\nuvicorn[standard]==0.24.0\ntensorflow==2.15.0\nopencv-python==4.8.1.78\npillow==10.1.0\nnumpy==1.24.3\npydantic==2.5.0\njoblib==1.3.2\npython-multipart==0.0.6\n</code></pre>"},{"location":"unidades/unidade5/#docker-compose-para-ambiente-completo","title":"Docker Compose para Ambiente Completo","text":"<pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./models:/app/models\n      - ./logs:/app/logs\n    environment:\n      - MODEL_PATH=/app/models/classifier.h5\n      - LOG_LEVEL=info\n    depends_on:\n      - redis\n    restart: unless-stopped\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_data:/data\n    restart: unless-stopped\n\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n      - ./ssl:/etc/nginx/ssl\n    depends_on:\n      - api\n    restart: unless-stopped\n\nvolumes:\n  redis_data:\n</code></pre>"},{"location":"unidades/unidade5/#configuracao-de-nginx-para-proxy-reverso","title":"Configura\u00e7\u00e3o de Nginx para Proxy Reverso","text":"<pre><code># nginx.conf\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream api_server {\n        server api:8000;\n    }\n\n    server {\n        listen 80;\n\n        location / {\n            proxy_pass http://api_server;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n\n            # Configura\u00e7\u00f5es para uploads grandes\n            client_max_body_size 10M;\n        }\n\n        # Health check\n        location /health {\n            access_log off;\n            return 200 \"healthy\\n\";\n            add_header Content-Type text/plain;\n        }\n    }\n}\n</code></pre>"},{"location":"unidades/unidade5/#boas-praticas-e-organizacao","title":"Boas Pr\u00e1ticas e Organiza\u00e7\u00e3o","text":""},{"location":"unidades/unidade5/#versionamento-de-modelos","title":"Versionamento de Modelos","text":"<p>\u00c9 crucial manter controle de vers\u00e3o dos modelos de ML:</p> <pre><code># app/utils/model_versioning.py\nimport os\nimport json\nfrom datetime import datetime\nfrom typing import Dict, Any\n\nclass ModelVersionManager:\n    def __init__(self, models_dir: str = \"models\"):\n        self.models_dir = models_dir\n        self.version_file = os.path.join(models_dir, \"versions.json\")\n\n    def register_model(self, model_name: str, model_path: str, metadata: Dict[str, Any]):\n        \"\"\"Registra novo modelo com metadados\"\"\"\n        versions = self._load_versions()\n\n        version_info = {\n            \"path\": model_path,\n            \"timestamp\": datetime.now().isoformat(),\n            \"metadata\": metadata,\n            \"active\": True\n        }\n\n        if model_name not in versions:\n            versions[model_name] = {}\n\n        # Criar nova vers\u00e3o\n        version_id = f\"v{len(versions[model_name]) + 1:03d}\"\n        versions[model_name][version_id] = version_info\n\n        # Desativar vers\u00f5es anteriores\n        for version_key in versions[model_name]:\n            if version_key != version_id:\n                versions[model_name][version_key][\"active\"] = False\n\n        self._save_versions(versions)\n        return version_id\n\n    def get_active_model(self, model_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Obt\u00e9m o modelo ativo\"\"\"\n        versions = self._load_versions()\n\n        if model_name not in versions:\n            raise ValueError(f\"Modelo {model_name} n\u00e3o encontrado\")\n\n        for version_id, info in versions[model_name].items():\n            if info[\"active\"]:\n                return {**info, \"version\": version_id, \"name\": model_name}\n\n        raise ValueError(f\"Nenhuma vers\u00e3o ativa encontrada para {model_name}\")\n\n    def _load_versions(self) -&gt; Dict[str, Any]:\n        \"\"\"Carrega arquivo de vers\u00f5es\"\"\"\n        if os.path.exists(self.version_file):\n            with open(self.version_file, 'r') as f:\n                return json.load(f)\n        return {}\n\n    def _save_versions(self, versions: Dict[str, Any]):\n        \"\"\"Salva arquivo de vers\u00f5es\"\"\"\n        os.makedirs(self.models_dir, exist_ok=True)\n        with open(self.version_file, 'w') as f:\n            json.dump(versions, f, indent=2)\n\n# Exemplo de uso\n# manager = ModelVersionManager()\n# version = manager.register_model(\n#     \"classifier\", \n#     \"models/classifier_v001.h5\",\n#     {\"accuracy\": 0.92, \"dataset\": \"custom_dataset_v2\"}\n# )\n</code></pre>"},{"location":"unidades/unidade5/#logging-e-monitoramento","title":"Logging e Monitoramento","text":"<pre><code># app/utils/logging_config.py\nimport logging\nimport sys\nfrom pathlib import Path\n\ndef setup_logging(log_level=\"INFO\", log_file=None):\n    \"\"\"Configura logging para a aplica\u00e7\u00e3o\"\"\"\n    logger = logging.getLogger(\"vision_api\")\n    logger.setLevel(getattr(logging, log_level.upper()))\n\n    # Formata\u00e7\u00e3o\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n\n    # Handler para console\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setFormatter(formatter)\n    logger.addHandler(console_handler)\n\n    # Handler para arquivo (opcional)\n    if log_file:\n        Path(log_file).parent.mkdir(parents=True, exist_ok=True)\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n    return logger\n\n# Uso no servi\u00e7o\nlogger = setup_logging(log_level=\"INFO\", log_file=\"logs/api.log\")\n\ndef log_prediction(image_path: str, prediction: str, confidence: float):\n    \"\"\"Registra predi\u00e7\u00e3o para auditoria\"\"\"\n    logger.info(f\"Prediction: image={image_path}, prediction={prediction}, confidence={confidence:.3f}\")\n</code></pre>"},{"location":"unidades/unidade5/#testes-automatizados","title":"Testes Automatizados","text":"<pre><code># tests/test_api.py\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom PIL import Image\nimport io\nimport numpy as np\n\nfrom app.main import app\n\nclient = TestClient(app)\n\ndef create_test_image(width=224, height=224):\n    \"\"\"Cria imagem de teste\"\"\"\n    image_array = np.random.randint(0, 255, (height, width, 3), dtype=np.uint8)\n    image = Image.fromarray(image_array)\n\n    img_byte_arr = io.BytesIO()\n    image.save(img_byte_arr, format='JPEG')\n    img_byte_arr.seek(0)\n\n    return img_byte_arr\n\ndef test_health_endpoint():\n    \"\"\"Testa endpoint de sa\u00fade\"\"\"\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n    assert response.json() == {\"status\": \"healthy\"}\n\ndef test_classify_endpoint():\n    \"\"\"Testa endpoint de classifica\u00e7\u00e3o\"\"\"\n    test_image = create_test_image()\n\n    response = client.post(\n        \"/classify\",\n        files={\"file\": (\"test.jpg\", test_image, \"image/jpeg\")}\n    )\n\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"success\"] is True\n    assert \"prediction\" in data\n    assert \"confidence\" in data\n    assert isinstance(data[\"confidence\"], float)\n\ndef test_invalid_image_type():\n    \"\"\"Testa upload de tipo de imagem inv\u00e1lido\"\"\"\n    # Criar conte\u00fado inv\u00e1lido\n    invalid_content = io.BytesIO(b\"invalid image content\")\n\n    response = client.post(\n        \"/classify\",\n        files={\"file\": (\"test.txt\", invalid_content, \"text/plain\")}\n    )\n\n    assert response.status_code == 400\n\n# tests/test_vision_service.py\nfrom unittest.mock import Mock, patch\nimport numpy as np\n\nfrom app.services.vision_service import VisionService\n\nclass TestVisionService:\n    def test_preprocess_image(self):\n        \"\"\"Testa pr\u00e9-processamento de imagem\"\"\"\n        service = VisionService()\n\n        # Criar imagem de teste\n        test_image = np.random.randint(0, 255, (300, 400, 3), dtype=np.uint8)\n\n        processed = service.preprocess_image(test_image)\n\n        # Verificar formato\n        assert processed.shape == (1, 224, 224, 3)  # Batch, height, width, channels\n        assert processed.dtype == np.float32\n        assert processed.max() &lt;= 1.0  # Normalizado\n        assert processed.min() &gt;= 0.0  # Normalizado\n\n    @patch('tensorflow.keras.models.load_model')\n    def test_classify_image(self, mock_load_model):\n        \"\"\"Testa classifica\u00e7\u00e3o de imagem\"\"\"\n        # Configurar mock\n        mock_model = Mock()\n        mock_model.predict.return_value = np.array([[0.1, 0.8, 0.1]])  # Probabilidades\n        mock_load_model.return_value = mock_model\n\n        service = VisionService()\n        service.model = mock_model\n        service.classes = ['classe_a', 'classe_b', 'classe_c']\n\n        # Criar imagem de teste\n        test_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n\n        result = service.classify_image(test_image)\n\n        # Verificar resultados\n        assert result['prediction'] == 'classe_b'  # Maior probabilidade\n        assert result['confidence'] == 0.8\n        assert 'processing_time' in result\n        assert isinstance(result['processing_time'], float)\n</code></pre>"},{"location":"unidades/unidade5/#documentacao-da-api","title":"Documenta\u00e7\u00e3o da API","text":"<p>A API criada com FastAPI automaticamente gera documenta\u00e7\u00e3o interativa em <code>/docs</code> e <code>/redoc</code>.</p> <p>Adicionalmente, \u00e9 importante documentar:</p> <ul> <li>Endpoints: Quais endpoints est\u00e3o dispon\u00edveis e como us\u00e1-los</li> <li>Par\u00e2metros: Quais par\u00e2metros s\u00e3o esperados</li> <li>Exemplos de uso: Como consumir a API</li> <li>Erros comuns: Poss\u00edveis erros e como trat\u00e1-los</li> </ul>"},{"location":"unidades/unidade5/#seguranca","title":"Seguran\u00e7a","text":"<pre><code># app/security.py\nfrom fastapi import HTTPException, status\nfrom fastapi.security import HTTPBearer\nimport os\n\nsecurity = HTTPBearer()\n\ndef verify_api_key(credentials):\n    \"\"\"Verifica chave de API\"\"\"\n    expected_key = os.getenv(\"API_KEY\", \"dev-key\")\n    if credentials.credentials != expected_key:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Chave de API inv\u00e1lida\"\n        )\n\ndef rate_limit(max_requests: int = 100, window_seconds: int = 3600):\n    \"\"\"Implementa rate limiting (simplificado)\"\"\"\n    # Em produ\u00e7\u00e3o, usar Redis ou outro sistema de cache\n    pass\n</code></pre>"},{"location":"unidades/unidade5/#consideracoes-finais","title":"Considera\u00e7\u00f5es Finais","text":"<p>A integra\u00e7\u00e3o de modelos de vis\u00e3o computacional em sistemas reais requer aten\u00e7\u00e3o a diversos aspectos al\u00e9m do modelo em si:</p> <ol> <li>Desempenho: Otimizar tempo de infer\u00eancia e uso de mem\u00f3ria</li> <li>Escalabilidade: Preparar o sistema para lidar com aumento de carga</li> <li>Monitoramento: Acompanhar m\u00e9tricas de desempenho e utiliza\u00e7\u00e3o</li> <li>Manuten\u00e7\u00e3o: Facilitar atualiza\u00e7\u00f5es e rollback de modelos</li> <li>Seguran\u00e7a: Proteger endpoints e dados sens\u00edveis</li> <li>Observabilidade: Registrar logs e m\u00e9tricas para troubleshooting</li> </ol> <p>Esses elementos s\u00e3o t\u00e3o importantes quanto o modelo de IA em si para garantir que a solu\u00e7\u00e3o seja sustent\u00e1vel e eficaz em ambiente de produ\u00e7\u00e3o.</p>"}]}